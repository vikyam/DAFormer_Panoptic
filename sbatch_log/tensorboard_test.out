slurmstepd: error: couldn't chdir to `/scratch/vramasamy/DAFormer': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/scratch/vramasamy/DAFormer': No such file or directory: going to /tmp instead
Mon Apr 18 13:14:17 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX TIT...  On   | 00000000:08:00.0 Off |                  N/A |
| 22%   54C    P8    18W / 250W |      1MiB / 12212MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Run job 220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02
2022-04-18 13:16:18,295 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.12 (main, Apr  5 2022, 06:56:58) [GCC 7.5.0]
CUDA available: True
GPU 0: GeForce GTX TITAN X
CUDA_HOME: /usr
NVCC: Build cuda_11.2.r11.2/compiler.29618528_0
GCC: gcc (Debian 8.3.0-6) 8.3.0
PyTorch: 1.7.1+cu110
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.8.2+cu110
OpenCV: 4.4.0
MMCV: 1.3.7
MMCV Compiler: GCC 8.3
MMCV CUDA Compiler: not available
MMSegmentation: 0.16.0+b419f32
------------------------------------------------------------

2022-04-18 13:16:18,295 - mmseg - INFO - Distributed training: False
2022-04-18 13:16:19,485 - mmseg - INFO - Config:
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TextLoggerHook', by_epoch=False),
        dict(type='TensorboardLoggerHook', interval=100, by_epoch=False)
    ])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
norm_cfg = dict(type='BN', requires_grad=True)
find_unused_parameters = True
model = dict(
    type='EncoderDecoder',
    pretrained='pretrained/mit_b5.pth',
    backbone=dict(type='mit_b5', style='pytorch'),
    decode_head=dict(
        type='DAFormerHead',
        in_channels=[64, 128, 320, 512],
        in_index=[0, 1, 2, 3],
        channels=256,
        dropout_ratio=0.1,
        num_classes=19,
        norm_cfg=dict(type='BN', requires_grad=True),
        align_corners=False,
        decoder_params=dict(
            embed_dims=256,
            embed_cfg=dict(type='mlp', act_cfg=None, norm_cfg=None),
            embed_neck_cfg=dict(type='mlp', act_cfg=None, norm_cfg=None),
            fusion_cfg=dict(
                type='aspp',
                sep=True,
                dilations=(1, 6, 12, 18),
                pool=False,
                act_cfg=dict(type='ReLU'),
                norm_cfg=dict(type='BN', requires_grad=True))),
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),
    train_cfg=dict(
        work_dir=
        'work_dirs/local-exp7/220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02'
    ),
    test_cfg=dict(mode='whole'))
dataset_type = 'CityscapesDataset'
data_root = 'data/cityscapes/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (512, 512)
synthia_train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(1280, 760)),
    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
cityscapes_train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(1024, 512)),
    dict(type='RandomCrop', crop_size=(512, 512)),
    dict(type='RandomFlip', prob=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1024, 512),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=4,
    train=dict(
        type='UDADataset',
        source=dict(
            type='SynthiaDataset',
            data_root='data/synthia/',
            img_dir='RGB',
            ann_dir='GT/LABELS',
            pipeline=[
                dict(type='LoadImageFromFile'),
                dict(type='LoadAnnotations'),
                dict(type='Resize', img_scale=(1280, 760)),
                dict(
                    type='RandomCrop',
                    crop_size=(512, 512),
                    cat_max_ratio=0.75),
                dict(type='RandomFlip', prob=0.5),
                dict(
                    type='Normalize',
                    mean=[123.675, 116.28, 103.53],
                    std=[58.395, 57.12, 57.375],
                    to_rgb=True),
                dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
                dict(type='DefaultFormatBundle'),
                dict(type='Collect', keys=['img', 'gt_semantic_seg'])
            ]),
        target=dict(
            type='CityscapesDataset',
            data_root='data/cityscapes/',
            img_dir='leftImg8bit/train',
            ann_dir='gtFine/train',
            pipeline=[
                dict(type='LoadImageFromFile'),
                dict(type='LoadAnnotations'),
                dict(type='Resize', img_scale=(1024, 512)),
                dict(type='RandomCrop', crop_size=(512, 512)),
                dict(type='RandomFlip', prob=0.5),
                dict(
                    type='Normalize',
                    mean=[123.675, 116.28, 103.53],
                    std=[58.395, 57.12, 57.375],
                    to_rgb=True),
                dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
                dict(type='DefaultFormatBundle'),
                dict(type='Collect', keys=['img', 'gt_semantic_seg'])
            ]),
        rare_class_sampling=dict(
            min_pixels=3000, class_temp=0.01, min_crop_ratio=0.5)),
    val=dict(
        type='CityscapesDataset',
        data_root='data/cityscapes/',
        img_dir='leftImg8bit/val',
        ann_dir='gtFine/val',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='CityscapesDataset',
        data_root='data/cityscapes/',
        img_dir='leftImg8bit/val',
        ann_dir='gtFine/val',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
uda = dict(
    type='DACS',
    alpha=0.999,
    pseudo_threshold=0.968,
    pseudo_weight_ignore_top=15,
    pseudo_weight_ignore_bottom=120,
    imnet_feature_dist_lambda=0.005,
    imnet_feature_dist_classes=[6, 7, 11, 12, 13, 14, 15, 16, 17, 18],
    imnet_feature_dist_scale_min_ratio=0.75,
    mix='class',
    blur=True,
    color_jitter_strength=0.2,
    color_jitter_probability=0.2,
    debug_img_interval=100,
    print_grad_magnitude=False)
use_ddp_wrapper = True
optimizer = dict(
    type='AdamW',
    lr=6e-05,
    betas=(0.9, 0.999),
    weight_decay=0.01,
    paramwise_cfg=dict(
        custom_keys=dict(
            head=dict(lr_mult=10.0),
            pos_block=dict(decay_mult=0.0),
            norm=dict(decay_mult=0.0))))
optimizer_config = None
lr_config = dict(
    policy='poly',
    warmup='linear',
    warmup_iters=1500,
    warmup_ratio=1e-06,
    power=1.0,
    min_lr=0.0,
    by_epoch=False)
n_gpus = 1
seed = 0
runner = dict(type='IterBasedRunner', max_iters=40000)
checkpoint_config = dict(by_epoch=False, interval=40000, max_keep_ckpts=1)
evaluation = dict(interval=4000, metric='mIoU')
name = '220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02'
exp = 7
name_dataset = 'synthia2cityscapes'
name_architecture = 'daformer_sepaspp_mitb5'
name_encoder = 'mitb5'
name_decoder = 'daformer_sepaspp'
name_uda = 'dacs_a999_fdthings_rcs0.01_cpl'
name_opt = 'adamw_6e-05_pmTrue_poly10warm_1x2_40k'
work_dir = 'work_dirs/local-exp7/220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02'
git_rev = 'b419f3286a5b1445c15f82c139ec94418e5b8032'
gpu_ids = range(0, 1)

2022-04-18 13:16:19,485 - mmseg - INFO - Set random seed to 0, deterministic: False
/scratch_net/biwidl204/vramasamy/DAFormer/mmseg/models/backbones/mix_transformer.py:214: UserWarning: DeprecationWarning: pretrained is a deprecated, please use "init_cfg" instead
  warnings.warn('DeprecationWarning: pretrained is a deprecated, '
2022-04-18 13:16:22,906 - mmseg - INFO - Load mit checkpoint.
2022-04-18 13:16:22,907 - mmseg - INFO - Use load_from_local loader
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/cnn/utils/weight_init.py:118: UserWarning: init_cfg without layer key, if you do not define override key either, this init_cfg will do nothing
  warnings.warn(
2022-04-18 13:16:23,326 - mmseg - INFO - Load mit checkpoint.
2022-04-18 13:16:23,326 - mmseg - INFO - Use load_from_local loader
2022-04-18 13:16:23,656 - mmseg - INFO - Load mit checkpoint.
2022-04-18 13:16:23,656 - mmseg - INFO - Use load_from_local loader
2022-04-18 13:16:23,985 - mmseg - INFO - DACS(
  (model): EncoderDecoder(
    (backbone): mit_b5(
      (patch_embed1): OverlapPatchEmbed(
        (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (patch_embed2): OverlapPatchEmbed(
        (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (patch_embed3): OverlapPatchEmbed(
        (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
      )
      (patch_embed4): OverlapPatchEmbed(
        (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (block1): ModuleList(
        (0): Block(
          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=64, out_features=64, bias=True)
            (kv): Linear(in_features=64, out_features=128, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=64, out_features=64, bias=True)
            (kv): Linear(in_features=64, out_features=128, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=64, out_features=64, bias=True)
            (kv): Linear(in_features=64, out_features=128, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (block2): ModuleList(
        (0): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (block3): ModuleList(
        (0): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (18): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (19): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (20): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (21): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (22): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (23): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (24): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (25): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (26): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (27): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (28): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (29): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (30): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (31): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (32): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (33): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (34): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (35): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (36): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (37): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (38): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (39): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm3): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (block4): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=512, out_features=512, bias=True)
            (kv): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=512, out_features=512, bias=True)
            (kv): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=512, out_features=512, bias=True)
            (kv): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm4): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    )
    (decode_head): DAFormerHead(
      input_transform=multiple_select, ignore_index=255, align_corners=False
      (loss_decode): CrossEntropyLoss()
      (conv_seg): Conv2d(256, 19, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout2d(p=0.1, inplace=False)
      (embed_layers): ModuleDict(
        (0): MLP(
          (proj): Linear(in_features=64, out_features=256, bias=True)
        )
        (1): MLP(
          (proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (2): MLP(
          (proj): Linear(in_features=320, out_features=256, bias=True)
        )
        (3): MLP(
          (proj): Linear(in_features=512, out_features=256, bias=True)
        )
      )
      (fuse_layer): ASPPWrapper(
        (aspp_modules): DepthwiseSeparableASPPModule(
          (0): ConvModule(
            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (1): DepthwiseSeparableConvModule(
            (depthwise_conv): ConvModule(
              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=1024, bias=False)
              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (pointwise_conv): ConvModule(
              (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
          )
          (2): DepthwiseSeparableConvModule(
            (depthwise_conv): ConvModule(
              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=1024, bias=False)
              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (pointwise_conv): ConvModule(
              (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
          )
          (3): DepthwiseSeparableConvModule(
            (depthwise_conv): ConvModule(
              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), groups=1024, bias=False)
              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (pointwise_conv): ConvModule(
              (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
          )
        )
        (bottleneck): ConvModule(
          (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
      )
    )
    init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
  )
  (ema_model): EncoderDecoder(
    (backbone): mit_b5(
      (patch_embed1): OverlapPatchEmbed(
        (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (patch_embed2): OverlapPatchEmbed(
        (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (patch_embed3): OverlapPatchEmbed(
        (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
      )
      (patch_embed4): OverlapPatchEmbed(
        (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (block1): ModuleList(
        (0): Block(
          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=64, out_features=64, bias=True)
            (kv): Linear(in_features=64, out_features=128, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=64, out_features=64, bias=True)
            (kv): Linear(in_features=64, out_features=128, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=64, out_features=64, bias=True)
            (kv): Linear(in_features=64, out_features=128, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (block2): ModuleList(
        (0): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (block3): ModuleList(
        (0): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (18): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (19): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (20): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (21): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (22): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (23): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (24): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (25): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (26): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (27): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (28): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (29): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (30): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (31): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (32): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (33): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (34): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (35): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (36): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (37): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (38): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (39): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm3): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (block4): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=512, out_features=512, bias=True)
            (kv): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=512, out_features=512, bias=True)
            (kv): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=512, out_features=512, bias=True)
            (kv): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm4): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    )
    (decode_head): DAFormerHead(
      input_transform=multiple_select, ignore_index=255, align_corners=False
      (loss_decode): CrossEntropyLoss()
      (conv_seg): Conv2d(256, 19, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout2d(p=0.1, inplace=False)
      (embed_layers): ModuleDict(
        (0): MLP(
          (proj): Linear(in_features=64, out_features=256, bias=True)
        )
        (1): MLP(
          (proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (2): MLP(
          (proj): Linear(in_features=320, out_features=256, bias=True)
        )
        (3): MLP(
          (proj): Linear(in_features=512, out_features=256, bias=True)
        )
      )
      (fuse_layer): ASPPWrapper(
        (aspp_modules): DepthwiseSeparableASPPModule(
          (0): ConvModule(
            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (1): DepthwiseSeparableConvModule(
            (depthwise_conv): ConvModule(
              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=1024, bias=False)
              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (pointwise_conv): ConvModule(
              (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
          )
          (2): DepthwiseSeparableConvModule(
            (depthwise_conv): ConvModule(
              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=1024, bias=False)
              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (pointwise_conv): ConvModule(
              (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
          )
          (3): DepthwiseSeparableConvModule(
            (depthwise_conv): ConvModule(
              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), groups=1024, bias=False)
              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (pointwise_conv): ConvModule(
              (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
          )
        )
        (bottleneck): ConvModule(
          (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
      )
    )
    init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
  )
  (imnet_model): EncoderDecoder(
    (backbone): mit_b5(
      (patch_embed1): OverlapPatchEmbed(
        (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (patch_embed2): OverlapPatchEmbed(
        (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (patch_embed3): OverlapPatchEmbed(
        (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
      )
      (patch_embed4): OverlapPatchEmbed(
        (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (block1): ModuleList(
        (0): Block(
          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=64, out_features=64, bias=True)
            (kv): Linear(in_features=64, out_features=128, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=64, out_features=64, bias=True)
            (kv): Linear(in_features=64, out_features=128, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=64, out_features=64, bias=True)
            (kv): Linear(in_features=64, out_features=128, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (block2): ModuleList(
        (0): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (block3): ModuleList(
        (0): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (18): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (19): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (20): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (21): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (22): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (23): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (24): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (25): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (26): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (27): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (28): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (29): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (30): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (31): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (32): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (33): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (34): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (35): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (36): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (37): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (38): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (39): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm3): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (block4): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=512, out_features=512, bias=True)
            (kv): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=512, out_features=512, bias=True)
            (kv): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=512, out_features=512, bias=True)
            (kv): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm4): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    )
    (decode_head): DAFormerHead(
      input_transform=multiple_select, ignore_index=255, align_corners=False
      (loss_decode): CrossEntropyLoss()
      (conv_seg): Conv2d(256, 19, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout2d(p=0.1, inplace=False)
      (embed_layers): ModuleDict(
        (0): MLP(
          (proj): Linear(in_features=64, out_features=256, bias=True)
        )
        (1): MLP(
          (proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (2): MLP(
          (proj): Linear(in_features=320, out_features=256, bias=True)
        )
        (3): MLP(
          (proj): Linear(in_features=512, out_features=256, bias=True)
        )
      )
      (fuse_layer): ASPPWrapper(
        (aspp_modules): DepthwiseSeparableASPPModule(
          (0): ConvModule(
            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (1): DepthwiseSeparableConvModule(
            (depthwise_conv): ConvModule(
              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=1024, bias=False)
              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (pointwise_conv): ConvModule(
              (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
          )
          (2): DepthwiseSeparableConvModule(
            (depthwise_conv): ConvModule(
              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=1024, bias=False)
              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (pointwise_conv): ConvModule(
              (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
          )
          (3): DepthwiseSeparableConvModule(
            (depthwise_conv): ConvModule(
              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), groups=1024, bias=False)
              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (pointwise_conv): ConvModule(
              (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
          )
        )
        (bottleneck): ConvModule(
          (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
      )
    )
    init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
  )
)
2022-04-18 13:16:24,215 - mmseg - INFO - Loaded 9400 images from data/synthia/RGB
2022-04-18 13:16:24,357 - mmseg - INFO - Loaded 2975 images from data/cityscapes/leftImg8bit/train
2022-04-18 13:16:24,440 - mmseg - INFO - RCS Classes: [6, 7, 17, 18, 4, 3, 12, 5, 15, 13, 11, 10, 8, 0, 1, 2]
2022-04-18 13:16:24,441 - mmseg - INFO - RCS ClassProb: [1.55649453e-01 1.45927146e-01 1.30805254e-01 1.29144296e-01
 1.22819245e-01 1.22646019e-01 9.97279957e-02 5.53424992e-02
 3.33626680e-02 2.43163528e-03 2.00466975e-03 1.35448776e-04
 3.78220921e-06 8.31826052e-10 3.41141226e-10 1.11511794e-14]
2022-04-18 13:16:40,872 - mmseg - INFO - Loaded 500 images from data/cityscapes/leftImg8bit/val
2022-04-18 13:16:40,873 - mmseg - INFO - Start running, host: vramasamy@biwirender10, work_dir: /scratch_net/biwidl204/vramasamy/DAFormer/work_dirs/local-exp7/220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02
2022-04-18 13:16:40,873 - mmseg - INFO - workflow: [('train', 1)], max: 40000 iters
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:3: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:3: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
2022-04-18 13:16:56,896 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.12 (main, Apr  5 2022, 06:56:58) [GCC 7.5.0]
CUDA available: True
GPU 0: GeForce RTX 2080 Ti
CUDA_HOME: /usr
NVCC: Build cuda_11.2.r11.2/compiler.29618528_0
GCC: gcc (Debian 8.3.0-6) 8.3.0
PyTorch: 1.7.1+cu110
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.8.2+cu110
OpenCV: 4.4.0
MMCV: 1.3.7
MMCV Compiler: GCC 8.3
MMCV CUDA Compiler: not available
MMSegmentation: 0.16.0+b419f32
------------------------------------------------------------

2022-04-18 13:16:56,897 - mmseg - INFO - Distributed training: False
2022-04-18 13:16:57,760 - mmseg - INFO - Config:
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TextLoggerHook', by_epoch=False),
        dict(type='TensorboardLoggerHook', interval=100, by_epoch=False)
    ])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
norm_cfg = dict(type='BN', requires_grad=True)
find_unused_parameters = True
model = dict(
    type='EncoderDecoder',
    pretrained='pretrained/mit_b5.pth',
    backbone=dict(type='mit_b5', style='pytorch'),
    decode_head=dict(
        type='DAFormerHead',
        in_channels=[64, 128, 320, 512],
        in_index=[0, 1, 2, 3],
        channels=256,
        dropout_ratio=0.1,
        num_classes=19,
        norm_cfg=dict(type='BN', requires_grad=True),
        align_corners=False,
        decoder_params=dict(
            embed_dims=256,
            embed_cfg=dict(type='mlp', act_cfg=None, norm_cfg=None),
            embed_neck_cfg=dict(type='mlp', act_cfg=None, norm_cfg=None),
            fusion_cfg=dict(
                type='aspp',
                sep=True,
                dilations=(1, 6, 12, 18),
                pool=False,
                act_cfg=dict(type='ReLU'),
                norm_cfg=dict(type='BN', requires_grad=True))),
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),
    train_cfg=dict(
        work_dir=
        'work_dirs/local-exp7/220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618'
    ),
    test_cfg=dict(mode='whole'))
dataset_type = 'CityscapesDataset'
data_root = 'data/cityscapes/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (512, 512)
synthia_train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(1280, 760)),
    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
cityscapes_train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(1024, 512)),
    dict(type='RandomCrop', crop_size=(512, 512)),
    dict(type='RandomFlip', prob=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1024, 512),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=4,
    train=dict(
        type='UDADataset',
        source=dict(
            type='SynthiaDataset',
            data_root='data/synthia/',
            img_dir='RGB',
            ann_dir='GT/LABELS',
            pipeline=[
                dict(type='LoadImageFromFile'),
                dict(type='LoadAnnotations'),
                dict(type='Resize', img_scale=(1280, 760)),
                dict(
                    type='RandomCrop',
                    crop_size=(512, 512),
                    cat_max_ratio=0.75),
                dict(type='RandomFlip', prob=0.5),
                dict(
                    type='Normalize',
                    mean=[123.675, 116.28, 103.53],
                    std=[58.395, 57.12, 57.375],
                    to_rgb=True),
                dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
                dict(type='DefaultFormatBundle'),
                dict(type='Collect', keys=['img', 'gt_semantic_seg'])
            ]),
        target=dict(
            type='CityscapesDataset',
            data_root='data/cityscapes/',
            img_dir='leftImg8bit/train',
            ann_dir='gtFine/train',
            pipeline=[
                dict(type='LoadImageFromFile'),
                dict(type='LoadAnnotations'),
                dict(type='Resize', img_scale=(1024, 512)),
                dict(type='RandomCrop', crop_size=(512, 512)),
                dict(type='RandomFlip', prob=0.5),
                dict(
                    type='Normalize',
                    mean=[123.675, 116.28, 103.53],
                    std=[58.395, 57.12, 57.375],
                    to_rgb=True),
                dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
                dict(type='DefaultFormatBundle'),
                dict(type='Collect', keys=['img', 'gt_semantic_seg'])
            ]),
        rare_class_sampling=dict(
            min_pixels=3000, class_temp=0.01, min_crop_ratio=0.5)),
    val=dict(
        type='CityscapesDataset',
        data_root='data/cityscapes/',
        img_dir='leftImg8bit/val',
        ann_dir='gtFine/val',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='CityscapesDataset',
        data_root='data/cityscapes/',
        img_dir='leftImg8bit/val',
        ann_dir='gtFine/val',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
uda = dict(
    type='DACS',
    alpha=0.999,
    pseudo_threshold=0.968,
    pseudo_weight_ignore_top=15,
    pseudo_weight_ignore_bottom=120,
    imnet_feature_dist_lambda=0.005,
    imnet_feature_dist_classes=[6, 7, 11, 12, 13, 14, 15, 16, 17, 18],
    imnet_feature_dist_scale_min_ratio=0.75,
    mix='class',
    blur=True,
    color_jitter_strength=0.2,
    color_jitter_probability=0.2,
    debug_img_interval=100,
    print_grad_magnitude=False)
use_ddp_wrapper = True
optimizer = dict(
    type='AdamW',
    lr=6e-05,
    betas=(0.9, 0.999),
    weight_decay=0.01,
    paramwise_cfg=dict(
        custom_keys=dict(
            head=dict(lr_mult=10.0),
            pos_block=dict(decay_mult=0.0),
            norm=dict(decay_mult=0.0))))
optimizer_config = None
lr_config = dict(
    policy='poly',
    warmup='linear',
    warmup_iters=1500,
    warmup_ratio=1e-06,
    power=1.0,
    min_lr=0.0,
    by_epoch=False)
n_gpus = 1
seed = 0
runner = dict(type='IterBasedRunner', max_iters=40000)
checkpoint_config = dict(by_epoch=False, interval=40000, max_keep_ckpts=1)
evaluation = dict(interval=4000, metric='mIoU')
name = '220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618'
exp = 7
name_dataset = 'synthia2cityscapes'
name_architecture = 'daformer_sepaspp_mitb5'
name_encoder = 'mitb5'
name_decoder = 'daformer_sepaspp'
name_uda = 'dacs_a999_fdthings_rcs0.01_cpl'
name_opt = 'adamw_6e-05_pmTrue_poly10warm_1x2_40k'
work_dir = 'work_dirs/local-exp7/220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618'
git_rev = 'b419f3286a5b1445c15f82c139ec94418e5b8032'
gpu_ids = range(0, 1)

2022-04-18 13:16:57,760 - mmseg - INFO - Set random seed to 0, deterministic: False
/scratch_net/biwidl204/vramasamy/DAFormer/mmseg/models/backbones/mix_transformer.py:214: UserWarning: DeprecationWarning: pretrained is a deprecated, please use "init_cfg" instead
  warnings.warn('DeprecationWarning: pretrained is a deprecated, '
2022-04-18 13:17:00,190 - mmseg - INFO - Load mit checkpoint.
2022-04-18 13:17:00,191 - mmseg - INFO - Use load_from_local loader
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/cnn/utils/weight_init.py:118: UserWarning: init_cfg without layer key, if you do not define override key either, this init_cfg will do nothing
  warnings.warn(
2022-04-18 13:17:03,996 - mmseg - INFO - Load mit checkpoint.
2022-04-18 13:17:03,996 - mmseg - INFO - Use load_from_local loader
2022-04-18 13:17:04,386 - mmseg - INFO - Load mit checkpoint.
2022-04-18 13:17:04,386 - mmseg - INFO - Use load_from_local loader
2022-04-18 13:17:04,769 - mmseg - INFO - DACS(
  (model): EncoderDecoder(
    (backbone): mit_b5(
      (patch_embed1): OverlapPatchEmbed(
        (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (patch_embed2): OverlapPatchEmbed(
        (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (patch_embed3): OverlapPatchEmbed(
        (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
      )
      (patch_embed4): OverlapPatchEmbed(
        (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (block1): ModuleList(
        (0): Block(
          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=64, out_features=64, bias=True)
            (kv): Linear(in_features=64, out_features=128, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=64, out_features=64, bias=True)
            (kv): Linear(in_features=64, out_features=128, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=64, out_features=64, bias=True)
            (kv): Linear(in_features=64, out_features=128, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (block2): ModuleList(
        (0): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (block3): ModuleList(
        (0): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (18): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (19): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (20): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (21): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (22): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (23): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (24): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (25): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (26): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (27): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (28): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (29): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (30): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (31): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (32): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (33): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (34): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (35): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (36): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (37): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (38): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (39): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm3): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (block4): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=512, out_features=512, bias=True)
            (kv): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=512, out_features=512, bias=True)
            (kv): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=512, out_features=512, bias=True)
            (kv): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm4): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    )
    (decode_head): DAFormerHead(
      input_transform=multiple_select, ignore_index=255, align_corners=False
      (loss_decode): CrossEntropyLoss()
      (conv_seg): Conv2d(256, 19, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout2d(p=0.1, inplace=False)
      (embed_layers): ModuleDict(
        (0): MLP(
          (proj): Linear(in_features=64, out_features=256, bias=True)
        )
        (1): MLP(
          (proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (2): MLP(
          (proj): Linear(in_features=320, out_features=256, bias=True)
        )
        (3): MLP(
          (proj): Linear(in_features=512, out_features=256, bias=True)
        )
      )
      (fuse_layer): ASPPWrapper(
        (aspp_modules): DepthwiseSeparableASPPModule(
          (0): ConvModule(
            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (1): DepthwiseSeparableConvModule(
            (depthwise_conv): ConvModule(
              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=1024, bias=False)
              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (pointwise_conv): ConvModule(
              (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
          )
          (2): DepthwiseSeparableConvModule(
            (depthwise_conv): ConvModule(
              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=1024, bias=False)
              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (pointwise_conv): ConvModule(
              (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
          )
          (3): DepthwiseSeparableConvModule(
            (depthwise_conv): ConvModule(
              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), groups=1024, bias=False)
              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (pointwise_conv): ConvModule(
              (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
          )
        )
        (bottleneck): ConvModule(
          (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
      )
    )
    init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
  )
  (ema_model): EncoderDecoder(
    (backbone): mit_b5(
      (patch_embed1): OverlapPatchEmbed(
        (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (patch_embed2): OverlapPatchEmbed(
        (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (patch_embed3): OverlapPatchEmbed(
        (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
      )
      (patch_embed4): OverlapPatchEmbed(
        (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (block1): ModuleList(
        (0): Block(
          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=64, out_features=64, bias=True)
            (kv): Linear(in_features=64, out_features=128, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=64, out_features=64, bias=True)
            (kv): Linear(in_features=64, out_features=128, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=64, out_features=64, bias=True)
            (kv): Linear(in_features=64, out_features=128, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (block2): ModuleList(
        (0): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (block3): ModuleList(
        (0): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (18): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (19): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (20): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (21): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (22): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (23): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (24): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (25): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (26): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (27): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (28): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (29): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (30): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (31): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (32): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (33): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (34): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (35): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (36): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (37): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (38): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (39): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm3): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (block4): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=512, out_features=512, bias=True)
            (kv): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=512, out_features=512, bias=True)
            (kv): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=512, out_features=512, bias=True)
            (kv): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm4): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    )
    (decode_head): DAFormerHead(
      input_transform=multiple_select, ignore_index=255, align_corners=False
      (loss_decode): CrossEntropyLoss()
      (conv_seg): Conv2d(256, 19, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout2d(p=0.1, inplace=False)
      (embed_layers): ModuleDict(
        (0): MLP(
          (proj): Linear(in_features=64, out_features=256, bias=True)
        )
        (1): MLP(
          (proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (2): MLP(
          (proj): Linear(in_features=320, out_features=256, bias=True)
        )
        (3): MLP(
          (proj): Linear(in_features=512, out_features=256, bias=True)
        )
      )
      (fuse_layer): ASPPWrapper(
        (aspp_modules): DepthwiseSeparableASPPModule(
          (0): ConvModule(
            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (1): DepthwiseSeparableConvModule(
            (depthwise_conv): ConvModule(
              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=1024, bias=False)
              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (pointwise_conv): ConvModule(
              (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
          )
          (2): DepthwiseSeparableConvModule(
            (depthwise_conv): ConvModule(
              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=1024, bias=False)
              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (pointwise_conv): ConvModule(
              (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
          )
          (3): DepthwiseSeparableConvModule(
            (depthwise_conv): ConvModule(
              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), groups=1024, bias=False)
              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (pointwise_conv): ConvModule(
              (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
          )
        )
        (bottleneck): ConvModule(
          (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
      )
    )
    init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
  )
  (imnet_model): EncoderDecoder(
    (backbone): mit_b5(
      (patch_embed1): OverlapPatchEmbed(
        (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (patch_embed2): OverlapPatchEmbed(
        (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (patch_embed3): OverlapPatchEmbed(
        (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
      )
      (patch_embed4): OverlapPatchEmbed(
        (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (block1): ModuleList(
        (0): Block(
          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=64, out_features=64, bias=True)
            (kv): Linear(in_features=64, out_features=128, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=64, out_features=64, bias=True)
            (kv): Linear(in_features=64, out_features=128, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=64, out_features=64, bias=True)
            (kv): Linear(in_features=64, out_features=128, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
      (block2): ModuleList(
        (0): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=128, out_features=128, bias=True)
            (kv): Linear(in_features=128, out_features=256, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
      (block3): ModuleList(
        (0): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (18): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (19): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (20): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (21): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (22): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (23): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (24): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (25): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (26): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (27): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (28): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (29): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (30): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (31): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (32): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (33): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (34): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (35): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (36): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (37): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (38): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (39): Block(
          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=320, out_features=320, bias=True)
            (kv): Linear(in_features=320, out_features=640, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=320, out_features=320, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=320, out_features=1280, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
            )
            (act): GELU()
            (fc2): Linear(in_features=1280, out_features=320, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm3): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
      (block4): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=512, out_features=512, bias=True)
            (kv): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=512, out_features=512, bias=True)
            (kv): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q): Linear(in_features=512, out_features=512, bias=True)
            (kv): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm4): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    )
    (decode_head): DAFormerHead(
      input_transform=multiple_select, ignore_index=255, align_corners=False
      (loss_decode): CrossEntropyLoss()
      (conv_seg): Conv2d(256, 19, kernel_size=(1, 1), stride=(1, 1))
      (dropout): Dropout2d(p=0.1, inplace=False)
      (embed_layers): ModuleDict(
        (0): MLP(
          (proj): Linear(in_features=64, out_features=256, bias=True)
        )
        (1): MLP(
          (proj): Linear(in_features=128, out_features=256, bias=True)
        )
        (2): MLP(
          (proj): Linear(in_features=320, out_features=256, bias=True)
        )
        (3): MLP(
          (proj): Linear(in_features=512, out_features=256, bias=True)
        )
      )
      (fuse_layer): ASPPWrapper(
        (aspp_modules): DepthwiseSeparableASPPModule(
          (0): ConvModule(
            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (1): DepthwiseSeparableConvModule(
            (depthwise_conv): ConvModule(
              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=1024, bias=False)
              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (pointwise_conv): ConvModule(
              (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
          )
          (2): DepthwiseSeparableConvModule(
            (depthwise_conv): ConvModule(
              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=1024, bias=False)
              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (pointwise_conv): ConvModule(
              (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
          )
          (3): DepthwiseSeparableConvModule(
            (depthwise_conv): ConvModule(
              (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), groups=1024, bias=False)
              (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (pointwise_conv): ConvModule(
              (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
          )
        )
        (bottleneck): ConvModule(
          (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
      )
    )
    init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
  )
)
2022-04-18 13:17:07,738 - mmseg - INFO - Loaded 9400 images from data/synthia/RGB
2022-04-18 13:17:07,898 - mmseg - INFO - Loaded 2975 images from data/cityscapes/leftImg8bit/train
2022-04-18 13:17:07,977 - mmseg - INFO - RCS Classes: [6, 7, 17, 18, 4, 3, 12, 5, 15, 13, 11, 10, 8, 0, 1, 2]
2022-04-18 13:17:07,978 - mmseg - INFO - RCS ClassProb: [1.55649453e-01 1.45927146e-01 1.30805254e-01 1.29144296e-01
 1.22819245e-01 1.22646019e-01 9.97279957e-02 5.53424992e-02
 3.33626680e-02 2.43163528e-03 2.00466975e-03 1.35448776e-04
 3.78220921e-06 8.31826052e-10 3.41141226e-10 1.11511794e-14]
2022-04-18 13:17:29,691 - mmseg - INFO - Loaded 500 images from data/cityscapes/leftImg8bit/val
2022-04-18 13:17:29,691 - mmseg - INFO - Start running, host: vramasamy@biwirender12, work_dir: /scratch_net/biwidl204/vramasamy/DAFormer/work_dirs/local-exp7/220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-18 13:17:29,692 - mmseg - INFO - workflow: [('train', 1)], max: 40000 iters
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:3: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:3: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:18:52,682 - mmseg - INFO - Iter [50/40000]	lr: 1.958e-06, eta: 17:50:29, time: 1.608, data_time: 0.039, memory: 9636, decode.loss_seg: 2.7923, decode.acc_seg: 4.5597, src.loss_imnet_feat_dist: 0.0652, mix.decode.loss_seg: 1.4964, mix.decode.acc_seg: 14.1946
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:20:06,960 - mmseg - INFO - Iter [100/40000]	lr: 3.950e-06, eta: 17:08:34, time: 1.486, data_time: 0.012, memory: 9636, decode.loss_seg: 2.5909, decode.acc_seg: 38.3200, src.loss_imnet_feat_dist: 0.0683, mix.decode.loss_seg: 1.4459, mix.decode.acc_seg: 29.9613
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:21:21,470 - mmseg - INFO - Iter [150/40000]	lr: 5.938e-06, eta: 16:54:46, time: 1.490, data_time: 0.013, memory: 9636, decode.loss_seg: 2.1720, decode.acc_seg: 60.7769, src.loss_imnet_feat_dist: 0.0819, mix.decode.loss_seg: 1.1057, mix.decode.acc_seg: 52.4092
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:22:31,377 - mmseg - INFO - Iter [200/40000]	lr: 7.920e-06, eta: 16:31:58, time: 1.398, data_time: 0.013, memory: 9636, decode.loss_seg: 1.7342, decode.acc_seg: 66.2707, src.loss_imnet_feat_dist: 0.0948, mix.decode.loss_seg: 0.8520, mix.decode.acc_seg: 61.6406
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:23:48,100 - mmseg - INFO - Iter [250/40000]	lr: 9.898e-06, eta: 16:35:54, time: 1.534, data_time: 0.013, memory: 9636, decode.loss_seg: 1.4029, decode.acc_seg: 70.1987, src.loss_imnet_feat_dist: 0.1029, mix.decode.loss_seg: 0.6544, mix.decode.acc_seg: 64.8861
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:25:04,928 - mmseg - INFO - Iter [300/40000]	lr: 1.187e-05, eta: 16:38:19, time: 1.537, data_time: 0.013, memory: 9636, decode.loss_seg: 1.1362, decode.acc_seg: 74.2714, src.loss_imnet_feat_dist: 0.1072, mix.decode.loss_seg: 0.5769, mix.decode.acc_seg: 67.3342
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:26:20,454 - mmseg - INFO - Iter [350/40000]	lr: 1.384e-05, eta: 16:37:13, time: 1.511, data_time: 0.013, memory: 9636, decode.loss_seg: 0.9499, decode.acc_seg: 74.8407, src.loss_imnet_feat_dist: 0.1146, mix.decode.loss_seg: 0.4888, mix.decode.acc_seg: 67.3454
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:27:38,423 - mmseg - INFO - Iter [400/40000]	lr: 1.580e-05, eta: 16:40:07, time: 1.559, data_time: 0.013, memory: 9636, decode.loss_seg: 0.8837, decode.acc_seg: 75.0756, src.loss_imnet_feat_dist: 0.1169, mix.decode.loss_seg: 0.4312, mix.decode.acc_seg: 69.2834
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:28:54,736 - mmseg - INFO - Iter [450/40000]	lr: 1.776e-05, eta: 16:39:39, time: 1.526, data_time: 0.013, memory: 9636, decode.loss_seg: 0.7554, decode.acc_seg: 77.2399, src.loss_imnet_feat_dist: 0.1226, mix.decode.loss_seg: 0.3945, mix.decode.acc_seg: 68.7804
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:30:08,519 - mmseg - INFO - Iter [500/40000]	lr: 1.971e-05, eta: 16:35:42, time: 1.476, data_time: 0.013, memory: 9636, decode.loss_seg: 0.6818, decode.acc_seg: 77.7712, src.loss_imnet_feat_dist: 0.1152, mix.decode.loss_seg: 0.3491, mix.decode.acc_seg: 67.6656
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:31:22,813 - mmseg - INFO - Iter [550/40000]	lr: 2.166e-05, eta: 16:32:51, time: 1.486, data_time: 0.013, memory: 9636, decode.loss_seg: 0.6264, decode.acc_seg: 79.2006, src.loss_imnet_feat_dist: 0.1175, mix.decode.loss_seg: 0.3702, mix.decode.acc_seg: 67.8815
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:32:35,474 - mmseg - INFO - Iter [600/40000]	lr: 2.360e-05, eta: 16:28:29, time: 1.453, data_time: 0.013, memory: 9636, decode.loss_seg: 0.5927, decode.acc_seg: 79.6232, src.loss_imnet_feat_dist: 0.1239, mix.decode.loss_seg: 0.3047, mix.decode.acc_seg: 70.5211
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:33:49,429 - mmseg - INFO - Iter [650/40000]	lr: 2.554e-05, eta: 16:25:54, time: 1.479, data_time: 0.013, memory: 9636, decode.loss_seg: 0.5397, decode.acc_seg: 80.9890, src.loss_imnet_feat_dist: 0.1166, mix.decode.loss_seg: 0.2958, mix.decode.acc_seg: 70.4295
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:35:04,086 - mmseg - INFO - Iter [700/40000]	lr: 2.747e-05, eta: 16:24:10, time: 1.493, data_time: 0.013, memory: 9636, decode.loss_seg: 0.5129, decode.acc_seg: 81.4985, src.loss_imnet_feat_dist: 0.1250, mix.decode.loss_seg: 0.2843, mix.decode.acc_seg: 72.0903
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:36:20,221 - mmseg - INFO - Iter [750/40000]	lr: 2.940e-05, eta: 16:23:48, time: 1.523, data_time: 0.013, memory: 9636, decode.loss_seg: 0.4480, decode.acc_seg: 82.4495, src.loss_imnet_feat_dist: 0.1156, mix.decode.loss_seg: 0.2119, mix.decode.acc_seg: 70.8133
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:37:33,845 - mmseg - INFO - Iter [800/40000]	lr: 3.132e-05, eta: 16:21:16, time: 1.472, data_time: 0.013, memory: 9636, decode.loss_seg: 0.4068, decode.acc_seg: 82.5496, src.loss_imnet_feat_dist: 0.1253, mix.decode.loss_seg: 0.2374, mix.decode.acc_seg: 70.0902
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:38:45,499 - mmseg - INFO - Iter [850/40000]	lr: 3.324e-05, eta: 16:17:22, time: 1.433, data_time: 0.013, memory: 9636, decode.loss_seg: 0.4450, decode.acc_seg: 82.7414, src.loss_imnet_feat_dist: 0.1269, mix.decode.loss_seg: 0.2453, mix.decode.acc_seg: 72.8787
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:39:03,480 - mmseg - INFO - Iter [400/40000]	lr: 1.580e-05, eta: 1 day, 12:49:23, time: 3.358, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 20.5002, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.9552
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:41:52,159 - mmseg - INFO - Iter [450/40000]	lr: 1.776e-05, eta: 1 day, 12:48:30, time: 3.374, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 21.5955, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.8826
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:44:39,032 - mmseg - INFO - Iter [500/40000]	lr: 1.971e-05, eta: 1 day, 12:44:51, time: 3.337, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 22.3742, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.0708
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:47:26,725 - mmseg - INFO - Iter [550/40000]	lr: 2.166e-05, eta: 1 day, 12:42:21, time: 3.354, data_time: 0.017, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 22.4148, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.0349
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:50:12,437 - mmseg - INFO - Iter [600/40000]	lr: 2.360e-05, eta: 1 day, 12:37:37, time: 3.314, data_time: 0.017, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 22.6934, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.4463
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:53:00,494 - mmseg - INFO - Iter [650/40000]	lr: 2.554e-05, eta: 1 day, 12:35:34, time: 3.361, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 20.8132, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.9794
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:55:46,224 - mmseg - INFO - Iter [700/40000]	lr: 2.747e-05, eta: 1 day, 12:31:13, time: 3.315, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 21.3936, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.4279
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 13:58:35,109 - mmseg - INFO - Iter [750/40000]	lr: 2.940e-05, eta: 1 day, 12:29:51, time: 3.378, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 22.8671, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.9150
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:01:24,385 - mmseg - INFO - Iter [800/40000]	lr: 3.132e-05, eta: 1 day, 12:28:36, time: 3.386, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 19.4792, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.8944
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:04:15,089 - mmseg - INFO - Iter [850/40000]	lr: 3.324e-05, eta: 1 day, 12:28:17, time: 3.414, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 20.5944, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.1461
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:07:00,454 - mmseg - INFO - Iter [900/40000]	lr: 3.515e-05, eta: 1 day, 12:23:48, time: 3.307, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 19.9737, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.2369
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:09:47,647 - mmseg - INFO - Iter [950/40000]	lr: 3.706e-05, eta: 1 day, 12:20:46, time: 3.344, data_time: 0.017, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 20.8742, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.5882
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:12:43,279 - mmseg - INFO - Iter [2200/40000]	lr: 5.670e-05, eta: 15:48:08, time: 1.513, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2458, decode.acc_seg: 87.1685, src.loss_imnet_feat_dist: 0.1301, mix.decode.loss_seg: 0.1928, mix.decode.acc_seg: 83.9493
y: 9779, decode.loss_seg: nan, decode.acc_seg: 21.8172, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.6671
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:14:00,710 - mmseg - INFO - Iter [2250/40000]	lr: 5.663e-05, eta: 15:47:30, time: 1.549, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2169, decode.acc_seg: 88.0093, src.loss_imnet_feat_dist: 0.1240, mix.decode.loss_seg: 0.1928, mix.decode.acc_seg: 82.9415
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:15:17,852 - mmseg - INFO - Iter [2300/40000]	lr: 5.655e-05, eta: 15:46:45, time: 1.543, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2117, decode.acc_seg: 88.7458, src.loss_imnet_feat_dist: 0.1230, mix.decode.loss_seg: 0.1936, mix.decode.acc_seg: 84.0613
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:16:37,055 - mmseg - INFO - Iter [2350/40000]	lr: 5.648e-05, eta: 15:46:31, time: 1.584, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2140, decode.acc_seg: 87.7598, src.loss_imnet_feat_dist: 0.1256, mix.decode.loss_seg: 0.1776, mix.decode.acc_seg: 85.1640
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:17:49,912 - mmseg - INFO - Iter [2400/40000]	lr: 5.640e-05, eta: 15:44:36, time: 1.457, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2082, decode.acc_seg: 87.7401, src.loss_imnet_feat_dist: 0.1220, mix.decode.loss_seg: 0.1916, mix.decode.acc_seg: 84.7897
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:19:03,517 - mmseg - INFO - Iter [2450/40000]	lr: 5.633e-05, eta: 15:42:53, time: 1.472, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2318, decode.acc_seg: 88.1056, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.2190, mix.decode.acc_seg: 84.3491
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:20:13,503 - mmseg - INFO - Iter [2500/40000]	lr: 5.625e-05, eta: 15:40:18, time: 1.400, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2307, decode.acc_seg: 87.2355, src.loss_imnet_feat_dist: 0.1275, mix.decode.loss_seg: 0.2014, mix.decode.acc_seg: 84.5165
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:20:56,755 - mmseg - INFO - Iter [1150/40000]	lr: 4.464e-05, eta: 1 day, 12:09:00, time: 3.362, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 20.3351, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.4788
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:21:25,824 - mmseg - INFO - Iter [2550/40000]	lr: 5.618e-05, eta: 15:38:20, time: 1.446, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2341, decode.acc_seg: 86.1992, src.loss_imnet_feat_dist: 0.1228, mix.decode.loss_seg: 0.2318, mix.decode.acc_seg: 83.3083
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:22:43,468 - mmseg - INFO - Iter [2600/40000]	lr: 5.610e-05, eta: 15:37:40, time: 1.553, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2435, decode.acc_seg: 87.0566, src.loss_imnet_feat_dist: 0.1277, mix.decode.loss_seg: 0.2572, mix.decode.acc_seg: 83.5081
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:23:59,823 - mmseg - INFO - Iter [2650/40000]	lr: 5.603e-05, eta: 15:36:41, time: 1.527, data_time: 0.013, memory: 9636, decode.loss_seg: 0.3050, decode.acc_seg: 85.5814, src.loss_imnet_feat_dist: 0.1371, mix.decode.loss_seg: 0.3036, mix.decode.acc_seg: 82.0459
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:25:12,282 - mmseg - INFO - Iter [2700/40000]	lr: 5.595e-05, eta: 15:34:47, time: 1.449, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2729, decode.acc_seg: 86.0630, src.loss_imnet_feat_dist: 0.1313, mix.decode.loss_seg: 0.2475, mix.decode.acc_seg: 82.5936
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:26:28,643 - mmseg - INFO - Iter [2750/40000]	lr: 5.588e-05, eta: 15:33:48, time: 1.527, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2286, decode.acc_seg: 87.2108, src.loss_imnet_feat_dist: 0.1295, mix.decode.loss_seg: 0.2217, mix.decode.acc_seg: 83.9949
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:27:39,430 - mmseg - INFO - Iter [2800/40000]	lr: 5.580e-05, eta: 15:31:34, time: 1.416, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2068, decode.acc_seg: 87.9996, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1954, mix.decode.acc_seg: 85.1445
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:28:51,587 - mmseg - INFO - Iter [2850/40000]	lr: 5.573e-05, eta: 15:29:40, time: 1.443, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2306, decode.acc_seg: 87.8867, src.loss_imnet_feat_dist: 0.1300, mix.decode.loss_seg: 0.2036, mix.decode.acc_seg: 85.9634
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:30:02,171 - mmseg - INFO - Iter [2900/40000]	lr: 5.565e-05, eta: 15:27:28, time: 1.412, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2081, decode.acc_seg: 87.8676, src.loss_imnet_feat_dist: 0.1339, mix.decode.loss_seg: 0.1984, mix.decode.acc_seg: 85.2750
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:31:13,952 - mmseg - INFO - Iter [2950/40000]	lr: 5.558e-05, eta: 15:25:33, time: 1.436, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1895, decode.acc_seg: 88.3853, src.loss_imnet_feat_dist: 0.1242, mix.decode.loss_seg: 0.2031, mix.decode.acc_seg: 85.1046
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:32:29,423 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-18 14:32:29,424 - mmseg - INFO - Iter [3000/40000]	lr: 5.550e-05, eta: 15:24:24, time: 1.509, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2207, decode.acc_seg: 87.8982, src.loss_imnet_feat_dist: 0.1274, mix.decode.loss_seg: 0.2014, mix.decode.acc_seg: 84.9357
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:33:48,842 - mmseg - INFO - Iter [3050/40000]	lr: 5.543e-05, eta: 15:24:03, time: 1.588, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2223, decode.acc_seg: 87.0205, src.loss_imnet_feat_dist: 0.1187, mix.decode.loss_seg: 0.2013, mix.decode.acc_seg: 85.2961
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:35:05,596 - mmseg - INFO - Iter [3100/40000]	lr: 5.535e-05, eta: 15:23:09, time: 1.535, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1985, decode.acc_seg: 87.8657, src.loss_imnet_feat_dist: 0.1212, mix.decode.loss_seg: 0.1871, mix.decode.acc_seg: 85.8569
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:36:24,021 - mmseg - INFO - Iter [3150/40000]	lr: 5.528e-05, eta: 15:22:33, time: 1.568, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1903, decode.acc_seg: 88.7624, src.loss_imnet_feat_dist: 0.1257, mix.decode.loss_seg: 0.1855, mix.decode.acc_seg: 87.4479
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:37:40,922 - mmseg - INFO - Iter [3200/40000]	lr: 5.520e-05, eta: 15:21:39, time: 1.538, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1871, decode.acc_seg: 88.6090, src.loss_imnet_feat_dist: 0.1246, mix.decode.loss_seg: 0.1794, mix.decode.acc_seg: 87.1029
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:38:59,018 - mmseg - INFO - Iter [3250/40000]	lr: 5.513e-05, eta: 15:20:57, time: 1.562, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1857, decode.acc_seg: 88.6291, src.loss_imnet_feat_dist: 0.1207, mix.decode.loss_seg: 0.1781, mix.decode.acc_seg: 86.5913
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:40:12,364 - mmseg - INFO - Iter [3300/40000]	lr: 5.505e-05, eta: 15:19:21, time: 1.467, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1839, decode.acc_seg: 89.4984, src.loss_imnet_feat_dist: 0.1218, mix.decode.loss_seg: 0.1740, mix.decode.acc_seg: 86.6519
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:40:33,712 - mmseg - INFO - Iter [1500/40000]	lr: 5.771e-05, eta: 1 day, 11:51:24, time: 3.341, data_time: 0.019, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 22.8320, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.3294
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:43:20,988 - mmseg - INFO - Iter [1550/40000]	lr: 5.768e-05, eta: 1 day, 11:48:27, time: 3.345, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 22.6789, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.8192
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:46:08,557 - mmseg - INFO - Iter [1600/40000]	lr: 5.760e-05, eta: 1 day, 11:45:38, time: 3.351, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 23.2515, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.1008
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:46:38,528 - mmseg - INFO - Iter [3550/40000]	lr: 5.468e-05, eta: 15:14:53, time: 1.575, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1936, decode.acc_seg: 88.7361, src.loss_imnet_feat_dist: 0.1311, mix.decode.loss_seg: 0.2346, mix.decode.acc_seg: 85.5553
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:47:55,929 - mmseg - INFO - Iter [3600/40000]	lr: 5.460e-05, eta: 15:13:58, time: 1.548, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1964, decode.acc_seg: 87.8757, src.loss_imnet_feat_dist: 0.1292, mix.decode.loss_seg: 0.2259, mix.decode.acc_seg: 84.1665
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:49:15,183 - mmseg - INFO - Iter [3650/40000]	lr: 5.453e-05, eta: 15:13:22, time: 1.585, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1926, decode.acc_seg: 88.0358, src.loss_imnet_feat_dist: 0.1219, mix.decode.loss_seg: 0.2184, mix.decode.acc_seg: 86.3205
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:50:32,628 - mmseg - INFO - Iter [3700/40000]	lr: 5.445e-05, eta: 15:12:27, time: 1.549, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2195, decode.acc_seg: 88.0009, src.loss_imnet_feat_dist: 0.1200, mix.decode.loss_seg: 0.2066, mix.decode.acc_seg: 86.6153
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:51:50,090 - mmseg - INFO - Iter [3750/40000]	lr: 5.438e-05, eta: 15:11:32, time: 1.549, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1911, decode.acc_seg: 88.7545, src.loss_imnet_feat_dist: 0.1213, mix.decode.loss_seg: 0.2265, mix.decode.acc_seg: 86.3925
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:53:06,744 - mmseg - INFO - Iter [3800/40000]	lr: 5.430e-05, eta: 15:10:28, time: 1.533, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1990, decode.acc_seg: 87.7607, src.loss_imnet_feat_dist: 0.1245, mix.decode.loss_seg: 0.2244, mix.decode.acc_seg: 85.1968
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:54:26,169 - mmseg - INFO - Iter [3850/40000]	lr: 5.423e-05, eta: 15:09:50, time: 1.589, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2850, decode.acc_seg: 84.9066, src.loss_imnet_feat_dist: 0.1296, mix.decode.loss_seg: 0.2747, mix.decode.acc_seg: 82.1799
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:55:41,661 - mmseg - INFO - Iter [3900/40000]	lr: 5.415e-05, eta: 15:08:34, time: 1.510, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1967, decode.acc_seg: 87.1516, src.loss_imnet_feat_dist: 0.1268, mix.decode.loss_seg: 0.2247, mix.decode.acc_seg: 84.3859
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 14:56:59,242 - mmseg - INFO - Iter [3950/40000]	lr: 5.408e-05, eta: 15:07:37, time: 1.552, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1890, decode.acc_seg: 89.4273, src.loss_imnet_feat_dist: 0.1262, mix.decode.loss_seg: 0.2051, mix.decode.acc_seg: 86.8790
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              [                                                  ] 0/500, elapsed: 0s, ETA:
[                                 ] 1/500, 0.8 task/s, elapsed: 1s, ETA:   621s
[                                 ] 2/500, 1.4 task/s, elapsed: 1s, ETA:   348s
[                                 ] 3/500, 1.9 task/s, elapsed: 2s, ETA:   257s
[                                 ] 4/500, 2.4 task/s, elapsed: 2s, ETA:   209s
[                                 ] 5/500, 2.7 task/s, elapsed: 2s, ETA:   181s
[                                 ] 6/500, 3.0 task/s, elapsed: 2s, ETA:   163s
[                                 ] 7/500, 3.3 task/s, elapsed: 2s, ETA:   149s
[                                 ] 8/500, 3.5 task/s, elapsed: 2s, ETA:   139s
[                                 ] 9/500, 3.7 task/s, elapsed: 2s, ETA:   131s
[                                ] 10/500, 3.9 task/s, elapsed: 3s, ETA:   125s
[                                ] 11/500, 4.1 task/s, elapsed: 3s, ETA:   119s
[                                ] 12/500, 4.2 task/s, elapsed: 3s, ETA:   115s
[                                ] 13/500, 4.4 task/s, elapsed: 3s, ETA:   111s
[                                ] 14/500, 4.5 task/s, elapsed: 3s, ETA:   108s
[                                ] 15/500, 4.6 task/s, elapsed: 3s, ETA:   105s
[>                               ] 16/500, 4.7 task/s, elapsed: 3s, ETA:   102s
[>                               ] 17/500, 4.8 task/s, elapsed: 4s, ETA:   100s
[>                               ] 18/500, 4.9 task/s, elapsed: 4s, ETA:    99s
[>                               ] 19/500, 5.0 task/s, elapsed: 4s, ETA:    97s
[>                               ] 20/500, 5.0 task/s, elapsed: 4s, ETA:    95s
[>                               ] 21/500, 5.1 task/s, elapsed: 4s, ETA:    94s
[>                               ] 22/500, 5.2 task/s, elapsed: 4s, ETA:    93s
[>                               ] 23/500, 5.2 task/s, elapsed: 4s, ETA:    92s
[>                               ] 24/500, 5.3 task/s, elapsed: 5s, ETA:    91s
[>                               ] 25/500, 5.3 task/s, elapsed: 5s, ETA:    90s
[>                               ] 26/500, 5.3 task/s, elapsed: 5s, ETA:    89s
[>                               ] 27/500, 5.4 task/s, elapsed: 5s, ETA:    88s
[>                               ] 28/500, 5.4 task/s, elapsed: 5s, ETA:    87s
[>                               ] 29/500, 5.4 task/s, elapsed: 5s, ETA:    86s
[>                               ] 30/500, 5.5 task/s, elapsed: 5s, ETA:    86s
[>                               ] 31/500, 5.5 task/s, elapsed: 6s, ETA:    85s
[>>                              ] 32/500, 5.5 task/s, elapsed: 6s, ETA:    84s
[>>                              ] 33/500, 5.6 task/s, elapsed: 6s, ETA:    84s
[>>                              ] 34/500, 5.6 task/s, elapsed: 6s, ETA:    83s
[>>                              ] 35/500, 5.6 task/s, elapsed: 6s, ETA:    83s
[>>                              ] 36/500, 5.6 task/s, elapsed: 6s, ETA:    82s
[>>                              ] 37/500, 5.7 task/s, elapsed: 7s, ETA:    82s
[>>                              ] 38/500, 5.7 task/s, elapsed: 7s, ETA:    81s
[>>                              ] 39/500, 5.7 task/s, elapsed: 7s, ETA:    80s
[>>                              ] 40/500, 5.8 task/s, elapsed: 7s, ETA:    80s
[>>                              ] 41/500, 5.8 task/s, elapsed: 7s, ETA:    79s
[>>                              ] 42/500, 5.8 task/s, elapsed: 7s, ETA:    79s
[>>                              ] 43/500, 5.8 task/s, elapsed: 7s, ETA:    78s
[>>                              ] 44/500, 5.9 task/s, elapsed: 7s, ETA:    78s
[>>                              ] 45/500, 5.9 task/s, elapsed: 8s, ETA:    77s
[>>                              ] 46/500, 5.9 task/s, elapsed: 8s, ETA:    77s
[>>>                             ] 47/500, 5.9 task/s, elapsed: 8s, ETA:    76s
[>>>                             ] 48/500, 6.0 task/s, elapsed: 8s, ETA:    76s
[>>>                             ] 49/500, 6.0 task/s, elapsed: 8s, ETA:    75s
[>>>                             ] 50/500, 6.0 task/s, elapsed: 8s, ETA:    75s
[>>>                             ] 51/500, 6.0 task/s, elapsed: 8s, ETA:    75s
[>>>                             ] 52/500, 6.0 task/s, elapsed: 9s, ETA:    74s
[>>>                             ] 53/500, 6.1 task/s, elapsed: 9s, ETA:    74s
[>>>                             ] 54/500, 6.1 task/s, elapsed: 9s, ETA:    73s
[>>>                             ] 55/500, 6.1 task/s, elapsed: 9s, ETA:    73s
[>>>                             ] 56/500, 6.1 task/s, elapsed: 9s, ETA:    73s
[>>>                             ] 57/500, 6.1 task/s, elapsed: 9s, ETA:    72s
[>>>                             ] 58/500, 6.1 task/s, elapsed: 9s, ETA:    72s
[>>>                            ] 59/500, 6.1 task/s, elapsed: 10s, ETA:    72s
[>>>                            ] 60/500, 6.2 task/s, elapsed: 10s, ETA:    71s
[>>>                            ] 61/500, 6.2 task/s, elapsed: 10s, ETA:    71s
[>>>                            ] 62/500, 6.2 task/s, elapsed: 10s, ETA:    71s
[>>>                            ] 63/500, 6.2 task/s, elapsed: 10s, ETA:    70s
[>>>                            ] 64/500, 6.2 task/s, elapsed: 10s, ETA:    70s
[>>>>                           ] 65/500, 6.2 task/s, elapsed: 10s, ETA:    70s
[>>>>                           ] 66/500, 6.2 task/s, elapsed: 11s, ETA:    70s
[>>>>                           ] 67/500, 6.3 task/s, elapsed: 11s, ETA:    69s
[>>>>                           ] 68/500, 6.3 task/s, elapsed: 11s, ETA:    69s
[>>>>                           ] 69/500, 6.3 task/s, elapsed: 11s, ETA:    69s
[>>>>                           ] 70/500, 6.3 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 71/500, 6.3 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 72/500, 6.3 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 73/500, 6.3 task/s, elapsed: 12s, ETA:    68s
[>>>>                           ] 74/500, 6.3 task/s, elapsed: 12s, ETA:    67s
[>>>>                           ] 75/500, 6.3 task/s, elapsed: 12s, ETA:    67s
[>>>>                           ] 76/500, 6.3 task/s, elapsed: 12s, ETA:    67s
[>>>>                           ] 77/500, 6.4 task/s, elapsed: 12s, ETA:    67s
[>>>>                           ] 78/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 79/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 80/500, 6.4 task/s, elapsed: 13s, ETA:    66s
[>>>>>                          ] 81/500, 6.4 task/s, elapsed: 13s, ETA:    66s
[>>>>>                          ] 82/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 83/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 84/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 85/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 86/500, 6.4 task/s, elapsed: 13s, ETA:    64s
[>>>>>                          ] 87/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 88/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 89/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 90/500, 6.5 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 91/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 92/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 93/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 94/500, 6.5 task/s, elapsed: 15s, ETA:    63s
[>>>>>                          ] 95/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>                          ] 96/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>>                         ] 97/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>>                         ] 98/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>>                         ] 99/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>>                        ] 100/500, 6.5 task/s, elapsed: 15s, ETA:    61s
[>>>>>>                        ] 101/500, 6.5 task/s, elapsed: 16s, ETA:    61s
[>>>>>>                        ] 102/500, 6.5 task/s, elapsed: 16s, ETA:    61s
[>>>>>>                        ] 103/500, 6.5 task/s, elapsed: 16s, ETA:    61s
[>>>>>>                        ] 104/500, 6.5 task/s, elapsed: 16s, ETA:    61s
[>>>>>>                        ] 105/500, 6.5 task/s, elapsed: 16s, ETA:    61s
[>>>>>>                        ] 106/500, 6.5 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 107/500, 6.5 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 108/500, 6.5 task/s, elapsed: 17s, ETA:    60s
[>>>>>>                        ] 109/500, 6.5 task/s, elapsed: 17s, ETA:    60s
[>>>>>>                        ] 110/500, 6.5 task/s, elapsed: 17s, ETA:    60s
[>>>>>>                        ] 111/500, 6.5 task/s, elapsed: 17s, ETA:    60s
[>>>>>>                        ] 112/500, 6.5 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 113/500, 6.5 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 114/500, 6.5 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 115/500, 6.5 task/s, elapsed: 18s, ETA:    59s
[>>>>>>                        ] 116/500, 6.5 task/s, elapsed: 18s, ETA:    59s
[>>>>>>>                       ] 117/500, 6.5 task/s, elapsed: 18s, ETA:    59s
[>>>>>>>                       ] 118/500, 6.5 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 119/500, 6.6 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 120/500, 6.6 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 121/500, 6.6 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 122/500, 6.6 task/s, elapsed: 19s, ETA:    58s
[>>>>>>>                       ] 123/500, 6.6 task/s, elapsed: 19s, ETA:    57s
[>>>>>>>                       ] 124/500, 6.6 task/s, elapsed: 19s, ETA:    57s
[>>>>>>>                       ] 125/500, 6.6 task/s, elapsed: 19s, ETA:    57s
[>>>>>>>                       ] 126/500, 6.6 task/s, elapsed: 19s, ETA:    57s
[>>>>>>>                       ] 127/500, 6.6 task/s, elapsed: 19s, ETA:    57s
[>>>>>>>                       ] 128/500, 6.6 task/s, elapsed: 19s, ETA:    56s
[>>>>>>>                       ] 129/500, 6.6 task/s, elapsed: 20s, ETA:    56s
[>>>>>>>                       ] 130/500, 6.6 task/s, elapsed: 20s, ETA:    56s
[>>>>>>>                       ] 131/500, 6.6 task/s, elapsed: 20s, ETA:    56s
[>>>>>>>                       ] 132/500, 6.6 task/s, elapsed: 20s, ETA:    56s
[>>>>>>>                       ] 133/500, 6.6 task/s, elapsed: 20s, ETA:    56s
[>>>>>>>>                      ] 134/500, 6.6 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>>                      ] 135/500, 6.6 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>>                      ] 136/500, 6.6 task/s, elapsed: 21s, ETA:    55s
[>>>>>>>>                      ] 137/500, 6.6 task/s, elapsed: 21s, ETA:    55s
[>>>>>>>>                      ] 138/500, 6.6 task/s, elapsed: 21s, ETA:    55s
[>>>>>>>>                      ] 139/500, 6.6 task/s, elapsed: 21s, ETA:    55s
[>>>>>>>>                      ] 140/500, 6.6 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 141/500, 6.6 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 142/500, 6.6 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 143/500, 6.6 task/s, elapsed: 22s, ETA:    54s
[>>>>>>>>                      ] 144/500, 6.6 task/s, elapsed: 22s, ETA:    54s
[>>>>>>>>                      ] 145/500, 6.6 task/s, elapsed: 22s, ETA:    54s
[>>>>>>>>                      ] 146/500, 6.6 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 147/500, 6.6 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 148/500, 6.6 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 149/500, 6.6 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>>                     ] 150/500, 6.6 task/s, elapsed: 23s, ETA:    53s
[>>>>>>>>>                     ] 151/500, 6.6 task/s, elapsed: 23s, ETA:    53s
[>>>>>>>>>                     ] 152/500, 6.6 task/s, elapsed: 23s, ETA:    53s
[>>>>>>>>>                     ] 153/500, 6.6 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 154/500, 6.6 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 155/500, 6.6 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 156/500, 6.6 task/s, elapsed: 24s, ETA:    52s
[>>>>>>>>>                     ] 157/500, 6.6 task/s, elapsed: 24s, ETA:    52s
[>>>>>>>>>                     ] 158/500, 6.6 task/s, elapsed: 24s, ETA:    52s
[>>>>>>>>>                     ] 159/500, 6.6 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 160/500, 6.6 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 161/500, 6.6 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 162/500, 6.6 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 163/500, 6.6 task/s, elapsed: 25s, ETA:    51s
[>>>>>>>>>                     ] 164/500, 6.6 task/s, elapsed: 25s, ETA:    51s
[>>>>>>>>>                     ] 165/500, 6.6 task/s, elapsed: 25s, ETA:    51s
[>>>>>>>>>                     ] 166/500, 6.6 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>>                    ] 167/500, 6.6 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>>                    ] 168/500, 6.6 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>>                    ] 169/500, 6.6 task/s, elapsed: 26s, ETA:    50s
[>>>>>>>>>>                    ] 170/500, 6.6 task/s, elapsed: 26s, ETA:    50s
[>>>>>>>>>>                    ] 171/500, 6.6 task/s, elapsed: 26s, ETA:    50s
[>>>>>>>>>>                    ] 172/500, 6.6 task/s, elapsed: 26s, ETA:    50s
[>>>>>>>>>>                    ] 173/500, 6.6 task/s, elapsed: 26s, ETA:    49s
[>>>>>>>>>>                    ] 174/500, 6.6 task/s, elapsed: 26s, ETA:    49s
[>>>>>>>>>>                    ] 175/500, 6.6 task/s, elapsed: 26s, ETA:    49s
[>>>>>>>>>>                    ] 176/500, 6.6 task/s, elapsed: 27s, ETA:    49s
[>>>>>>>>>>                    ] 177/500, 6.6 task/s, elapsed: 27s, ETA:    49s
[>>>>>>>>>>                    ] 178/500, 6.6 task/s, elapsed: 27s, ETA:    49s
[>>>>>>>>>>                    ] 179/500, 6.6 task/s, elapsed: 27s, ETA:    48s
[>>>>>>>>>>                    ] 180/500, 6.6 task/s, elapsed: 27s, ETA:    48s
[>>>>>>>>>>                    ] 181/500, 6.6 task/s, elapsed: 27s, ETA:    48s
[>>>>>>>>>>                    ] 182/500, 6.6 task/s, elapsed: 27s, ETA:    48s
[>>>>>>>>>>                    ] 183/500, 6.6 task/s, elapsed: 28s, ETA:    48s
[>>>>>>>>>>>                   ] 184/500, 6.6 task/s, elapsed: 28s, ETA:    48s
[>>>>>>>>>>>                   ] 185/500, 6.6 task/s, elapsed: 28s, ETA:    48s
[>>>>>>>>>>>                   ] 186/500, 6.6 task/s, elapsed: 28s, ETA:    47s
[>>>>>>>>>>>                   ] 187/500, 6.6 task/s, elapsed: 28s, ETA:    47s
[>>>>>>>>>>>                   ] 188/500, 6.6 task/s, elapsed: 28s, ETA:    47s
[>>>>>>>>>>>                   ] 189/500, 6.6 task/s, elapsed: 29s, ETA:    47s
[>>>>>>>>>>>                   ] 190/500, 6.6 task/s, elapsed: 29s, ETA:    47s
[>>>>>>>>>>>                   ] 191/500, 6.6 task/s, elapsed: 29s, ETA:    47s
[>>>>>>>>>>>                   ] 192/500, 6.6 task/s, elapsed: 29s, ETA:    47s
[>>>>>>>>>>>                   ] 193/500, 6.6 task/s, elapsed: 29s, ETA:    46s
[>>>>>>>>>>>                   ] 194/500, 6.6 task/s, elapsed: 29s, ETA:    46s
[>>>>>>>>>>>                   ] 195/500, 6.6 task/s, elapsed: 29s, ETA:    46s
[>>>>>>>>>>>                   ] 196/500, 6.6 task/s, elapsed: 30s, ETA:    46s
[>>>>>>>>>>>                   ] 197/500, 6.6 task/s, elapsed: 30s, ETA:    46s
[>>>>>>>>>>>                   ] 198/500, 6.6 task/s, elapsed: 30s, ETA:    46s
[>>>>>>>>>>>                   ] 199/500, 6.6 task/s, elapsed: 30s, ETA:    45s
[>>>>>>>>>>>>                  ] 200/500, 6.6 task/s, elapsed: 30s, ETA:    45s
[>>>>>>>>>>>>                  ] 201/500, 6.6 task/s, elapsed: 30s, ETA:    45s
[>>>>>>>>>>>>                  ] 202/500, 6.6 task/s, elapsed: 31s, ETA:    45s
[>>>>>>>>>>>>                  ] 203/500, 6.6 task/s, elapsed: 31s, ETA:    45s
[>>>>>>>>>>>>                  ] 204/500, 6.6 task/s, elapsed: 31s, ETA:    45s
[>>>>>>>>>>>>                  ] 205/500, 6.6 task/s, elapsed: 31s, ETA:    45s
[>>>>>>>>>>>>                  ] 206/500, 6.6 task/s, elapsed: 31s, ETA:    44s
[>>>>>>>>>>>>                  ] 207/500, 6.6 task/s, elapsed: 31s, ETA:    44s
[>>>>>>>>>>>>                  ] 208/500, 6.6 task/s, elapsed: 31s, ETA:    44s
[>>>>>>>>>>>>                  ] 209/500, 6.6 task/s, elapsed: 32s, ETA:    44s
[>>>>>>>>>>>>                  ] 210/500, 6.6 task/s, elapsed: 32s, ETA:    44s
[>>>>>>>>>>>>                  ] 211/500, 6.6 task/s, elapsed: 32s, ETA:    44s
[>>>>>>>>>>>>                  ] 212/500, 6.6 task/s, elapsed: 32s, ETA:    43s
[>>>>>>>>>>>>                  ] 213/500, 6.6 task/s, elapsed: 32s, ETA:    43s
[>>>>>>>>>>>>                  ] 214/500, 6.6 task/s, elapsed: 32s, ETA:    43s
[>>>>>>>>>>>>                  ] 215/500, 6.6 task/s, elapsed: 32s, ETA:    43s
[>>>>>>>>>>>>                  ] 216/500, 6.6 task/s, elapsed: 33s, ETA:    43s
[>>>>>>>>>>>>>                 ] 217/500, 6.6 task/s, elapsed: 33s, ETA:    43s
[>>>>>>>>>>>>>                 ] 218/500, 6.6 task/s, elapsed: 33s, ETA:    43s
[>>>>>>>>>>>>>                 ] 219/500, 6.6 task/s, elapsed: 33s, ETA:    42s
[>>>>>>>>>>>>>                 ] 220/500, 6.6 task/s, elapsed: 33s, ETA:    42s
[>>>>>>>>>>>>>                 ] 221/500, 6.6 task/s, elapsed: 33s, ETA:    42s
[>>>>>>>>>>>>>                 ] 222/500, 6.6 task/s, elapsed: 33s, ETA:    42s
[>>>>>>>>>>>>>                 ] 223/500, 6.6 task/s, elapsed: 34s, ETA:    42s
[>>>>>>>>>>>>>                 ] 224/500, 6.6 task/s, elapsed: 34s, ETA:    42s
[>>>>>>>>>>>>>                 ] 225/500, 6.7 task/s, elapsed: 34s, ETA:    41s
[>>>>>>>>>>>>>                 ] 226/500, 6.7 task/s, elapsed: 34s, ETA:    41s
[>>>>>>>>>>>>>                 ] 227/500, 6.7 task/s, elapsed: 34s, ETA:    41s
[>>>>>>>>>>>>>                 ] 228/500, 6.7 task/s, elapsed: 34s, ETA:    41s
[>>>>>>>>>>>>>                 ] 229/500, 6.7 task/s, elapsed: 34s, ETA:    41s
[>>>>>>>>>>>>>                 ] 230/500, 6.7 task/s, elapsed: 35s, ETA:    41s
[>>>>>>>>>>>>>                 ] 231/500, 6.7 task/s, elapsed: 35s, ETA:    40s
[>>>>>>>>>>>>>                 ] 232/500, 6.7 task/s, elapsed: 35s, ETA:    40s
[>>>>>>>>>>>>>                 ] 233/500, 6.7 task/s, elapsed: 35s, ETA:    40s
[>>>>>>>>>>>>>>                ] 234/500, 6.7 task/s, elapsed: 35s, ETA:    40s
[>>>>>>>>>>>>>>                ] 235/500, 6.7 task/s, elapsed: 35s, ETA:    40s
[>>>>>>>>>>>>>>                ] 236/500, 6.7 task/s, elapsed: 35s, ETA:    40s
[>>>>>>>>>>>>>>                ] 237/500, 6.7 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 238/500, 6.7 task/s, elapsed: 36s, ETA:    39s
[>>>>>>>>>>>>>>                ] 239/500, 6.7 task/s, elapsed: 36s, ETA:    39s
[>>>>>>>>>>>>>>                ] 240/500, 6.7 task/s, elapsed: 36s, ETA:    39s
[>>>>>>>>>>>>>>                ] 241/500, 6.7 task/s, elapsed: 36s, ETA:    39s
[>>>>>>>>>>>>>>                ] 242/500, 6.7 task/s, elapsed: 36s, ETA:    39s
[>>>>>>>>>>>>>>                ] 243/500, 6.7 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 244/500, 6.7 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 245/500, 6.7 task/s, elapsed: 37s, ETA:    38s
[>>>>>>>>>>>>>>                ] 246/500, 6.7 task/s, elapsed: 37s, ETA:    38s
[>>>>>>>>>>>>>>                ] 247/500, 6.7 task/s, elapsed: 37s, ETA:    38s
[>>>>>>>>>>>>>>                ] 248/500, 6.7 task/s, elapsed: 37s, ETA:    38s
[>>>>>>>>>>>>>>                ] 249/500, 6.7 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 250/500, 6.7 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 251/500, 6.7 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 252/500, 6.7 task/s, elapsed: 38s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 253/500, 6.7 task/s, elapsed: 38s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 254/500, 6.7 task/s, elapsed: 38s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 255/500, 6.7 task/s, elapsed: 38s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 256/500, 6.7 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 257/500, 6.7 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 258/500, 6.7 task/s, elapsed: 39s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 259/500, 6.7 task/s, elapsed: 39s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 260/500, 6.7 task/s, elapsed: 39s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 261/500, 6.7 task/s, elapsed: 39s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 262/500, 6.7 task/s, elapsed: 39s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 263/500, 6.7 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 264/500, 6.7 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 265/500, 6.7 task/s, elapsed: 40s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 266/500, 6.7 task/s, elapsed: 40s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 267/500, 6.7 task/s, elapsed: 40s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 268/500, 6.7 task/s, elapsed: 40s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 269/500, 6.7 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 270/500, 6.7 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 271/500, 6.7 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 272/500, 6.7 task/s, elapsed: 41s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 273/500, 6.7 task/s, elapsed: 41s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 274/500, 6.7 task/s, elapsed: 41s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 275/500, 6.7 task/s, elapsed: 41s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 276/500, 6.7 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 277/500, 6.7 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 278/500, 6.7 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 279/500, 6.7 task/s, elapsed: 42s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 280/500, 6.7 task/s, elapsed: 42s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 281/500, 6.7 task/s, elapsed: 42s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 282/500, 6.7 task/s, elapsed: 42s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 283/500, 6.7 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 284/500, 6.7 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 285/500, 6.7 task/s, elapsed: 43s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 286/500, 6.7 task/s, elapsed: 43s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 287/500, 6.7 task/s, elapsed: 43s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 288/500, 6.7 task/s, elapsed: 43s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 289/500, 6.7 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 290/500, 6.7 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 291/500, 6.7 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 292/500, 6.7 task/s, elapsed: 44s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 293/500, 6.7 task/s, elapsed: 44s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 294/500, 6.7 task/s, elapsed: 44s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 295/500, 6.7 task/s, elapsed: 44s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 296/500, 6.7 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 297/500, 6.7 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 298/500, 6.7 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 299/500, 6.7 task/s, elapsed: 45s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 300/500, 6.7 task/s, elapsed: 45s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 301/500, 6.7 task/s, elapsed: 45s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 302/500, 6.7 task/s, elapsed: 45s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 303/500, 6.7 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 304/500, 6.7 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 305/500, 6.7 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 306/500, 6.7 task/s, elapsed: 46s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 307/500, 6.7 task/s, elapsed: 46s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 308/500, 6.7 task/s, elapsed: 46s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 309/500, 6.7 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 310/500, 6.7 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 311/500, 6.7 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 312/500, 6.7 task/s, elapsed: 47s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 313/500, 6.7 task/s, elapsed: 47s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 314/500, 6.7 task/s, elapsed: 47s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 315/500, 6.7 task/s, elapsed: 47s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 316/500, 6.7 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 317/500, 6.7 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 318/500, 6.7 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 319/500, 6.7 task/s, elapsed: 48s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 320/500, 6.7 task/s, elapsed: 48s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 321/500, 6.7 task/s, elapsed: 48s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 322/500, 6.7 task/s, elapsed: 48s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 323/500, 6.7 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 324/500, 6.7 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 325/500, 6.7 task/s, elapsed: 49s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 326/500, 6.7 task/s, elapsed: 49s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 327/500, 6.7 task/s, elapsed: 49s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 328/500, 6.7 task/s, elapsed: 49s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 329/500, 6.7 task/s, elapsed: 49s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 330/500, 6.7 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 331/500, 6.7 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 332/500, 6.7 task/s, elapsed: 50s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 333/500, 6.7 task/s, elapsed: 50s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 334/500, 6.7 task/s, elapsed: 50s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 335/500, 6.7 task/s, elapsed: 50s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 336/500, 6.7 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 337/500, 6.7 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 338/500, 6.7 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 339/500, 6.7 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 340/500, 6.7 task/s, elapsed: 51s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 341/500, 6.7 task/s, elapsed: 51s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 342/500, 6.7 task/s, elapsed: 51s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 343/500, 6.7 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 344/500, 6.7 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 345/500, 6.7 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 346/500, 6.7 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 347/500, 6.7 task/s, elapsed: 52s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 348/500, 6.7 task/s, elapsed: 52s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 349/500, 6.7 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 350/500, 6.7 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 351/500, 6.7 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 352/500, 6.7 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 353/500, 6.7 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 354/500, 6.7 task/s, elapsed: 53s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 355/500, 6.7 task/s, elapsed: 53s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 356/500, 6.7 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 357/500, 6.7 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 358/500, 6.7 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 359/500, 6.7 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 360/500, 6.7 task/s, elapsed: 54s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 361/500, 6.7 task/s, elapsed: 54s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 362/500, 6.7 task/s, elapsed: 54s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 363/500, 6.7 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 364/500, 6.7 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 365/500, 6.7 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 366/500, 6.7 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 367/500, 6.7 task/s, elapsed: 55s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 368/500, 6.7 task/s, elapsed: 55s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 369/500, 6.7 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 370/500, 6.7 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 371/500, 6.7 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 372/500, 6.7 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 373/500, 6.7 task/s, elapsed: 56s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 374/500, 6.7 task/s, elapsed: 56s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 375/500, 6.7 task/s, elapsed: 56s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 376/500, 6.7 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 377/500, 6.7 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 378/500, 6.7 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 379/500, 6.7 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 380/500, 6.7 task/s, elapsed: 57s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 381/500, 6.7 task/s, elapsed: 57s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 382/500, 6.7 task/s, elapsed: 57s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 383/500, 6.7 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 384/500, 6.7 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 385/500, 6.7 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 386/500, 6.7 task/s, elapsed: 58s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 387/500, 6.7 task/s, elapsed: 58s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 388/500, 6.7 task/s, elapsed: 58s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 389/500, 6.7 task/s, elapsed: 58s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 390/500, 6.7 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 391/500, 6.7 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 392/500, 6.7 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 393/500, 6.7 task/s, elapsed: 59s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 394/500, 6.7 task/s, elapsed: 59s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 395/500, 6.7 task/s, elapsed: 59s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 396/500, 6.7 task/s, elapsed: 59s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 397/500, 6.7 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 398/500, 6.7 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 399/500, 6.7 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 400/500, 6.7 task/s, elapsed: 60s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 401/500, 6.7 task/s, elapsed: 60s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 402/500, 6.7 task/s, elapsed: 60s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 403/500, 6.7 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 404/500, 6.7 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 405/500, 6.7 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 406/500, 6.7 task/s, elapsed: 61s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 407/500, 6.7 task/s, elapsed: 61s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 408/500, 6.7 task/s, elapsed: 61s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 409/500, 6.7 task/s, elapsed: 61s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 410/500, 6.7 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 411/500, 6.7 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 412/500, 6.7 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 413/500, 6.7 task/s, elapsed: 62s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 414/500, 6.7 task/s, elapsed: 62s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 415/500, 6.7 task/s, elapsed: 62s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 416/500, 6.7 task/s, elapsed: 62s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 417/500, 6.7 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 418/500, 6.7 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 419/500, 6.7 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 420/500, 6.7 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 421/500, 6.7 task/s, elapsed: 63s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 422/500, 6.7 task/s, elapsed: 63s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 423/500, 6.7 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 424/500, 6.7 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 425/500, 6.7 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 426/500, 6.7 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 427/500, 6.7 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 428/500, 6.7 task/s, elapsed: 64s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 429/500, 6.7 task/s, elapsed: 64s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 430/500, 6.7 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 431/500, 6.7 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 432/500, 6.7 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 433/500, 6.7 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 434/500, 6.7 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 435/500, 6.7 task/s, elapsed: 65s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 436/500, 6.7 task/s, elapsed: 65s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 437/500, 6.7 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 438/500, 6.7 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 439/500, 6.7 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 440/500, 6.7 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 441/500, 6.7 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 442/500, 6.7 task/s, elapsed: 66s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 443/500, 6.7 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 444/500, 6.7 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 445/500, 6.7 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 446/500, 6.7 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 447/500, 6.7 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 448/500, 6.7 task/s, elapsed: 67s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 449/500, 6.7 task/s, elapsed: 67s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 450/500, 6.7 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 451/500, 6.7 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 452/500, 6.7 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 453/500, 6.7 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 454/500, 6.7 task/s, elapsed: 68s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 455/500, 6.7 task/s, elapsed: 68s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 456/500, 6.7 task/s, elapsed: 68s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 457/500, 6.7 task/s, elapsed: 68s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 458/500, 6.7 task/s, elapsed: 68s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 459/500, 6.7 task/s, elapsed: 68s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 460/500, 6.7 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 461/500, 6.7 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 462/500, 6.7 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 463/500, 6.7 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 464/500, 6.7 task/s, elapsed: 69s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 465/500, 6.7 task/s, elapsed: 69s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 466/500, 6.7 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 467/500, 6.7 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 468/500, 6.7 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 469/500, 6.7 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 470/500, 6.7 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 471/500, 6.7 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 472/500, 6.7 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 473/500, 6.7 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 474/500, 6.7 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 475/500, 6.7 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 476/500, 6.7 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 477/500, 6.7 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 478/500, 6.7 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 479/500, 6.7 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 480/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 481/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 482/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 483/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 484/500, 6.7 task/s, elapsed: 72s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 485/500, 6.7 task/s, elapsed: 72s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 486/500, 6.7 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 487/500, 6.7 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 488/500, 6.7 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 489/500, 6.7 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 490/500, 6.7 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 491/500, 6.7 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 492/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 493/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 494/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 495/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 496/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 497/500, 6.7 task/s, elapsed: 74s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 498/500, 6.7 task/s, elapsed: 74s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 499/500, 6.7 task/s, elapsed: 74s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 500/500, 6.7 task/s, elapsed: 75s, ETA:     0s2022-04-18 15:00:11,486 - mmseg - INFO - per class results:
2022-04-18 15:00:11,489 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|      road     | 87.33 | 90.43 |
|    sidewalk   | 43.84 | 74.31 |
|    building   | 84.56 |  93.9 |
|      wall     |  4.26 |  4.45 |
|     fence     |  1.16 |  1.24 |
|      pole     | 38.45 |  46.5 |
| traffic light | 44.39 | 52.79 |
|  traffic sign | 38.46 | 44.77 |
|   vegetation  | 84.26 | 92.16 |
|    terrain    |  0.0  |  0.0  |
|      sky      | 84.26 | 98.76 |
|     person    | 62.12 | 88.88 |
|     rider     | 30.49 | 51.73 |
|      car      | 80.33 |  97.8 |
|     truck     |  0.0  |  0.0  |
|      bus      | 50.55 | 73.91 |
|     train     |  0.0  |  0.0  |
|   motorcycle  | 37.42 | 43.33 |
|    bicycle    | 46.03 | 49.67 |
+---------------+-------+-------+
2022-04-18 15:00:11,489 - mmseg - INFO - Summary:
2022-04-18 15:00:11,489 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 87.37 | 43.05 | 52.88 |
+-------+-------+-------+
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:00:11,503 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-18 15:00:11,503 - mmseg - INFO - Iter [500/40000]	lr: 5.400e-05, eta: 15:06:02, time: 1.467, data_time: 0.013, memory: 9636, aAcc: 0.8737, mIoU: 0.4305, mAcc: 0.5288, IoU.road: 0.8733, IoU.sidewalk: 0.4384, IoU.building: 0.8456, IoU.wall: 0.0426, IoU.fence: 0.0116, IoU.pole: 0.3845, IoU.traffic light: 0.4439, IoU.traffic sign: 0.3846, IoU.vegetation: 0.8426, IoU.terrain: 0.0000, IoU.sky: 0.8426, IoU.person: 0.6212, IoU.rider: 0.3049, IoU.car: 0.8033, IoU.truck: 0.0000, IoU.bus: 0.5055, IoU.train: 0.0000, IoU.motorcycle: 0.3742, IoU.bicycle: 0.4603, Acc.road: 0.9043, Acc.sidewalk: 0.7431, Acc.building: 0.9390, Acc.wall: 0.0445, Acc.fence: 0.0124, Acc.pole: 0.4650, Acc.traffic light: 0.5279, Acc.traffic sign: 0.4477, Acc.vegetation: 0.9216, Acc.terrain: 0.0000, Acc.sky: 0.9876, Acc.person: 0.8888, Acc.rider: 0.5173, Acc.car: 0.9780, Acc.truck: 0.0000, Acc.bus: 0.7391, Acc.train: 0.0000, Acc.motorcycle: 0.4333, Acc.bicycle: 0.4967, decode.loss_seg: 0.1904, decode.acc_seg: 89.0001, src.loss_imnet_feat_dist: 0.1210, mix.decode.loss_seg: 0.2166, mix.decode.acc_seg: 85.3231
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:01:28,736 - mmseg - INFO - Iter [4050/40000]	lr: 5.393e-05, eta: 15:22:38, time: 3.923, data_time: 2.393, memory: 9636, decode.loss_seg: 0.1855, decode.acc_seg: 88.6723, src.loss_imnet_feat_dist: 0.1204, mix.decode.loss_seg: 0.1958, mix.decode.acc_seg: 86.7658
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:02:45,562 - mmseg - INFO - Iter [4100/40000]	lr: 5.385e-05, eta: 15:21:19, time: 1.537, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1869, decode.acc_seg: 87.8737, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.2045, mix.decode.acc_seg: 86.1191
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:04:00,013 - mmseg - INFO - Iter [4150/40000]	lr: 5.378e-05, eta: 15:19:40, time: 1.489, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1771, decode.acc_seg: 89.3847, src.loss_imnet_feat_dist: 0.1211, mix.decode.loss_seg: 0.1712, mix.decode.acc_seg: 88.0373
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:05:17,977 - mmseg - INFO - Iter [4200/40000]	lr: 5.370e-05, eta: 15:18:32, time: 1.559, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1679, decode.acc_seg: 89.6260, src.loss_imnet_feat_dist: 0.1228, mix.decode.loss_seg: 0.1872, mix.decode.acc_seg: 87.5052
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:06:33,203 - mmseg - INFO - Iter [4250/40000]	lr: 5.363e-05, eta: 15:17:00, time: 1.504, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1767, decode.acc_seg: 88.1487, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.2149, mix.decode.acc_seg: 86.3924
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:07:44,842 - mmseg - INFO - Iter [4300/40000]	lr: 5.355e-05, eta: 15:14:59, time: 1.433, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1749, decode.acc_seg: 88.7845, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.2062, mix.decode.acc_seg: 87.1925
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:09:01,518 - mmseg - INFO - Iter [4350/40000]	lr: 5.348e-05, eta: 15:13:40, time: 1.534, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1852, decode.acc_seg: 88.4423, src.loss_imnet_feat_dist: 0.1220, mix.decode.loss_seg: 0.2509, mix.decode.acc_seg: 85.5898
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:10:11,480 - mmseg - INFO - Iter [4400/40000]	lr: 5.340e-05, eta: 15:11:27, time: 1.399, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1837, decode.acc_seg: 88.9953, src.loss_imnet_feat_dist: 0.1183, mix.decode.loss_seg: 0.2083, mix.decode.acc_seg: 86.6039
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:11:29,840 - mmseg - INFO - Iter [4450/40000]	lr: 5.333e-05, eta: 15:10:23, time: 1.567, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1798, decode.acc_seg: 88.3152, src.loss_imnet_feat_dist: 0.1254, mix.decode.loss_seg: 0.1780, mix.decode.acc_seg: 86.8217
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:12:47,406 - mmseg - INFO - Iter [4500/40000]	lr: 5.325e-05, eta: 15:09:12, time: 1.551, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1869, decode.acc_seg: 88.5364, src.loss_imnet_feat_dist: 0.1214, mix.decode.loss_seg: 0.2302, mix.decode.acc_seg: 85.0516
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:14:01,968 - mmseg - INFO - Iter [4550/40000]	lr: 5.318e-05, eta: 15:07:37, time: 1.491, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1938, decode.acc_seg: 88.5390, src.loss_imnet_feat_dist: 0.1257, mix.decode.loss_seg: 0.1922, mix.decode.acc_seg: 87.0313
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:15:18,936 - mmseg - INFO - Iter [4600/40000]	lr: 5.310e-05, eta: 15:06:22, time: 1.539, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1733, decode.acc_seg: 88.4705, src.loss_imnet_feat_dist: 0.1227, mix.decode.loss_seg: 0.1808, mix.decode.acc_seg: 86.7422
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:16:37,004 - mmseg - INFO - Iter [4650/40000]	lr: 5.303e-05, eta: 15:05:15, time: 1.561, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1793, decode.acc_seg: 88.7598, src.loss_imnet_feat_dist: 0.1205, mix.decode.loss_seg: 0.1998, mix.decode.acc_seg: 87.3462
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:17:52,783 - mmseg - INFO - Iter [4700/40000]	lr: 5.295e-05, eta: 15:03:50, time: 1.516, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1794, decode.acc_seg: 88.6930, src.loss_imnet_feat_dist: 0.1236, mix.decode.loss_seg: 0.2018, mix.decode.acc_seg: 87.0141
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:19:10,391 - mmseg - INFO - Iter [4750/40000]	lr: 5.288e-05, eta: 15:02:39, time: 1.552, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1753, decode.acc_seg: 88.7544, src.loss_imnet_feat_dist: 0.1184, mix.decode.loss_seg: 0.1941, mix.decode.acc_seg: 87.7277
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:20:26,890 - mmseg - INFO - Iter [4800/40000]	lr: 5.280e-05, eta: 15:01:20, time: 1.530, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1815, decode.acc_seg: 89.5935, src.loss_imnet_feat_dist: 0.1168, mix.decode.loss_seg: 0.2307, mix.decode.acc_seg: 87.1715
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:21:39,982 - mmseg - INFO - Iter [4850/40000]	lr: 5.273e-05, eta: 14:59:36, time: 1.462, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1657, decode.acc_seg: 89.3612, src.loss_imnet_feat_dist: 0.1201, mix.decode.loss_seg: 0.1751, mix.decode.acc_seg: 88.9059
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:22:56,308 - mmseg - INFO - Iter [4900/40000]	lr: 5.265e-05, eta: 14:58:16, time: 1.527, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1830, decode.acc_seg: 87.8818, src.loss_imnet_feat_dist: 0.1177, mix.decode.loss_seg: 0.1995, mix.decode.acc_seg: 86.8103
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:24:10,159 - mmseg - INFO - Iter [4950/40000]	lr: 5.258e-05, eta: 14:56:38, time: 1.477, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2102, decode.acc_seg: 87.4430, src.loss_imnet_feat_dist: 0.1196, mix.decode.loss_seg: 0.1822, mix.decode.acc_seg: 87.5216
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:25:27,294 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-18 15:25:27,294 - mmseg - INFO - Iter [5000/40000]	lr: 5.250e-05, eta: 14:55:24, time: 1.543, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2037, decode.acc_seg: 88.2955, src.loss_imnet_feat_dist: 0.1267, mix.decode.loss_seg: 0.1979, mix.decode.acc_seg: 87.4489
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:26:46,481 - mmseg - INFO - Iter [5050/40000]	lr: 5.243e-05, eta: 14:54:25, time: 1.584, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1795, decode.acc_seg: 88.9232, src.loss_imnet_feat_dist: 0.1268, mix.decode.loss_seg: 0.1845, mix.decode.acc_seg: 86.9701
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:28:03,805 - mmseg - INFO - Iter [5100/40000]	lr: 5.235e-05, eta: 14:53:12, time: 1.546, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1591, decode.acc_seg: 89.2756, src.loss_imnet_feat_dist: 0.1195, mix.decode.loss_seg: 0.1813, mix.decode.acc_seg: 87.8411
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:29:22,515 - mmseg - INFO - Iter [5150/40000]	lr: 5.228e-05, eta: 14:52:08, time: 1.574, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1846, decode.acc_seg: 88.5366, src.loss_imnet_feat_dist: 0.1232, mix.decode.loss_seg: 0.1783, mix.decode.acc_seg: 87.9313
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:30:39,751 - mmseg - INFO - Iter [5200/40000]	lr: 5.220e-05, eta: 14:50:54, time: 1.545, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1909, decode.acc_seg: 89.7572, src.loss_imnet_feat_dist: 0.1236, mix.decode.loss_seg: 0.1842, mix.decode.acc_seg: 88.8099
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:31:58,207 - mmseg - INFO - Iter [5250/40000]	lr: 5.213e-05, eta: 14:49:48, time: 1.569, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1715, decode.acc_seg: 89.0664, src.loss_imnet_feat_dist: 0.1227, mix.decode.loss_seg: 0.1886, mix.decode.acc_seg: 87.1524
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:33:15,289 - mmseg - INFO - Iter [5300/40000]	lr: 5.205e-05, eta: 14:48:33, time: 1.542, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1785, decode.acc_seg: 89.2743, src.loss_imnet_feat_dist: 0.1176, mix.decode.loss_seg: 0.2088, mix.decode.acc_seg: 87.7238
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:34:32,448 - mmseg - INFO - Iter [5350/40000]	lr: 5.198e-05, eta: 14:47:18, time: 1.543, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1781, decode.acc_seg: 89.6705, src.loss_imnet_feat_dist: 0.1191, mix.decode.loss_seg: 0.1951, mix.decode.acc_seg: 88.2796
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:35:49,668 - mmseg - INFO - Iter [5400/40000]	lr: 5.190e-05, eta: 14:46:04, time: 1.544, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1667, decode.acc_seg: 89.2405, src.loss_imnet_feat_dist: 0.1117, mix.decode.loss_seg: 0.2112, mix.decode.acc_seg: 86.8147
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:37:08,843 - mmseg - INFO - Iter [5450/40000]	lr: 5.183e-05, eta: 14:45:02, time: 1.583, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1759, decode.acc_seg: 89.2347, src.loss_imnet_feat_dist: 0.1220, mix.decode.loss_seg: 0.1969, mix.decode.acc_seg: 87.0432
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:38:22,219 - mmseg - INFO - Iter [5500/40000]	lr: 5.175e-05, eta: 14:43:24, time: 1.468, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1602, decode.acc_seg: 90.9156, src.loss_imnet_feat_dist: 0.1215, mix.decode.loss_seg: 0.1797, mix.decode.acc_seg: 89.5618
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:39:39,564 - mmseg - INFO - Iter [5550/40000]	lr: 5.168e-05, eta: 14:42:10, time: 1.547, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1773, decode.acc_seg: 89.0422, src.loss_imnet_feat_dist: 0.1234, mix.decode.loss_seg: 0.1848, mix.decode.acc_seg: 88.5134
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:40:55,963 - mmseg - INFO - Iter [5600/40000]	lr: 5.160e-05, eta: 14:40:51, time: 1.528, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1715, decode.acc_seg: 89.8744, src.loss_imnet_feat_dist: 0.1210, mix.decode.loss_seg: 0.1874, mix.decode.acc_seg: 88.4443
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:42:14,794 - mmseg - INFO - Iter [5650/40000]	lr: 5.153e-05, eta: 14:39:46, time: 1.577, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1739, decode.acc_seg: 88.9801, src.loss_imnet_feat_dist: 0.1151, mix.decode.loss_seg: 0.1903, mix.decode.acc_seg: 86.9603
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:43:28,054 - mmseg - INFO - Iter [5700/40000]	lr: 5.145e-05, eta: 14:38:08, time: 1.465, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1736, decode.acc_seg: 88.7272, src.loss_imnet_feat_dist: 0.1200, mix.decode.loss_seg: 0.2005, mix.decode.acc_seg: 86.8992
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:44:45,974 - mmseg - INFO - Iter [5750/40000]	lr: 5.138e-05, eta: 14:36:57, time: 1.558, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2503, decode.acc_seg: 87.0596, src.loss_imnet_feat_dist: 0.1269, mix.decode.loss_seg: 0.2370, mix.decode.acc_seg: 84.5103
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:44:53,048 - mmseg - INFO - Iter [2650/40000]	lr: 5.603e-05, eta: 1 day, 10:47:58, time: 3.390, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 19.4066, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 54.0979
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:47:39,950 - mmseg - INFO - Iter [2700/40000]	lr: 5.595e-05, eta: 1 day, 10:44:59, time: 3.338, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 24.4551, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.5265
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:50:27,032 - mmseg - INFO - Iter [2750/40000]	lr: 5.588e-05, eta: 1 day, 10:42:03, time: 3.342, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 21.8653, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.9964
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:53:13,822 - mmseg - INFO - Iter [2800/40000]	lr: 5.580e-05, eta: 1 day, 10:39:04, time: 3.336, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 21.3640, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.5186
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:56:04,063 - mmseg - INFO - Iter [2850/40000]	lr: 5.573e-05, eta: 1 day, 10:36:50, time: 3.405, data_time: 0.017, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 21.9936, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.8593
6
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:57:36,460 - mmseg - INFO - Iter [6250/40000]	lr: 5.063e-05, eta: 14:24:22, time: 1.534, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1535, decode.acc_seg: 89.8346, src.loss_imnet_feat_dist: 0.1188, mix.decode.loss_seg: 0.1753, mix.decode.acc_seg: 88.5739
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 15:58:47,360 - mmseg - INFO - Iter [6300/40000]	lr: 5.055e-05, eta: 14:22:33, time: 1.418, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1467, decode.acc_seg: 90.0464, src.loss_imnet_feat_dist: 0.1194, mix.decode.loss_seg: 0.1707, mix.decode.acc_seg: 88.7051
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:00:04,923 - mmseg - INFO - Iter [6350/40000]	lr: 5.048e-05, eta: 14:21:21, time: 1.551, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1627, decode.acc_seg: 89.3859, src.loss_imnet_feat_dist: 0.1208, mix.decode.loss_seg: 0.1776, mix.decode.acc_seg: 88.5401
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:01:18,483 - mmseg - INFO - Iter [6400/40000]	lr: 5.040e-05, eta: 14:19:47, time: 1.471, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1659, decode.acc_seg: 88.8853, src.loss_imnet_feat_dist: 0.1183, mix.decode.loss_seg: 0.2154, mix.decode.acc_seg: 87.7525
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:02:31,887 - mmseg - INFO - Iter [6450/40000]	lr: 5.033e-05, eta: 14:18:13, time: 1.468, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1571, decode.acc_seg: 89.2850, src.loss_imnet_feat_dist: 0.1210, mix.decode.loss_seg: 0.1664, mix.decode.acc_seg: 88.9408
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:03:46,960 - mmseg - INFO - Iter [6500/40000]	lr: 5.025e-05, eta: 14:16:47, time: 1.501, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1591, decode.acc_seg: 89.9047, src.loss_imnet_feat_dist: 0.1163, mix.decode.loss_seg: 0.1972, mix.decode.acc_seg: 87.5325
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:04:22,856 - mmseg - INFO - Exp name: 220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02
2022-04-18 16:04:22,857 - mmseg - INFO - Iter [3000/40000]	lr: 5.550e-05, eta: 1 day, 10:27:33, time: 3.313, data_time: 0.017, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 19.4744, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.5734
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:07:12,195 - mmseg - INFO - Iter [3050/40000]	lr: 5.543e-05, eta: 1 day, 10:25:06, time: 3.387, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 21.0162, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.1871
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:07:22,694 - mmseg - INFO - Iter [6650/40000]	lr: 5.003e-05, eta: 14:11:45, time: 1.473, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1712, decode.acc_seg: 89.4345, src.loss_imnet_feat_dist: 0.1151, mix.decode.loss_seg: 0.1831, mix.decode.acc_seg: 88.4960
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:08:35,943 - mmseg - INFO - Iter [6700/40000]	lr: 4.995e-05, eta: 14:10:11, time: 1.465, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1495, decode.acc_seg: 89.6640, src.loss_imnet_feat_dist: 0.1193, mix.decode.loss_seg: 0.1746, mix.decode.acc_seg: 87.9435
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:09:49,740 - mmseg - INFO - Iter [6750/40000]	lr: 4.988e-05, eta: 14:08:41, time: 1.476, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1677, decode.acc_seg: 89.5187, src.loss_imnet_feat_dist: 0.1113, mix.decode.loss_seg: 0.1862, mix.decode.acc_seg: 88.5360
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:11:06,535 - mmseg - INFO - Iter [6800/40000]	lr: 4.980e-05, eta: 14:07:25, time: 1.536, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2099, decode.acc_seg: 88.8269, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.2211, mix.decode.acc_seg: 86.9872
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:12:24,574 - mmseg - INFO - Iter [6850/40000]	lr: 4.973e-05, eta: 14:06:16, time: 1.561, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1795, decode.acc_seg: 89.4364, src.loss_imnet_feat_dist: 0.1186, mix.decode.loss_seg: 0.1899, mix.decode.acc_seg: 88.1899
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:13:42,121 - mmseg - INFO - Iter [6900/40000]	lr: 4.965e-05, eta: 14:05:04, time: 1.551, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1424, decode.acc_seg: 89.6342, src.loss_imnet_feat_dist: 0.1220, mix.decode.loss_seg: 0.1584, mix.decode.acc_seg: 89.0771
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:15:00,664 - mmseg - INFO - Iter [6950/40000]	lr: 4.958e-05, eta: 14:03:57, time: 1.571, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1624, decode.acc_seg: 89.0674, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1702, mix.decode.acc_seg: 88.0965
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:15:33,088 - mmseg - INFO - Iter [3200/40000]	lr: 5.520e-05, eta: 1 day, 10:16:19, time: 3.339, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 21.2103, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.3793
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:18:21,227 - mmseg - INFO - Iter [3250/40000]	lr: 5.513e-05, eta: 1 day, 10:13:37, time: 3.363, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 24.2288, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.7710
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:21:08,650 - mmseg - INFO - Iter [3300/40000]	lr: 5.505e-05, eta: 1 day, 10:10:47, time: 3.348, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 23.0655, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 65.4763
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:23:57,858 - mmseg - INFO - Iter [3350/40000]	lr: 5.498e-05, eta: 1 day, 10:08:16, time: 3.384, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 22.4591, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.9828
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:26:26,743 - mmseg - INFO - Iter [7400/40000]	lr: 4.890e-05, eta: 13:52:12, time: 1.533, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1531, decode.acc_seg: 88.9121, src.loss_imnet_feat_dist: 0.1170, mix.decode.loss_seg: 0.1646, mix.decode.acc_seg: 88.0986
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:27:46,301 - mmseg - INFO - Iter [7450/40000]	lr: 4.883e-05, eta: 13:51:09, time: 1.591, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1428, decode.acc_seg: 90.3968, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1535, mix.decode.acc_seg: 89.2120
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:29:03,527 - mmseg - INFO - Iter [7500/40000]	lr: 4.875e-05, eta: 13:49:55, time: 1.544, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1431, decode.acc_seg: 89.8001, src.loss_imnet_feat_dist: 0.1156, mix.decode.loss_seg: 0.1610, mix.decode.acc_seg: 88.0224
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:30:22,345 - mmseg - INFO - Iter [7550/40000]	lr: 4.868e-05, eta: 13:48:48, time: 1.576, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1469, decode.acc_seg: 89.7641, src.loss_imnet_feat_dist: 0.1163, mix.decode.loss_seg: 0.1778, mix.decode.acc_seg: 87.8568
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:31:39,904 - mmseg - INFO - Iter [7600/40000]	lr: 4.860e-05, eta: 13:47:35, time: 1.551, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1518, decode.acc_seg: 90.1851, src.loss_imnet_feat_dist: 0.1170, mix.decode.loss_seg: 0.1565, mix.decode.acc_seg: 89.3013
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:32:52,071 - mmseg - INFO - Iter [7650/40000]	lr: 4.853e-05, eta: 13:46:00, time: 1.443, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1529, decode.acc_seg: 90.0133, src.loss_imnet_feat_dist: 0.1210, mix.decode.loss_seg: 0.1735, mix.decode.acc_seg: 87.6836
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:34:01,931 - mmseg - INFO - Iter [7700/40000]	lr: 4.845e-05, eta: 13:44:15, time: 1.397, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1804, decode.acc_seg: 88.0702, src.loss_imnet_feat_dist: 0.1168, mix.decode.loss_seg: 0.2221, mix.decode.acc_seg: 84.6339
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:35:09,058 - mmseg - INFO - Iter [3550/40000]	lr: 5.468e-05, eta: 1 day, 9:57:11, time: 3.356, data_time: 0.017, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 21.7264, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.4536
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:37:56,727 - mmseg - INFO - Iter [3600/40000]	lr: 5.460e-05, eta: 1 day, 9:54:24, time: 3.353, data_time: 0.017, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 21.4914, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.8250
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:40:46,571 - mmseg - INFO - Iter [3650/40000]	lr: 5.453e-05, eta: 1 day, 9:51:58, time: 3.397, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 22.4204, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.3915
[                                                  ] 0/500, elapsed: 0s, ETA:
[                                 ] 1/500, 1.7 task/s, elapsed: 1s, ETA:   290s
[                                 ] 2/500, 2.7 task/s, elapsed: 1s, ETA:   185s
[                                 ] 3/500, 3.4 task/s, elapsed: 1s, ETA:   146s
[                                 ] 4/500, 3.9 task/s, elapsed: 1s, ETA:   127s
[                                 ] 5/500, 4.3 task/s, elapsed: 1s, ETA:   115s
[                                 ] 6/500, 4.6 task/s, elapsed: 1s, ETA:   107s
[                                 ] 7/500, 4.9 task/s, elapsed: 1s, ETA:   102s
[                                 ] 8/500, 5.1 task/s, elapsed: 2s, ETA:    97s
[                                 ] 9/500, 5.2 task/s, elapsed: 2s, ETA:    94s
[                                ] 10/500, 5.4 task/s, elapsed: 2s, ETA:    91s
[                                ] 11/500, 5.5 task/s, elapsed: 2s, ETA:    89s
[                                ] 12/500, 5.6 task/s, elapsed: 2s, ETA:    87s
[                                ] 13/500, 5.7 task/s, elapsed: 2s, ETA:    85s
[                                ] 14/500, 5.8 task/s, elapsed: 2s, ETA:    84s
[                                ] 15/500, 5.8 task/s, elapsed: 3s, ETA:    84s
[>                               ] 16/500, 5.8 task/s, elapsed: 3s, ETA:    83s
[>                               ] 17/500, 5.9 task/s, elapsed: 3s, ETA:    82s
[>                               ] 18/500, 5.9 task/s, elapsed: 3s, ETA:    82s
[>                               ] 19/500, 5.9 task/s, elapsed: 3s, ETA:    81s
[>                               ] 20/500, 5.9 task/s, elapsed: 3s, ETA:    81s
[>                               ] 21/500, 6.0 task/s, elapsed: 4s, ETA:    80s
[>                               ] 22/500, 6.0 task/s, elapsed: 4s, ETA:    80s
[>                               ] 23/500, 6.0 task/s, elapsed: 4s, ETA:    79s
[>                               ] 24/500, 6.0 task/s, elapsed: 4s, ETA:    79s
[>                               ] 25/500, 6.1 task/s, elapsed: 4s, ETA:    78s
[>                               ] 26/500, 6.1 task/s, elapsed: 4s, ETA:    78s
[>                               ] 27/500, 6.1 task/s, elapsed: 4s, ETA:    78s
[>                               ] 28/500, 6.1 task/s, elapsed: 5s, ETA:    77s
[>                               ] 29/500, 6.1 task/s, elapsed: 5s, ETA:    77s
[>                               ] 30/500, 6.1 task/s, elapsed: 5s, ETA:    76s
[>                               ] 31/500, 6.2 task/s, elapsed: 5s, ETA:    76s
[>>                              ] 32/500, 6.2 task/s, elapsed: 5s, ETA:    76s
[>>                              ] 33/500, 6.2 task/s, elapsed: 5s, ETA:    75s
[>>                              ] 34/500, 6.2 task/s, elapsed: 5s, ETA:    75s
[>>                              ] 35/500, 6.2 task/s, elapsed: 6s, ETA:    75s
[>>                              ] 36/500, 6.2 task/s, elapsed: 6s, ETA:    75s
[>>                              ] 37/500, 6.2 task/s, elapsed: 6s, ETA:    74s
[>>                              ] 38/500, 6.2 task/s, elapsed: 6s, ETA:    74s
[>>                              ] 39/500, 6.3 task/s, elapsed: 6s, ETA:    74s
[>>                              ] 40/500, 6.3 task/s, elapsed: 6s, ETA:    73s
[>>                              ] 41/500, 6.3 task/s, elapsed: 7s, ETA:    73s
[>>                              ] 42/500, 6.3 task/s, elapsed: 7s, ETA:    73s
[>>                              ] 43/500, 6.3 task/s, elapsed: 7s, ETA:    73s
[>>                              ] 44/500, 6.3 task/s, elapsed: 7s, ETA:    72s
[>>                              ] 45/500, 6.3 task/s, elapsed: 7s, ETA:    72s
[>>                              ] 46/500, 6.3 task/s, elapsed: 7s, ETA:    72s
[>>>                             ] 47/500, 6.3 task/s, elapsed: 7s, ETA:    72s
[>>>                             ] 48/500, 6.3 task/s, elapsed: 8s, ETA:    72s
[>>>                             ] 49/500, 6.3 task/s, elapsed: 8s, ETA:    72s
[>>>                             ] 50/500, 6.3 task/s, elapsed: 8s, ETA:    72s
[>>>                             ] 51/500, 6.3 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 52/500, 6.3 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 53/500, 6.3 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 54/500, 6.3 task/s, elapsed: 9s, ETA:    71s
[>>>                             ] 55/500, 6.3 task/s, elapsed: 9s, ETA:    71s
[>>>                             ] 56/500, 6.3 task/s, elapsed: 9s, ETA:    71s
[>>>                             ] 57/500, 6.3 task/s, elapsed: 9s, ETA:    71s
[>>>                             ] 58/500, 6.3 task/s, elapsed: 9s, ETA:    70s
[>>>                             ] 59/500, 6.3 task/s, elapsed: 9s, ETA:    70s
[>>>                            ] 60/500, 6.3 task/s, elapsed: 10s, ETA:    70s
[>>>                            ] 61/500, 6.3 task/s, elapsed: 10s, ETA:    70s
[>>>                            ] 62/500, 6.3 task/s, elapsed: 10s, ETA:    70s
[>>>                            ] 63/500, 6.3 task/s, elapsed: 10s, ETA:    70s
[>>>                            ] 64/500, 6.3 task/s, elapsed: 10s, ETA:    70s
[>>>>                           ] 65/500, 6.3 task/s, elapsed: 10s, ETA:    69s
[>>>>                           ] 66/500, 6.3 task/s, elapsed: 11s, ETA:    69s
[>>>>                           ] 67/500, 6.3 task/s, elapsed: 11s, ETA:    69s
[>>>>                           ] 68/500, 6.3 task/s, elapsed: 11s, ETA:    69s
[>>>>                           ] 69/500, 6.3 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 70/500, 6.3 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 71/500, 6.3 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 72/500, 6.3 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 73/500, 6.3 task/s, elapsed: 12s, ETA:    67s
[>>>>                           ] 74/500, 6.3 task/s, elapsed: 12s, ETA:    67s
[>>>>                           ] 75/500, 6.4 task/s, elapsed: 12s, ETA:    67s
[>>>>                           ] 76/500, 6.4 task/s, elapsed: 12s, ETA:    67s
[>>>>                           ] 77/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 78/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 79/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 80/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>>                          ] 81/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 82/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 83/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 84/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 85/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 86/500, 6.4 task/s, elapsed: 13s, ETA:    64s
[>>>>>                          ] 87/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 88/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 89/500, 6.5 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 90/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 91/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 92/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 93/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 94/500, 6.5 task/s, elapsed: 15s, ETA:    63s
[>>>>>                          ] 95/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>                          ] 96/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>>                         ] 97/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>>                         ] 98/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>>                         ] 99/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>>                        ] 100/500, 6.5 task/s, elapsed: 15s, ETA:    61s
[>>>>>>                        ] 101/500, 6.5 task/s, elapsed: 15s, ETA:    61s
[>>>>>>                        ] 102/500, 6.5 task/s, elapsed: 16s, ETA:    61s
[>>>>>>                        ] 103/500, 6.5 task/s, elapsed: 16s, ETA:    61s
[>>>>>>                        ] 104/500, 6.5 task/s, elapsed: 16s, ETA:    61s
[>>>>>>                        ] 105/500, 6.5 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 106/500, 6.5 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 107/500, 6.5 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 108/500, 6.5 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 109/500, 6.6 task/s, elapsed: 17s, ETA:    60s
[>>>>>>                        ] 110/500, 6.6 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 111/500, 6.6 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 112/500, 6.6 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 113/500, 6.6 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 114/500, 6.6 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 115/500, 6.6 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 116/500, 6.6 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 117/500, 6.6 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 118/500, 6.6 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 119/500, 6.6 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 120/500, 6.6 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 121/500, 6.6 task/s, elapsed: 18s, ETA:    57s
[>>>>>>>                       ] 122/500, 6.6 task/s, elapsed: 18s, ETA:    57s
[>>>>>>>                       ] 123/500, 6.6 task/s, elapsed: 19s, ETA:    57s
[>>>>>>>                       ] 124/500, 6.6 task/s, elapsed: 19s, ETA:    57s
[>>>>>>>                       ] 125/500, 6.6 task/s, elapsed: 19s, ETA:    57s
[>>>>>>>                       ] 126/500, 6.6 task/s, elapsed: 19s, ETA:    57s
[>>>>>>>                       ] 127/500, 6.6 task/s, elapsed: 19s, ETA:    56s
[>>>>>>>                       ] 128/500, 6.6 task/s, elapsed: 19s, ETA:    56s
[>>>>>>>                       ] 129/500, 6.6 task/s, elapsed: 19s, ETA:    56s
[>>>>>>>                       ] 130/500, 6.6 task/s, elapsed: 20s, ETA:    56s
[>>>>>>>                       ] 131/500, 6.6 task/s, elapsed: 20s, ETA:    56s
[>>>>>>>                       ] 132/500, 6.6 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>                       ] 133/500, 6.6 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>>                      ] 134/500, 6.6 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>>                      ] 135/500, 6.6 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>>                      ] 136/500, 6.6 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>>                      ] 137/500, 6.6 task/s, elapsed: 21s, ETA:    55s
[>>>>>>>>                      ] 138/500, 6.6 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 139/500, 6.7 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 140/500, 6.7 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 141/500, 6.7 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 142/500, 6.7 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 143/500, 6.7 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 144/500, 6.7 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 145/500, 6.7 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 146/500, 6.7 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 147/500, 6.7 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 148/500, 6.7 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 149/500, 6.7 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>>                     ] 150/500, 6.7 task/s, elapsed: 22s, ETA:    52s
[>>>>>>>>>                     ] 151/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 152/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 153/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 154/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 155/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 156/500, 6.7 task/s, elapsed: 23s, ETA:    51s
[>>>>>>>>>                     ] 157/500, 6.7 task/s, elapsed: 23s, ETA:    51s
[>>>>>>>>>                     ] 158/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 159/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 160/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 161/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 162/500, 6.7 task/s, elapsed: 24s, ETA:    50s
[>>>>>>>>>                     ] 163/500, 6.7 task/s, elapsed: 24s, ETA:    50s
[>>>>>>>>>                     ] 164/500, 6.7 task/s, elapsed: 24s, ETA:    50s
[>>>>>>>>>                     ] 165/500, 6.7 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>                     ] 166/500, 6.7 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>>                    ] 167/500, 6.7 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>>                    ] 168/500, 6.7 task/s, elapsed: 25s, ETA:    49s
[>>>>>>>>>>                    ] 169/500, 6.7 task/s, elapsed: 25s, ETA:    49s
[>>>>>>>>>>                    ] 170/500, 6.7 task/s, elapsed: 25s, ETA:    49s
[>>>>>>>>>>                    ] 171/500, 6.7 task/s, elapsed: 25s, ETA:    49s
[>>>>>>>>>>                    ] 172/500, 6.7 task/s, elapsed: 26s, ETA:    49s
[>>>>>>>>>>                    ] 173/500, 6.7 task/s, elapsed: 26s, ETA:    49s
[>>>>>>>>>>                    ] 174/500, 6.7 task/s, elapsed: 26s, ETA:    48s
[>>>>>>>>>>                    ] 175/500, 6.7 task/s, elapsed: 26s, ETA:    48s
[>>>>>>>>>>                    ] 176/500, 6.7 task/s, elapsed: 26s, ETA:    48s
[>>>>>>>>>>                    ] 177/500, 6.7 task/s, elapsed: 26s, ETA:    48s
[>>>>>>>>>>                    ] 178/500, 6.7 task/s, elapsed: 26s, ETA:    48s
[>>>>>>>>>>                    ] 179/500, 6.7 task/s, elapsed: 27s, ETA:    48s
[>>>>>>>>>>                    ] 180/500, 6.7 task/s, elapsed: 27s, ETA:    48s
[>>>>>>>>>>                    ] 181/500, 6.7 task/s, elapsed: 27s, ETA:    47s
[>>>>>>>>>>                    ] 182/500, 6.7 task/s, elapsed: 27s, ETA:    47s
[>>>>>>>>>>                    ] 183/500, 6.7 task/s, elapsed: 27s, ETA:    47s
[>>>>>>>>>>>                   ] 184/500, 6.7 task/s, elapsed: 27s, ETA:    47s
[>>>>>>>>>>>                   ] 185/500, 6.7 task/s, elapsed: 27s, ETA:    47s
[>>>>>>>>>>>                   ] 186/500, 6.7 task/s, elapsed: 28s, ETA:    47s
[>>>>>>>>>>>                   ] 187/500, 6.7 task/s, elapsed: 28s, ETA:    46s
[>>>>>>>>>>>                   ] 188/500, 6.7 task/s, elapsed: 28s, ETA:    46s
[>>>>>>>>>>>                   ] 189/500, 6.8 task/s, elapsed: 28s, ETA:    46s
[>>>>>>>>>>>                   ] 190/500, 6.8 task/s, elapsed: 28s, ETA:    46s
[>>>>>>>>>>>                   ] 191/500, 6.8 task/s, elapsed: 28s, ETA:    46s
[>>>>>>>>>>>                   ] 192/500, 6.8 task/s, elapsed: 28s, ETA:    46s
[>>>>>>>>>>>                   ] 193/500, 6.8 task/s, elapsed: 29s, ETA:    45s
[>>>>>>>>>>>                   ] 194/500, 6.8 task/s, elapsed: 29s, ETA:    45s
[>>>>>>>>>>>                   ] 195/500, 6.8 task/s, elapsed: 29s, ETA:    45s
[>>>>>>>>>>>                   ] 196/500, 6.8 task/s, elapsed: 29s, ETA:    45s
[>>>>>>>>>>>                   ] 197/500, 6.8 task/s, elapsed: 29s, ETA:    45s
[>>>>>>>>>>>                   ] 198/500, 6.8 task/s, elapsed: 29s, ETA:    45s
[>>>>>>>>>>>                   ] 199/500, 6.8 task/s, elapsed: 29s, ETA:    45s
[>>>>>>>>>>>>                  ] 200/500, 6.8 task/s, elapsed: 30s, ETA:    44s
[>>>>>>>>>>>>                  ] 201/500, 6.8 task/s, elapsed: 30s, ETA:    44s
[>>>>>>>>>>>>                  ] 202/500, 6.8 task/s, elapsed: 30s, ETA:    44s
[>>>>>>>>>>>>                  ] 203/500, 6.8 task/s, elapsed: 30s, ETA:    44s
[>>>>>>>>>>>>                  ] 204/500, 6.8 task/s, elapsed: 30s, ETA:    44s
[>>>>>>>>>>>>                  ] 205/500, 6.8 task/s, elapsed: 30s, ETA:    44s
[>>>>>>>>>>>>                  ] 206/500, 6.8 task/s, elapsed: 31s, ETA:    44s
[>>>>>>>>>>>>                  ] 207/500, 6.8 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 208/500, 6.8 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 209/500, 6.8 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 210/500, 6.7 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 211/500, 6.7 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 212/500, 6.7 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 213/500, 6.7 task/s, elapsed: 32s, ETA:    43s
[>>>>>>>>>>>>                  ] 214/500, 6.7 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>                  ] 215/500, 6.7 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>                  ] 216/500, 6.7 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>>                 ] 217/500, 6.7 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>>                 ] 218/500, 6.7 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>>                 ] 219/500, 6.7 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>>                 ] 220/500, 6.7 task/s, elapsed: 33s, ETA:    42s
[>>>>>>>>>>>>>                 ] 221/500, 6.7 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 222/500, 6.7 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 223/500, 6.7 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 224/500, 6.7 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 225/500, 6.7 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 226/500, 6.7 task/s, elapsed: 34s, ETA:    41s
[>>>>>>>>>>>>>                 ] 227/500, 6.7 task/s, elapsed: 34s, ETA:    41s
[>>>>>>>>>>>>>                 ] 228/500, 6.7 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 229/500, 6.7 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 230/500, 6.7 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 231/500, 6.7 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 232/500, 6.7 task/s, elapsed: 35s, ETA:    40s
[>>>>>>>>>>>>>                 ] 233/500, 6.7 task/s, elapsed: 35s, ETA:    40s
[>>>>>>>>>>>>>>                ] 234/500, 6.7 task/s, elapsed: 35s, ETA:    40s
[>>>>>>>>>>>>>>                ] 235/500, 6.7 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 236/500, 6.7 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 237/500, 6.7 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 238/500, 6.7 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 239/500, 6.7 task/s, elapsed: 36s, ETA:    39s
[>>>>>>>>>>>>>>                ] 240/500, 6.7 task/s, elapsed: 36s, ETA:    39s
[>>>>>>>>>>>>>>                ] 241/500, 6.7 task/s, elapsed: 36s, ETA:    39s
[>>>>>>>>>>>>>>                ] 242/500, 6.7 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 243/500, 6.7 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 244/500, 6.7 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 245/500, 6.7 task/s, elapsed: 37s, ETA:    38s
[>>>>>>>>>>>>>>                ] 246/500, 6.7 task/s, elapsed: 37s, ETA:    38s
[>>>>>>>>>>>>>>                ] 247/500, 6.7 task/s, elapsed: 37s, ETA:    38s
[>>>>>>>>>>>>>>                ] 248/500, 6.7 task/s, elapsed: 37s, ETA:    38s
[>>>>>>>>>>>>>>                ] 249/500, 6.7 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 250/500, 6.7 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 251/500, 6.7 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 252/500, 6.7 task/s, elapsed: 38s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 253/500, 6.7 task/s, elapsed: 38s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 254/500, 6.7 task/s, elapsed: 38s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 255/500, 6.7 task/s, elapsed: 38s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 256/500, 6.7 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 257/500, 6.7 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 258/500, 6.7 task/s, elapsed: 39s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 259/500, 6.7 task/s, elapsed: 39s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 260/500, 6.7 task/s, elapsed: 39s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 261/500, 6.7 task/s, elapsed: 39s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 262/500, 6.7 task/s, elapsed: 39s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 263/500, 6.7 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 264/500, 6.7 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 265/500, 6.7 task/s, elapsed: 40s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 266/500, 6.7 task/s, elapsed: 40s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 267/500, 6.7 task/s, elapsed: 40s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 268/500, 6.7 task/s, elapsed: 40s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 269/500, 6.7 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 270/500, 6.7 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 271/500, 6.7 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 272/500, 6.7 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 273/500, 6.7 task/s, elapsed: 41s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 274/500, 6.7 task/s, elapsed: 41s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 275/500, 6.7 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 276/500, 6.7 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 277/500, 6.7 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 278/500, 6.7 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 279/500, 6.7 task/s, elapsed: 42s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 280/500, 6.7 task/s, elapsed: 42s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 281/500, 6.7 task/s, elapsed: 42s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 282/500, 6.7 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>              ] 283/500, 6.7 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 284/500, 6.7 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 285/500, 6.7 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 286/500, 6.7 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 287/500, 6.7 task/s, elapsed: 43s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 288/500, 6.7 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 289/500, 6.7 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 290/500, 6.7 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 291/500, 6.7 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 292/500, 6.7 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 293/500, 6.7 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 294/500, 6.7 task/s, elapsed: 44s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 295/500, 6.7 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 296/500, 6.7 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 297/500, 6.7 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 298/500, 6.7 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 299/500, 6.7 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 300/500, 6.7 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 301/500, 6.7 task/s, elapsed: 45s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 302/500, 6.7 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 303/500, 6.7 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 304/500, 6.7 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 305/500, 6.7 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 306/500, 6.7 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 307/500, 6.7 task/s, elapsed: 46s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 308/500, 6.7 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 309/500, 6.7 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 310/500, 6.7 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 311/500, 6.7 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 312/500, 6.7 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 313/500, 6.8 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 314/500, 6.8 task/s, elapsed: 47s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 315/500, 6.8 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>            ] 316/500, 6.8 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 317/500, 6.8 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 318/500, 6.8 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 319/500, 6.8 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 320/500, 6.8 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 321/500, 6.8 task/s, elapsed: 48s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 322/500, 6.8 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 323/500, 6.8 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 324/500, 6.8 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 325/500, 6.8 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 326/500, 6.8 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 327/500, 6.8 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 328/500, 6.8 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 329/500, 6.8 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 330/500, 6.8 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 331/500, 6.8 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 332/500, 6.8 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 333/500, 6.8 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 334/500, 6.8 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 335/500, 6.8 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 336/500, 6.8 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 337/500, 6.8 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 338/500, 6.8 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 339/500, 6.8 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 340/500, 6.8 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 341/500, 6.8 task/s, elapsed: 50s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 342/500, 6.8 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 343/500, 6.8 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 344/500, 6.8 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 345/500, 6.8 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 346/500, 6.8 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 347/500, 6.8 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 348/500, 6.8 task/s, elapsed: 51s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>          ] 349/500, 6.8 task/s, elapsed: 51s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 350/500, 6.8 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 351/500, 6.8 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 352/500, 6.8 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 353/500, 6.8 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 354/500, 6.8 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 355/500, 6.8 task/s, elapsed: 52s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 356/500, 6.8 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 357/500, 6.8 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 358/500, 6.8 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 359/500, 6.8 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 360/500, 6.8 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 361/500, 6.8 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 362/500, 6.8 task/s, elapsed: 53s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 363/500, 6.8 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 364/500, 6.8 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 365/500, 6.8 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 366/500, 6.8 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 367/500, 6.8 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 368/500, 6.8 task/s, elapsed: 54s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 369/500, 6.8 task/s, elapsed: 54s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 370/500, 6.8 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 371/500, 6.8 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 372/500, 6.8 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 373/500, 6.8 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 374/500, 6.8 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 375/500, 6.8 task/s, elapsed: 55s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 376/500, 6.8 task/s, elapsed: 55s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 377/500, 6.8 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 378/500, 6.8 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 379/500, 6.8 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 380/500, 6.8 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 381/500, 6.8 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 382/500, 6.8 task/s, elapsed: 56s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>        ] 383/500, 6.8 task/s, elapsed: 56s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 384/500, 6.8 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 385/500, 6.8 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 386/500, 6.8 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 387/500, 6.8 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 388/500, 6.8 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 389/500, 6.8 task/s, elapsed: 57s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 390/500, 6.8 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 391/500, 6.8 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 392/500, 6.8 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 393/500, 6.8 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 394/500, 6.8 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 395/500, 6.8 task/s, elapsed: 58s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 396/500, 6.8 task/s, elapsed: 58s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 397/500, 6.8 task/s, elapsed: 58s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 398/500, 6.8 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 399/500, 6.8 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 400/500, 6.8 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 401/500, 6.8 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 402/500, 6.8 task/s, elapsed: 59s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 403/500, 6.8 task/s, elapsed: 59s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 404/500, 6.8 task/s, elapsed: 59s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 405/500, 6.8 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 406/500, 6.8 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 407/500, 6.8 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 408/500, 6.8 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 409/500, 6.8 task/s, elapsed: 60s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 410/500, 6.8 task/s, elapsed: 60s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 411/500, 6.8 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 412/500, 6.8 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 413/500, 6.8 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 414/500, 6.8 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 415/500, 6.8 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 416/500, 6.8 task/s, elapsed: 61s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 417/500, 6.8 task/s, elapsed: 61s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 418/500, 6.8 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 419/500, 6.8 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 420/500, 6.8 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 421/500, 6.8 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 422/500, 6.8 task/s, elapsed: 62s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 423/500, 6.8 task/s, elapsed: 62s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 424/500, 6.8 task/s, elapsed: 62s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 425/500, 6.8 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 426/500, 6.8 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 427/500, 6.8 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 428/500, 6.8 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 429/500, 6.8 task/s, elapsed: 63s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 430/500, 6.8 task/s, elapsed: 63s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 431/500, 6.8 task/s, elapsed: 63s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 432/500, 6.8 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 433/500, 6.8 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 434/500, 6.8 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 435/500, 6.8 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 436/500, 6.8 task/s, elapsed: 64s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 437/500, 6.8 task/s, elapsed: 64s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 438/500, 6.8 task/s, elapsed: 64s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 439/500, 6.8 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 440/500, 6.8 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 441/500, 6.8 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 442/500, 6.8 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 443/500, 6.8 task/s, elapsed: 65s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 444/500, 6.8 task/s, elapsed: 65s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 445/500, 6.8 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 446/500, 6.8 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 447/500, 6.8 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 448/500, 6.8 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 449/500, 6.8 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 450/500, 6.8 task/s, elapsed: 66s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 451/500, 6.8 task/s, elapsed: 66s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 452/500, 6.8 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 453/500, 6.8 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 454/500, 6.8 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 455/500, 6.8 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 456/500, 6.8 task/s, elapsed: 67s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 457/500, 6.8 task/s, elapsed: 67s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 458/500, 6.8 task/s, elapsed: 67s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 459/500, 6.8 task/s, elapsed: 68s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 460/500, 6.8 task/s, elapsed: 68s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 461/500, 6.8 task/s, elapsed: 68s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 462/500, 6.8 task/s, elapsed: 68s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 463/500, 6.8 task/s, elapsed: 68s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 464/500, 6.8 task/s, elapsed: 68s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 465/500, 6.8 task/s, elapsed: 69s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 466/500, 6.8 task/s, elapsed: 69s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 467/500, 6.8 task/s, elapsed: 69s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 468/500, 6.8 task/s, elapsed: 69s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 469/500, 6.8 task/s, elapsed: 69s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 470/500, 6.8 task/s, elapsed: 69s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 471/500, 6.8 task/s, elapsed: 69s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 472/500, 6.8 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 473/500, 6.8 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 474/500, 6.8 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 475/500, 6.8 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 476/500, 6.8 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 477/500, 6.8 task/s, elapsed: 70s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 478/500, 6.8 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 479/500, 6.8 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 480/500, 6.8 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 481/500, 6.8 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 482/500, 6.8 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 483/500, 6.8 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 484/500, 6.8 task/s, elapsed: 72s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 485/500, 6.8 task/s, elapsed: 72s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 486/500, 6.8 task/s, elapsed: 72s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 487/500, 6.8 task/s, elapsed: 72s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 488/500, 6.8 task/s, elapsed: 72s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 489/500, 6.8 task/s, elapsed: 72s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 490/500, 6.8 task/s, elapsed: 72s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 491/500, 6.8 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 492/500, 6.8 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 493/500, 6.8 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 494/500, 6.8 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 495/500, 6.8 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 496/500, 6.8 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 497/500, 6.8 task/s, elapsed: 73s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 498/500, 6.8 task/s, elapsed: 74s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 499/500, 6.8 task/s, elapsed: 74s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 500/500, 6.8 task/s, elapsed: 74s, ETA:     0s2022-04-18 16:43:14,567 - mmseg - INFO - per class results:
2022-04-18 16:43:14,569 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|      road     | 86.84 |  88.9 |
|    sidewalk   | 45.12 | 81.65 |
|    building   | 85.51 | 95.61 |
|      wall     | 15.02 | 16.22 |
|     fence     |  2.86 |  3.07 |
|      pole     | 42.65 | 51.02 |
| traffic light | 48.46 | 55.61 |
|  traffic sign | 49.54 |  58.0 |
|   vegetation  | 85.51 | 93.03 |
|    terrain    |  0.0  |  0.0  |
|      sky      | 87.71 | 98.79 |
|     person    | 70.77 | 83.36 |
|     rider     | 41.18 | 60.85 |
|      car      | 83.09 | 96.96 |
|     truck     |  0.0  |  0.0  |
|      bus      | 30.58 | 38.12 |
|     train     |  0.0  |  0.0  |
|   motorcycle  | 50.65 | 63.73 |
|    bicycle    |  62.3 | 77.02 |
+---------------+-------+-------+
2022-04-18 16:43:14,569 - mmseg - INFO - Summary:
2022-04-18 16:43:14,569 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 87.94 | 46.72 | 55.89 |
+-------+-------+-------+
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:43:14,586 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-18 16:43:14,587 - mmseg - INFO - Iter [500/40000]	lr: 4.800e-05, eta: 13:34:56, time: 1.406, data_time: 0.013, memory: 9636, aAcc: 0.8794, mIoU: 0.4672, mAcc: 0.5589, IoU.road: 0.8684, IoU.sidewalk: 0.4512, IoU.building: 0.8551, IoU.wall: 0.1502, IoU.fence: 0.0286, IoU.pole: 0.4265, IoU.traffic light: 0.4846, IoU.traffic sign: 0.4954, IoU.vegetation: 0.8551, IoU.terrain: 0.0000, IoU.sky: 0.8771, IoU.person: 0.7077, IoU.rider: 0.4118, IoU.car: 0.8309, IoU.truck: 0.0000, IoU.bus: 0.3058, IoU.train: 0.0000, IoU.motorcycle: 0.5065, IoU.bicycle: 0.6230, Acc.road: 0.8890, Acc.sidewalk: 0.8165, Acc.building: 0.9561, Acc.wall: 0.1622, Acc.fence: 0.0307, Acc.pole: 0.5102, Acc.traffic light: 0.5561, Acc.traffic sign: 0.5800, Acc.vegetation: 0.9303, Acc.terrain: 0.0000, Acc.sky: 0.9879, Acc.person: 0.8336, Acc.rider: 0.6085, Acc.car: 0.9696, Acc.truck: 0.0000, Acc.bus: 0.3812, Acc.train: 0.0000, Acc.motorcycle: 0.6373, Acc.bicycle: 0.7702, decode.loss_seg: 0.1477, decode.acc_seg: 89.0006, src.loss_imnet_feat_dist: 0.1142, mix.decode.loss_seg: 0.1582, mix.decode.acc_seg: 88.8903
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:44:32,329 - mmseg - INFO - Iter [8050/40000]	lr: 4.793e-05, eta: 13:41:34, time: 3.916, data_time: 2.376, memory: 9636, decode.loss_seg: 0.1646, decode.acc_seg: 90.0086, src.loss_imnet_feat_dist: 0.1149, mix.decode.loss_seg: 0.1777, mix.decode.acc_seg: 88.3035
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:45:49,716 - mmseg - INFO - Iter [8100/40000]	lr: 4.785e-05, eta: 13:40:18, time: 1.548, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1440, decode.acc_seg: 89.8708, src.loss_imnet_feat_dist: 0.1074, mix.decode.loss_seg: 0.1728, mix.decode.acc_seg: 87.9735
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:47:07,548 - mmseg - INFO - Iter [8150/40000]	lr: 4.778e-05, eta: 13:39:03, time: 1.557, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1468, decode.acc_seg: 90.3137, src.loss_imnet_feat_dist: 0.1176, mix.decode.loss_seg: 0.1650, mix.decode.acc_seg: 88.6319
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:48:25,138 - mmseg - INFO - Iter [8200/40000]	lr: 4.770e-05, eta: 13:37:48, time: 1.552, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1393, decode.acc_seg: 89.8340, src.loss_imnet_feat_dist: 0.1121, mix.decode.loss_seg: 0.1511, mix.decode.acc_seg: 88.4845
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:49:44,288 - mmseg - INFO - Iter [8250/40000]	lr: 4.763e-05, eta: 13:36:38, time: 1.583, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1572, decode.acc_seg: 89.4344, src.loss_imnet_feat_dist: 0.1154, mix.decode.loss_seg: 0.1876, mix.decode.acc_seg: 87.3665
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:51:01,580 - mmseg - INFO - Iter [8300/40000]	lr: 4.755e-05, eta: 13:35:22, time: 1.546, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1468, decode.acc_seg: 89.1623, src.loss_imnet_feat_dist: 0.1188, mix.decode.loss_seg: 0.1647, mix.decode.acc_seg: 87.6754
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:51:59,278 - mmseg - INFO - Iter [3850/40000]	lr: 5.423e-05, eta: 1 day, 9:41:05, time: 3.364, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 20.5924, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.5484
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:54:46,829 - mmseg - INFO - Iter [3900/40000]	lr: 5.415e-05, eta: 1 day, 9:38:15, time: 3.351, data_time: 0.018, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 22.3326, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.5270
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:57:34,759 - mmseg - INFO - Iter [3950/40000]	lr: 5.408e-05, eta: 1 day, 9:35:29, time: 3.359, data_time: 0.017, memory: 9779, decode.loss_seg: nan, decode.acc_seg: 21.8019, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.8034
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:58:27,066 - mmseg - INFO - Iter [8600/40000]	lr: 4.710e-05, eta: 13:26:35, time: 1.523, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1471, decode.acc_seg: 90.6531, src.loss_imnet_feat_dist: 0.1159, mix.decode.loss_seg: 0.1839, mix.decode.acc_seg: 87.9621
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 16:59:41,388 - mmseg - INFO - Iter [8650/40000]	lr: 4.703e-05, eta: 13:25:08, time: 1.486, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1531, decode.acc_seg: 88.9014, src.loss_imnet_feat_dist: 0.1127, mix.decode.loss_seg: 0.2075, mix.decode.acc_seg: 86.1420
[                                                  ] 0/500, elapsed: 0s, ETA:
[                                 ] 1/500, 0.4 task/s, elapsed: 3s, ETA:  1418s
[                                 ] 2/500, 0.6 task/s, elapsed: 3s, ETA:   804s
[                                 ] 3/500, 0.8 task/s, elapsed: 4s, ETA:   599s
[                                 ] 4/500, 1.0 task/s, elapsed: 4s, ETA:   497s
[                                 ] 5/500, 1.1 task/s, elapsed: 4s, ETA:   435s
[                                 ] 6/500, 1.3 task/s, elapsed: 5s, ETA:   393s
[                                 ] 7/500, 1.4 task/s, elapsed: 5s, ETA:   364s
[                                 ] 8/500, 1.4 task/s, elapsed: 6s, ETA:   341s
[                                 ] 9/500, 1.5 task/s, elapsed: 6s, ETA:   324s
[                                ] 10/500, 1.6 task/s, elapsed: 6s, ETA:   310s
[                                ] 11/500, 1.6 task/s, elapsed: 7s, ETA:   298s
[                                ] 12/500, 1.7 task/s, elapsed: 7s, ETA:   289s
[                                ] 13/500, 1.7 task/s, elapsed: 7s, ETA:   281s
[                                ] 14/500, 1.8 task/s, elapsed: 8s, ETA:   273s
[                                ] 15/500, 1.8 task/s, elapsed: 8s, ETA:   267s
[>                               ] 16/500, 1.8 task/s, elapsed: 9s, ETA:   262s
[>                               ] 17/500, 1.9 task/s, elapsed: 9s, ETA:   257s
[>                               ] 18/500, 1.9 task/s, elapsed: 9s, ETA:   252s
[>                              ] 19/500, 1.9 task/s, elapsed: 10s, ETA:   248s
[>                              ] 20/500, 2.0 task/s, elapsed: 10s, ETA:   245s
[>                              ] 21/500, 2.0 task/s, elapsed: 11s, ETA:   242s
[>                              ] 22/500, 2.0 task/s, elapsed: 11s, ETA:   238s
[>                              ] 23/500, 2.0 task/s, elapsed: 11s, ETA:   236s
[>                              ] 24/500, 2.0 task/s, elapsed: 12s, ETA:   233s
[>                              ] 25/500, 2.1 task/s, elapsed: 12s, ETA:   231s
[>                              ] 26/500, 2.1 task/s, elapsed: 13s, ETA:   228s
[>                              ] 27/500, 2.1 task/s, elapsed: 13s, ETA:   226s
[>                              ] 28/500, 2.1 task/s, elapsed: 13s, ETA:   224s
[>                              ] 29/500, 2.1 task/s, elapsed: 14s, ETA:   222s
[>                              ] 30/500, 2.1 task/s, elapsed: 14s, ETA:   221s
[>                              ] 31/500, 2.1 task/s, elapsed: 14s, ETA:   219s
[>                              ] 32/500, 2.2 task/s, elapsed: 15s, ETA:   217s
[>>                             ] 33/500, 2.2 task/s, elapsed: 15s, ETA:   216s
[>>                             ] 34/500, 2.2 task/s, elapsed: 16s, ETA:   214s
[>>                             ] 35/500, 2.2 task/s, elapsed: 16s, ETA:   213s
[>>                             ] 36/500, 2.2 task/s, elapsed: 16s, ETA:   211s
[>>                             ] 37/500, 2.2 task/s, elapsed: 17s, ETA:   210s
[>>                             ] 38/500, 2.2 task/s, elapsed: 17s, ETA:   209s
[>>                             ] 39/500, 2.2 task/s, elapsed: 18s, ETA:   208s
[>>                             ] 40/500, 2.2 task/s, elapsed: 18s, ETA:   206s
[>>                             ] 41/500, 2.2 task/s, elapsed: 18s, ETA:   205s
[>>                             ] 42/500, 2.2 task/s, elapsed: 19s, ETA:   204s
[>>                             ] 43/500, 2.2 task/s, elapsed: 19s, ETA:   203s
[>>                             ] 44/500, 2.3 task/s, elapsed: 20s, ETA:   202s
[>>                             ] 45/500, 2.3 task/s, elapsed: 20s, ETA:   201s
[>>                             ] 46/500, 2.3 task/s, elapsed: 20s, ETA:   200s
[>>                             ] 47/500, 2.3 task/s, elapsed: 21s, ETA:   199s
[>>                             ] 48/500, 2.3 task/s, elapsed: 21s, ETA:   198s
[>>>                            ] 49/500, 2.3 task/s, elapsed: 21s, ETA:   198s
[>>>                            ] 50/500, 2.3 task/s, elapsed: 22s, ETA:   197s
[>>>                            ] 51/500, 2.3 task/s, elapsed: 22s, ETA:   196s
[>>>                            ] 52/500, 2.3 task/s, elapsed: 23s, ETA:   195s
[>>>                            ] 53/500, 2.3 task/s, elapsed: 23s, ETA:   194s
[>>>                            ] 54/500, 2.3 task/s, elapsed: 23s, ETA:   194s
[>>>                            ] 55/500, 2.3 task/s, elapsed: 24s, ETA:   193s
[>>>                            ] 56/500, 2.3 task/s, elapsed: 24s, ETA:   192s
[>>>                            ] 57/500, 2.3 task/s, elapsed: 25s, ETA:   191s
[>>>                            ] 58/500, 2.3 task/s, elapsed: 25s, ETA:   191s
[>>>                            ] 59/500, 2.3 task/s, elapsed: 25s, ETA:   190s
[>>>                            ] 60/500, 2.3 task/s, elapsed: 26s, ETA:   189s
[>>>                            ] 61/500, 2.3 task/s, elapsed: 26s, ETA:   188s
[>>>                            ] 62/500, 2.3 task/s, elapsed: 27s, ETA:   188s
[>>>                            ] 63/500, 2.3 task/s, elapsed: 27s, ETA:   187s
[>>>                            ] 64/500, 2.3 task/s, elapsed: 27s, ETA:   186s
[>>>>                           ] 65/500, 2.3 task/s, elapsed: 28s, ETA:   186s
[>>>>                           ] 66/500, 2.3 task/s, elapsed: 28s, ETA:   185s
[>>>>                           ] 67/500, 2.4 task/s, elapsed: 28s, ETA:   184s
[>>>>                           ] 68/500, 2.4 task/s, elapsed: 29s, ETA:   183s
[>>>>                           ] 69/500, 2.4 task/s, elapsed: 29s, ETA:   183s
[>>>>                           ] 70/500, 2.4 task/s, elapsed: 30s, ETA:   182s
[>>>>                           ] 71/500, 2.4 task/s, elapsed: 30s, ETA:   182s
[>>>>                           ] 72/500, 2.4 task/s, elapsed: 30s, ETA:   181s/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:00:53,723 - mmseg - INFO - Iter [8700/40000]	lr: 4.695e-05, eta: 13:23:34, time: 1.447, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1466, decode.acc_seg: 89.1167, src.loss_imnet_feat_dist: 0.1154, mix.decode.loss_seg: 0.1762, mix.decode.acc_seg: 87.2943
ask/s, elapsed: 34s, ETA:   176s
[>>>>>                          ] 81/500, 2.4 task/s, elapsed: 34s, ETA:   175s
[>>>>>                          ] 82/500, 2.4 task/s, elapsed: 34s, ETA:   175s
[>>>>>                          ] 83/500, 2.4 task/s, elapsed: 35s, ETA:   174s
[>>>>>                          ] 84/500, 2.4 task/s, elapsed: 35s, ETA:   174s
[>>>>>                          ] 85/500, 2.4 task/s, elapsed: 35s, ETA:   173s
[>>>>>                          ] 86/500, 2.4 task/s, elapsed: 36s, ETA:   173s
[>>>>>                          ] 87/500, 2.4 task/s, elapsed: 36s, ETA:   172s
[>>>>>                          ] 88/500, 2.4 task/s, elapsed: 37s, ETA:   172s
[>>>>>                          ] 89/500, 2.4 task/s, elapsed: 37s, ETA:   171s
[>>>>>                          ] 90/500, 2.4 task/s, elapsed: 37s, ETA:   170s
[>>>>>                          ] 91/500, 2.4 task/s, elapsed: 38s, ETA:   170s
[>>>>>                          ] 92/500, 2.4 task/s, elapsed: 38s, ETA:   169s
[>>>>>                          ] 93/500, 2.4 task/s, elapsed: 39s, ETA:   169s
[>>>>>                          ] 94/500, 2.4 task/s, elapsed: 39s, ETA:   168s
[>>>>>                          ] 95/500, 2.4 task/s, elapsed: 39s, ETA:   168s
[>>>>>                          ] 96/500, 2.4 task/s, elapsed: 40s, ETA:   167s
[>>>>>>                         ] 97/500, 2.4 task/s, elapsed: 40s, ETA:   167s
[>>>>>>                         ] 98/500, 2.4 task/s, elapsed: 41s, ETA:   166s
[>>>>>>                         ] 99/500, 2.4 task/s, elapsed: 41s, ETA:   166s
[>>>>>>                        ] 100/500, 2.4 task/s, elapsed: 41s, ETA:   165s
[>>>>>>                        ] 101/500, 2.4 task/s, elapsed: 42s, ETA:   165s
[>>>>>>                        ] 102/500, 2.4 task/s, elapsed: 42s, ETA:   164s
[>>>>>>                        ] 103/500, 2.4 task/s, elapsed: 42s, ETA:   164s
[>>>>>>                        ] 104/500, 2.4 task/s, elapsed: 43s, ETA:   163s
[>>>>>>                        ] 105/500, 2.4 task/s, elapsed: 43s, ETA:   163s
[>>>>>>                        ] 106/500, 2.4 task/s, elapsed: 44s, ETA:   162s
[>>>>>>                        ] 107/500, 2.4 task/s, elapsed: 44s, ETA:   162s
[>>>>>>                        ] 108/500, 2.4 task/s, elapsed: 44s, ETA:   161s
[>>>>>>                        ] 109/500, 2.4 task/s, elapsed: 45s, ETA:   161s
[>>>>>>                        ] 110/500, 2.4 task/s, elapsed: 45s, ETA:   160s
[>>>>>>                        ] 111/500, 2.4 task/s, elapsed: 46s, ETA:   160s
[>>>>>>                        ] 112/500, 2.4 task/s, elapsed: 46s, ETA:   159s
[>>>>>>                        ] 113/500, 2.4 task/s, elapsed: 46s, ETA:   159s
[>>>>>>                        ] 114/500, 2.4 task/s, elapsed: 47s, ETA:   158s
[>>>>>>                        ] 115/500, 2.4 task/s, elapsed: 47s, ETA:   158s
[>>>>>>                        ] 116/500, 2.4 task/s, elapsed: 47s, ETA:   157s
[>>>>>>>                       ] 117/500, 2.4 task/s, elapsed: 48s, ETA:   157s
[>>>>>>>                       ] 118/500, 2.4 task/s, elapsed: 48s, ETA:   156s
[>>>>>>>                       ] 119/500, 2.4 task/s, elapsed: 49s, ETA:   156s
[>>>>>>>                       ] 120/500, 2.4 task/s, elapsed: 49s, ETA:   155s
[>>>>>>>                       ] 121/500, 2.4 task/s, elapsed: 49s, ETA:   155s
[>>>>>>>                       ] 122/500, 2.4 task/s, elapsed: 50s, ETA:   154s
[>>>>>>>                       ] 123/500, 2.5 task/s, elapsed: 50s, ETA:   154s
[>>>>>>>                       ] 124/500, 2.5 task/s, elapsed: 51s, ETA:   153s
[>>>>>>>                       ] 125/500, 2.5 task/s, elapsed: 51s, ETA:   153s
[>>>>>>>                       ] 126/500, 2.5 task/s, elapsed: 51s, ETA:   152s
[>>>>>>>                       ] 127/500, 2.5 task/s, elapsed: 52s, ETA:   152s
[>>>>>>>                       ] 128/500, 2.5 task/s, elapsed: 52s, ETA:   151s
[>>>>>>>                       ] 129/500, 2.5 task/s, elapsed: 53s, ETA:   151s
[>>>>>>>                       ] 130/500, 2.5 task/s, elapsed: 53s, ETA:   151s
[>>>>>>>                       ] 131/500, 2.5 task/s, elapsed: 53s, ETA:   150s
[>>>>>>>                       ] 132/500, 2.5 task/s, elapsed: 54s, ETA:   150s
[>>>>>>>                       ] 133/500, 2.5 task/s, elapsed: 54s, ETA:   149s
[>>>>>>>>                      ] 134/500, 2.5 task/s, elapsed: 54s, ETA:   149s
[>>>>>>>>                      ] 135/500, 2.5 task/s, elapsed: 55s, ETA:   148s
[>>>>>>>>                      ] 136/500, 2.5 task/s, elapsed: 55s, ETA:   148s
[>>>>>>>>                      ] 137/500, 2.5 task/s, elapsed: 56s, ETA:   147s
[>>>>>>>>                      ] 138/500, 2.5 task/s, elapsed: 56s, ETA:   147s
[>>>>>>>>                      ] 139/500, 2.5 task/s, elapsed: 56s, ETA:   146s
[>>>>>>>>                      ] 140/500, 2.5 task/s, elapsed: 57s, ETA:   146s
[>>>>>>>>                      ] 141/500, 2.5 task/s, elapsed: 57s, ETA:   146s
[>>>>>>>>                      ] 142/500, 2.5 task/s, elapsed: 58s, ETA:   145s
[>>>>>>>>                      ] 143/500, 2.5 task/s, elapsed: 58s, ETA:   145s
[>>>>>>>>                      ] 144/500, 2.5 task/s, elapsed: 58s, ETA:   144s
[>>>>>>>>                      ] 145/500, 2.5 task/s, elapsed: 59s, ETA:   144s
[>>>>>>>>                      ] 146/500, 2.5 task/s, elapsed: 59s, ETA:   143s
[>>>>>>>>                      ] 147/500, 2.5 task/s, elapsed: 59s, ETA:   143s
[>>>>>>>>                      ] 148/500, 2.5 task/s, elapsed: 60s, ETA:   142s
[>>>>>>>>                      ] 149/500, 2.5 task/s, elapsed: 60s, ETA:   142s
[>>>>>>>>>                     ] 150/500, 2.5 task/s, elapsed: 61s, ETA:   142s
[>>>>>>>>>                     ] 151/500, 2.5 task/s, elapsed: 61s, ETA:   141s
[>>>>>>>>>                     ] 152/500, 2.5 task/s, elapsed: 61s, ETA:   141s
[>>>>>>>>>                     ] 153/500, 2.5 task/s, elapsed: 62s, ETA:   140s
[>>>>>>>>>                     ] 154/500, 2.5 task/s, elapsed: 62s, ETA:   140s
[>>>>>>>>>                     ] 155/500, 2.5 task/s, elapsed: 63s, ETA:   139s
[>>>>>>>>>                     ] 156/500, 2.5 task/s, elapsed: 63s, ETA:   139s
[>>>>>>>>>                     ] 157/500, 2.5 task/s, elapsed: 63s, ETA:   138s
[>>>>>>>>>                     ] 158/500, 2.5 task/s, elapsed: 64s, ETA:   138s
[>>>>>>>>>                     ] 159/500, 2.5 task/s, elapsed: 64s, ETA:   138s
[>>>>>>>>>                     ] 160/500, 2.5 task/s, elapsed: 65s, ETA:   137s
[>>>>>>>>>                     ] 161/500, 2.5 task/s, elapsed: 65s, ETA:   137s
[>>>>>>>>>                     ] 162/500, 2.5 task/s, elapsed: 65s, ETA:   136s
[>>>>>>>>>                     ] 163/500, 2.5 task/s, elapsed: 66s, ETA:   136s
[>>>>>>>>>                     ] 164/500, 2.5 task/s, elapsed: 66s, ETA:   135s
[>>>>>>>>>                     ] 165/500, 2.5 task/s, elapsed: 66s, ETA:   135s
[>>>>>>>>>                     ] 166/500, 2.5 task/s, elapsed: 67s, ETA:   134s
[>>>>>>>>>>                    ] 167/500, 2.5 task/s, elapsed: 67s, ETA:   134s
[>>>>>>>>>>                    ] 168/500, 2.5 task/s, elapsed: 68s, ETA:   134s
[>>>>>>>>>>                    ] 169/500, 2.5 task/s, elapsed: 68s, ETA:   133s
[>>>>>>>>>>                    ] 170/500, 2.5 task/s, elapsed: 68s, ETA:   133s
[>>>>>>>>>>                    ] 171/500, 2.5 task/s, elapsed: 69s, ETA:   132s
[>>>>>>>>>>                    ] 172/500, 2.5 task/s, elapsed: 69s, ETA:   132s
[>>>>>>>>>>                    ] 173/500, 2.5 task/s, elapsed: 70s, ETA:   131s
[>>>>>>>>>>                    ] 174/500, 2.5 task/s, elapsed: 70s, ETA:   131s
[>>>>>>>>>>                    ] 175/500, 2.5 task/s, elapsed: 70s, ETA:   131s
[>>>>>>>>>>                    ] 176/500, 2.5 task/s, elapsed: 71s, ETA:   130s
[>>>>>>>>>>                    ] 177/500, 2.5 task/s, elapsed: 71s, ETA:   130s
[>>>>>>>>>>                    ] 178/500, 2.5 task/s, elapsed: 71s, ETA:   129s
[>>>>>>>>>>                    ] 179/500, 2.5 task/s, elapsed: 72s, ETA:   129s
[>>>>>>>>>>                    ] 180/500, 2.5 task/s, elapsed: 72s, ETA:   128s
[>>>>>>>>>>                    ] 181/500, 2.5 task/s, elapsed: 73s, ETA:   128s
[>>>>>>>>>>                    ] 182/500, 2.5 task/s, elapsed: 73s, ETA:   128s
[>>>>>>>>>>                    ] 183/500, 2.5 task/s, elapsed: 73s, ETA:   127s
[>>>>>>>>>>>                   ] 184/500, 2.5 task/s, elapsed: 74s, ETA:   127s
[>>>>>>>>>>>                   ] 185/500, 2.5 task/s, elapsed: 74s, ETA:   126s
[>>>>>>>>>>>                   ] 186/500, 2.5 task/s, elapsed: 75s, ETA:   126s
[>>>>>>>>>>>                   ] 187/500, 2.5 task/s, elapsed: 75s, ETA:   126s
[>>>>>>>>>>>                   ] 188/500, 2.5 task/s, elapsed: 75s, ETA:   125s
[>>>>>>>>>>>                   ] 189/500, 2.5 task/s, elapsed: 76s, ETA:   125s
[>>>>>>>>>>>                   ] 190/500, 2.5 task/s, elapsed: 76s, ETA:   124s
[>>>>>>>>>>>                   ] 191/500, 2.5 task/s, elapsed: 77s, ETA:   124s
[>>>>>>>>>>>                   ] 192/500, 2.5 task/s, elapsed: 77s, ETA:   123s
[>>>>>>>>>>>                   ] 193/500, 2.5 task/s, elapsed: 77s, ETA:   123s
[>>>>>>>>>>>                   ] 194/500, 2.5 task/s, elapsed: 78s, ETA:   123s
[>>>>>>>>>>>                   ] 195/500, 2.5 task/s, elapsed: 78s, ETA:   122s
[>>>>>>>>>>>                   ] 196/500, 2.5 task/s, elapsed: 78s, ETA:   122s
[>>>>>>>>>>>                   ] 197/500, 2.5 task/s, elapsed: 79s, ETA:   121s
[>>>>>>>>>>>                   ] 198/500, 2.5 task/s, elapsed: 79s, ETA:   121s
[>>>>>>>>>>>                   ] 199/500, 2.5 task/s, elapsed: 80s, ETA:   120s
[>>>>>>>>>>>>                  ] 200/500, 2.5 task/s, elapsed: 80s, ETA:   120s
[>>>>>>>>>>>>                  ] 201/500, 2.5 task/s, elapsed: 80s, ETA:   120s
[>>>>>>>>>>>>                  ] 202/500, 2.5 task/s, elapsed: 81s, ETA:   119s
[>>>>>>>>>>>>                  ] 203/500, 2.5 task/s, elapsed: 81s, ETA:   119s
[>>>>>>>>>>>>                  ] 204/500, 2.5 task/s, elapsed: 82s, ETA:   118s
[>>>>>>>>>>>>                  ] 205/500, 2.5 task/s, elapsed: 82s, ETA:   118s
[>>>>>>>>>>>>                  ] 206/500, 2.5 task/s, elapsed: 82s, ETA:   118s
[>>>>>>>>>>>>                  ] 207/500, 2.5 task/s, elapsed: 83s, ETA:   117s
[>>>>>>>>>>>>                  ] 208/500, 2.5 task/s, elapsed: 83s, ETA:   117s
[>>>>>>>>>>>>                  ] 209/500, 2.5 task/s, elapsed: 84s, ETA:   116s
[>>>>>>>>>>>>                  ] 210/500, 2.5 task/s, elapsed: 84s, ETA:   116s
[>>>>>>>>>>>>                  ] 211/500, 2.5 task/s, elapsed: 84s, ETA:   115s
[>>>>>>>>>>>>                  ] 212/500, 2.5 task/s, elapsed: 85s, ETA:   115s
[>>>>>>>>>>>>                  ] 213/500, 2.5 task/s, elapsed: 85s, ETA:   115s
[>>>>>>>>>>>>                  ] 214/500, 2.5 task/s, elapsed: 85s, ETA:   114s
[>>>>>>>>>>>>                  ] 215/500, 2.5 task/s, elapsed: 86s, ETA:   114s
[>>>>>>>>>>>>                  ] 216/500, 2.5 task/s, elapsed: 86s, ETA:   113s
[>>>>>>>>>>>>>                 ] 217/500, 2.5 task/s, elapsed: 87s, ETA:   113s
[>>>>>>>>>>>>>                 ] 218/500, 2.5 task/s, elapsed: 87s, ETA:   113s
[>>>>>>>>>>>>>                 ] 219/500, 2.5 task/s, elapsed: 87s, ETA:   112s
[>>>>>>>>>>>>>                 ] 220/500, 2.5 task/s, elapsed: 88s, ETA:   112s
[>>>>>>>>>>>>>                 ] 221/500, 2.5 task/s, elapsed: 88s, ETA:   111s
[>>>>>>>>>>>>>                 ] 222/500, 2.5 task/s, elapsed: 89s, ETA:   111s
[>>>>>>>>>>>>>                 ] 223/500, 2.5 task/s, elapsed: 89s, ETA:   110s
[>>>>>>>>>>>>>                 ] 224/500, 2.5 task/s, elapsed: 89s, ETA:   110s
[>>>>>>>>>>>>>                 ] 225/500, 2.5 task/s, elapsed: 90s, ETA:   110s
[>>>>>>>>>>>>>                 ] 226/500, 2.5 task/s, elapsed: 90s, ETA:   109s
[>>>>>>>>>>>>>                 ] 227/500, 2.5 task/s, elapsed: 90s, ETA:   109s
[>>>>>>>>>>>>>                 ] 228/500, 2.5 task/s, elapsed: 91s, ETA:   108s
[>>>>>>>>>>>>>                 ] 229/500, 2.5 task/s, elapsed: 91s, ETA:   108s
[>>>>>>>>>>>>>                 ] 230/500, 2.5 task/s, elapsed: 92s, ETA:   108s
[>>>>>>>>>>>>>                 ] 231/500, 2.5 task/s, elapsed: 92s, ETA:   107s
[>>>>>>>>>>>>>                 ] 232/500, 2.5 task/s, elapsed: 92s, ETA:   107s
[>>>>>>>>>>>>>                 ] 233/500, 2.5 task/s, elapsed: 93s, ETA:   106s
[>>>>>>>>>>>>>>                ] 234/500, 2.5 task/s, elapsed: 93s, ETA:   106s
[>>>>>>>>>>>>>>                ] 235/500, 2.5 task/s, elapsed: 94s, ETA:   106s
[>>>>>>>>>>>>>>                ] 236/500, 2.5 task/s, elapsed: 94s, ETA:   105s
[>>>>>>>>>>>>>>                ] 237/500, 2.5 task/s, elapsed: 94s, ETA:   105s
[>>>>>>>>>>>>>>                ] 238/500, 2.5 task/s, elapsed: 95s, ETA:   104s
[>>>>>>>>>>>>>>                ] 239/500, 2.5 task/s, elapsed: 95s, ETA:   104s
[>>>>>>>>>>>>>>                ] 240/500, 2.5 task/s, elapsed: 96s, ETA:   103s
[>>>>>>>>>>>>>>                ] 241/500, 2.5 task/s, elapsed: 96s, ETA:   103s
[>>>>>>>>>>>>>>                ] 242/500, 2.5 task/s, elapsed: 96s, ETA:   103s
[>>>>>>>>>>>>>>                ] 243/500, 2.5 task/s, elapsed: 97s, ETA:   102s
[>>>>>>>>>>>>>>                ] 244/500, 2.5 task/s, elapsed: 97s, ETA:   102s
[>>>>>>>>>>>>>>                ] 245/500, 2.5 task/s, elapsed: 97s, ETA:   101s
[>>>>>>>>>>>>>>                ] 246/500, 2.5 task/s, elapsed: 98s, ETA:   101s
[>>>>>>>>>>>>>>                ] 247/500, 2.5 task/s, elapsed: 98s, ETA:   101s
[>>>>>>>>>>>>>>                ] 248/500, 2.5 task/s, elapsed: 99s, ETA:   100s
[>>>>>>>>>>>>>>                ] 249/500, 2.5 task/s, elapsed: 99s, ETA:   100s
[>>>>>>>>>>>>>>>               ] 250/500, 2.5 task/s, elapsed: 99s, ETA:    99s
[>>>>>>>>>>>>>>               ] 251/500, 2.5 task/s, elapsed: 100s, ETA:    99s
[>>>>>>>>>>>>>>               ] 252/500, 2.5 task/s, elapsed: 100s, ETA:    99s
[>>>>>>>>>>>>>>               ] 253/500, 2.5 task/s, elapsed: 101s, ETA:    98s
[>>>>>>>>>>>>>>               ] 254/500, 2.5 task/s, elapsed: 101s, ETA:    98s
[>>>>>>>>>>>>>>               ] 255/500, 2.5 task/s, elapsed: 101s, ETA:    97s
[>>>>>>>>>>>>>>               ] 256/500, 2.5 task/s, elapsed: 102s, ETA:    97s
[>>>>>>>>>>>>>>               ] 257/500, 2.5 task/s, elapsed: 102s, ETA:    97s
[>>>>>>>>>>>>>>               ] 258/500, 2.5 task/s, elapsed: 102s, ETA:    96s
[>>>>>>>>>>>>>>>              ] 259/500, 2.5 task/s, elapsed: 103s, ETA:    96s
[>>>>>>>>>>>>>>>              ] 260/500, 2.5 task/s, elapsed: 103s, ETA:    95s
[>>>>>>>>>>>>>>>              ] 261/500, 2.5 task/s, elapsed: 104s, ETA:    95s
[>>>>>>>>>>>>>>>              ] 262/500, 2.5 task/s, elapsed: 104s, ETA:    94s
[>>>>>>>>>>>>>>>              ] 263/500, 2.5 task/s, elapsed: 104s, ETA:    94s
[>>>>>>>>>>>>>>>              ] 264/500, 2.5 task/s, elapsed: 105s, ETA:    94s
[>>>>>>>>>>>>>>>              ] 265/500, 2.5 task/s, elapsed: 105s, ETA:    93s
[>>>>>>>>>>>>>>>              ] 266/500, 2.5 task/s, elapsed: 106s, ETA:    93s
[>>>>>>>>>>>>>>>              ] 267/500, 2.5 task/s, elapsed: 106s, ETA:    92s
[>>>>>>>>>>>>>>>              ] 268/500, 2.5 task/s, elapsed: 106s, ETA:    92s
[>>>>>>>>>>>>>>>              ] 269/500, 2.5 task/s, elapsed: 107s, ETA:    92s
[>>>>>>>>>>>>>>>              ] 270/500, 2.5 task/s, elapsed: 107s, ETA:    91s
[>>>>>>>>>>>>>>>              ] 271/500, 2.5 task/s, elapsed: 108s, ETA:    91s
[>>>>>>>>>>>>>>>              ] 272/500, 2.5 task/s, elapsed: 108s, ETA:    90s
[>>>>>>>>>>>>>>>              ] 273/500, 2.5 task/s, elapsed: 108s, ETA:    90s
[>>>>>>>>>>>>>>>              ] 274/500, 2.5 task/s, elapsed: 109s, ETA:    90s
[>>>>>>>>>>>>>>>              ] 275/500, 2.5 task/s, elapsed: 109s, ETA:    89s
[>>>>>>>>>>>>>>>>             ] 276/500, 2.5 task/s, elapsed: 109s, ETA:    89s
[>>>>>>>>>>>>>>>>             ] 277/500, 2.5 task/s, elapsed: 110s, ETA:    88s
[>>>>>>>>>>>>>>>>             ] 278/500, 2.5 task/s, elapsed: 110s, ETA:    88s
[>>>>>>>>>>>>>>>>             ] 279/500, 2.5 task/s, elapsed: 111s, ETA:    88s
[>>>>>>>>>>>>>>>>             ] 280/500, 2.5 task/s, elapsed: 111s, ETA:    87s
[>>>>>>>>>>>>>>>>             ] 281/500, 2.5 task/s, elapsed: 111s, ETA:    87s
[>>>>>>>>>>>>>>>>             ] 282/500, 2.5 task/s, elapsed: 112s, ETA:    86s
[>>>>>>>>>>>>>>>>             ] 283/500, 2.5 task/s, elapsed: 112s, ETA:    86s
[>>>>>>>>>>>>>>>>             ] 284/500, 2.5 task/s, elapsed: 113s, ETA:    86s
[>>>>>>>>>>>>>>>>             ] 285/500, 2.5 task/s, elapsed: 113s, ETA:    85s
[>>>>>>>>>>>>>>>>             ] 286/500, 2.5 task/s, elapsed: 113s, ETA:    85s
[>>>>>>>>>>>>>>>>             ] 287/500, 2.5 task/s, elapsed: 114s, ETA:    84s
[>>>>>>>>>>>>>>>>             ] 288/500, 2.5 task/s, elapsed: 114s, ETA:    84s
[>>>>>>>>>>>>>>>>             ] 289/500, 2.5 task/s, elapsed: 114s, ETA:    84s
[>>>>>>>>>>>>>>>>             ] 290/500, 2.5 task/s, elapsed: 115s, ETA:    83s
[>>>>>>>>>>>>>>>>             ] 291/500, 2.5 task/s, elapsed: 115s, ETA:    83s
[>>>>>>>>>>>>>>>>             ] 292/500, 2.5 task/s, elapsed: 116s, ETA:    82s
[>>>>>>>>>>>>>>>>             ] 293/500, 2.5 task/s, elapsed: 116s, ETA:    82s
[>>>>>>>>>>>>>>>>>            ] 294/500, 2.5 task/s, elapsed: 116s, ETA:    82s
[>>>>>>>>>>>>>>>>>            ] 295/500, 2.5 task/s, elapsed: 117s, ETA:    81s
[>>>>>>>>>>>>>>>>>            ] 296/500, 2.5 task/s, elapsed: 117s, ETA:    81s
[>>>>>>>>>>>>>>>>>            ] 297/500, 2.5 task/s, elapsed: 118s, ETA:    80s
[>>>>>>>>>>>>>>>>>            ] 298/500, 2.5 task/s, elapsed: 118s, ETA:    80s
[>>>>>>>>>>>>>>>>>            ] 299/500, 2.5 task/s, elapsed: 118s, ETA:    80s
[>>>>>>>>>>>>>>>>>            ] 300/500, 2.5 task/s, elapsed: 119s, ETA:    79s
[>>>>>>>>>>>>>>>>>            ] 301/500, 2.5 task/s, elapsed: 119s, ETA:    79s
[>>>>>>>>>>>>>>>>>            ] 302/500, 2.5 task/s, elapsed: 120s, ETA:    78s
[>>>>>>>>>>>>>>>>>            ] 303/500, 2.5 task/s, elapsed: 120s, ETA:    78s
[>>>>>>>>>>>>>>>>>            ] 304/500, 2.5 task/s, elapsed: 120s, ETA:    78s
[>>>>>>>>>>>>>>>>>            ] 305/500, 2.5 task/s, elapsed: 121s, ETA:    77s
[>>>>>>>>>>>>>>>>>            ] 306/500, 2.5 task/s, elapsed: 121s, ETA:    77s
[>>>>>>>>>>>>>>>>>            ] 307/500, 2.5 task/s, elapsed: 121s, ETA:    76s
[>>>>>>>>>>>>>>>>>            ] 308/500, 2.5 task/s, elapsed: 122s, ETA:    76s
[>>>>>>>>>>>>>>>>>            ] 309/500, 2.5 task/s, elapsed: 122s, ETA:    76s
[>>>>>>>>>>>>>>>>>            ] 310/500, 2.5 task/s, elapsed: 123s, ETA:    75s
[>>>>>>>>>>>>>>>>>>           ] 311/500, 2.5 task/s, elapsed: 123s, ETA:    75s
[>>>>>>>>>>>>>>>>>>           ] 312/500, 2.5 task/s, elapsed: 124s, ETA:    74s
[>>>>>>>>>>>>>>>>>>           ] 313/500, 2.5 task/s, elapsed: 124s, ETA:    74s
[>>>>>>>>>>>>>>>>>>           ] 314/500, 2.5 task/s, elapsed: 124s, ETA:    74s
[>>>>>>>>>>>>>>>>>>           ] 315/500, 2.5 task/s, elapsed: 125s, ETA:    73s
[>>>>>>>>>>>>>>>>>>           ] 316/500, 2.5 task/s, elapsed: 125s, ETA:    73s
[>>>>>>>>>>>>>>>>>>           ] 317/500, 2.5 task/s, elapsed: 126s, ETA:    73s
[>>>>>>>>>>>>>>>>>>           ] 318/500, 2.5 task/s, elapsed: 126s, ETA:    72s
[>>>>>>>>>>>>>>>>>>           ] 319/500, 2.5 task/s, elapsed: 126s, ETA:    72s
[>>>>>>>>>>>>>>>>>>           ] 320/500, 2.5 task/s, elapsed: 127s, ETA:    71s
[>>>>>>>>>>>>>>>>>>           ] 321/500, 2.5 task/s, elapsed: 127s, ETA:    71s
[>>>>>>>>>>>>>>>>>>           ] 322/500, 2.5 task/s, elapsed: 128s, ETA:    71s
[>>>>>>>>>>>>>>>>>>           ] 323/500, 2.5 task/s, elapsed: 128s, ETA:    70s
[>>>>>>>>>>>>>>>>>>           ] 324/500, 2.5 task/s, elapsed: 128s, ETA:    70s
[>>>>>>>>>>>>>>>>>>           ] 325/500, 2.5 task/s, elapsed: 129s, ETA:    69s
[>>>>>>>>>>>>>>>>>>           ] 326/500, 2.5 task/s, elapsed: 129s, ETA:    69s
[>>>>>>>>>>>>>>>>>>           ] 327/500, 2.5 task/s, elapsed: 130s, ETA:    69s
[>>>>>>>>>>>>>>>>>>>          ] 328/500, 2.5 task/s, elapsed: 130s, ETA:    68s
[>>>>>>>>>>>>>>>>>>>          ] 329/500, 2.5 task/s, elapsed: 131s, ETA:    68s
[>>>>>>>>>>>>>>>>>>>          ] 330/500, 2.5 task/s, elapsed: 131s, ETA:    67s
[>>>>>>>>>>>>>>>>>>>          ] 331/500, 2.5 task/s, elapsed: 131s, ETA:    67s
[>>>>>>>>>>>>>>>>>>>          ] 332/500, 2.5 task/s, elapsed: 132s, ETA:    67s
[>>>>>>>>>>>>>>>>>>>          ] 333/500, 2.5 task/s, elapsed: 132s, ETA:    66s
[>>>>>>>>>>>>>>>>>>>          ] 334/500, 2.5 task/s, elapsed: 133s, ETA:    66s
[>>>>>>>>>>>>>>>>>>>          ] 335/500, 2.5 task/s, elapsed: 133s, ETA:    66s
[>>>>>>>>>>>>>>>>>>>          ] 336/500, 2.5 task/s, elapsed: 133s, ETA:    65s
[>>>>>>>>>>>>>>>>>>>          ] 337/500, 2.5 task/s, elapsed: 134s, ETA:    65s
[>>>>>>>>>>>>>>>>>>>          ] 338/500, 2.5 task/s, elapsed: 134s, ETA:    64s
[>>>>>>>>>>>>>>>>>>>          ] 339/500, 2.5 task/s, elapsed: 135s, ETA:    64s
[>>>>>>>>>>>>>>>>>>>          ] 340/500, 2.5 task/s, elapsed: 135s, ETA:    64s
[>>>>>>>>>>>>>>>>>>>          ] 341/500, 2.5 task/s, elapsed: 135s, ETA:    63s
[>>>>>>>>>>>>>>>>>>>          ] 342/500, 2.5 task/s, elapsed: 136s, ETA:    63s
[>>>>>>>>>>>>>>>>>>>          ] 343/500, 2.5 task/s, elapsed: 136s, ETA:    62s
[>>>>>>>>>>>>>>>>>>>          ] 344/500, 2.5 task/s, elapsed: 137s, ETA:    62s
[>>>>>>>>>>>>>>>>>>>>         ] 345/500, 2.5 task/s, elapsed: 137s, ETA:    62s
[>>>>>>>>>>>>>>>>>>>>         ] 346/500, 2.5 task/s, elapsed: 137s, ETA:    61s
[>>>>>>>>>>>>>>>>>>>>         ] 347/500, 2.5 task/s, elapsed: 138s, ETA:    61s
[>>>>>>>>>>>>>>>>>>>>         ] 348/500, 2.5 task/s, elapsed: 138s, ETA:    60s
[>>>>>>>>>>>>>>>>>>>>         ] 349/500, 2.5 task/s, elapsed: 139s, ETA:    60s
[>>>>>>>>>>>>>>>>>>>>         ] 350/500, 2.5 task/s, elapsed: 139s, ETA:    60s
[>>>>>>>>>>>>>>>>>>>>         ] 351/500, 2.5 task/s, elapsed: 139s, ETA:    59s
[>>>>>>>>>>>>>>>>>>>>         ] 352/500, 2.5 task/s, elapsed: 140s, ETA:    59s
[>>>>>>>>>>>>>>>>>>>>         ] 353/500, 2.5 task/s, elapsed: 140s, ETA:    58s
[>>>>>>>>>>>>>>>>>>>>         ] 354/500, 2.5 task/s, elapsed: 141s, ETA:    58s
[>>>>>>>>>>>>>>>>>>>>         ] 355/500, 2.5 task/s, elapsed: 141s, ETA:    58s
[>>>>>>>>>>>>>>>>>>>>         ] 356/500, 2.5 task/s, elapsed: 141s, ETA:    57s
[>>>>>>>>>>>>>>>>>>>>         ] 357/500, 2.5 task/s, elapsed: 142s, ETA:    57s
[>>>>>>>>>>>>>>>>>>>>         ] 358/500, 2.5 task/s, elapsed: 142s, ETA:    56s
[>>>>>>>>>>>>>>>>>>>>         ] 359/500, 2.5 task/s, elapsed: 142s, ETA:    56s
[>>>>>>>>>>>>>>>>>>>>         ] 360/500, 2.5 task/s, elapsed: 143s, ETA:    56s
[>>>>>>>>>>>>>>>>>>>>         ] 361/500, 2.5 task/s, elapsed: 143s, ETA:    55s
[>>>>>>>>>>>>>>>>>>>>         ] 362/500, 2.5 task/s, elapsed: 144s, ETA:    55s
[>>>>>>>>>>>>>>>>>>>>>        ] 363/500, 2.5 task/s, elapsed: 144s, ETA:    54s
[>>>>>>>>>>>>>>>>>>>>>        ] 364/500, 2.5 task/s, elapsed: 144s, ETA:    54s
[>>>>>>>>>>>>>>>>>>>>>        ] 365/500, 2.5 task/s, elapsed: 145s, ETA:    54s
[>>>>>>>>>>>>>>>>>>>>>        ] 366/500, 2.5 task/s, elapsed: 145s, ETA:    53s
[>>>>>>>>>>>>>>>>>>>>>        ] 367/500, 2.5 task/s, elapsed: 146s, ETA:    53s
[>>>>>>>>>>>>>>>>>>>>>        ] 368/500, 2.5 task/s, elapsed: 146s, ETA:    52s
[>>>>>>>>>>>>>>>>>>>>>        ] 369/500, 2.5 task/s, elapsed: 146s, ETA:    52s
[>>>>>>>>>>>>>>>>>>>>>        ] 370/500, 2.5 task/s, elapsed: 147s, ETA:    52s
[>>>>>>>>>>>>>>>>>>>>>        ] 371/500, 2.5 task/s, elapsed: 147s, ETA:    51s
[>>>>>>>>>>>>>>>>>>>>>        ] 372/500, 2.5 task/s, elapsed: 147s, ETA:    51s
[>>>>>>>>>>>>>>>>>>>>>        ] 373/500, 2.5 task/s, elapsed: 148s, ETA:    50s
[>>>>>>>>>>>>>>>>>>>>>        ] 374/500, 2.5 task/s, elapsed: 148s, ETA:    50s
[>>>>>>>>>>>>>>>>>>>>>        ] 375/500, 2.5 task/s, elapsed: 149s, ETA:    50s
[>>>>>>>>>>>>>>>>>>>>>        ] 376/500, 2.5 task/s, elapsed: 149s, ETA:    49s
[>>>>>>>>>>>>>>>>>>>>>        ] 377/500, 2.5 task/s, elapsed: 149s, ETA:    49s
[>>>>>>>>>>>>>>>>>>>>>        ] 378/500, 2.5 task/s, elapsed: 150s, ETA:    48s
[>>>>>>>>>>>>>>>>>>>>>        ] 379/500, 2.5 task/s, elapsed: 150s, ETA:    48s
[>>>>>>>>>>>>>>>>>>>>>>       ] 380/500, 2.5 task/s, elapsed: 151s, ETA:    48s
[>>>>>>>>>>>>>>>>>>>>>>       ] 381/500, 2.5 task/s, elapsed: 151s, ETA:    47s
[>>>>>>>>>>>>>>>>>>>>>>       ] 382/500, 2.5 task/s, elapsed: 151s, ETA:    47s
[>>>>>>>>>>>>>>>>>>>>>>       ] 383/500, 2.5 task/s, elapsed: 152s, ETA:    46s
[>>>>>>>>>>>>>>>>>>>>>>       ] 384/500, 2.5 task/s, elapsed: 152s, ETA:    46s
[>>>>>>>>>>>>>>>>>>>>>>       ] 385/500, 2.5 task/s, elapsed: 153s, ETA:    46s
[>>>>>>>>>>>>>>>>>>>>>>       ] 386/500, 2.5 task/s, elapsed: 153s, ETA:    45s
[>>>>>>>>>>>>>>>>>>>>>>       ] 387/500, 2.5 task/s, elapsed: 153s, ETA:    45s
[>>>>>>>>>>>>>>>>>>>>>>       ] 388/500, 2.5 task/s, elapsed: 154s, ETA:    44s
[>>>>>>>>>>>>>>>>>>>>>>       ] 389/500, 2.5 task/s, elapsed: 154s, ETA:    44s
[>>>>>>>>>>>>>>>>>>>>>>       ] 390/500, 2.5 task/s, elapsed: 155s, ETA:    44s
[>>>>>>>>>>>>>>>>>>>>>>       ] 391/500, 2.5 task/s, elapsed: 155s, ETA:    43s
[>>>>>>>>>>>>>>>>>>>>>>       ] 392/500, 2.5 task/s, elapsed: 155s, ETA:    43s
[>>>>>>>>>>>>>>>>>>>>>>       ] 393/500, 2.5 task/s, elapsed: 156s, ETA:    42s
[>>>>>>>>>>>>>>>>>>>>>>       ] 394/500, 2.5 task/s, elapsed: 156s, ETA:    42s
[>>>>>>>>>>>>>>>>>>>>>>       ] 395/500, 2.5 task/s, elapsed: 156s, ETA:    42s
[>>>>>>>>>>>>>>>>>>>>>>       ] 396/500, 2.5 task/s, elapsed: 157s, ETA:    41s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 397/500, 2.5 task/s, elapsed: 157s, ETA:    41s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 398/500, 2.5 task/s, elapsed: 158s, ETA:    40s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 399/500, 2.5 task/s, elapsed: 158s, ETA:    40s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 400/500, 2.5 task/s, elapsed: 158s, ETA:    40s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 401/500, 2.5 task/s, elapsed: 159s, ETA:    39s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 402/500, 2.5 task/s, elapsed: 159s, ETA:    39s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 403/500, 2.5 task/s, elapsed: 160s, ETA:    38s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 404/500, 2.5 task/s, elapsed: 160s, ETA:    38s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 405/500, 2.5 task/s, elapsed: 160s, ETA:    38s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 406/500, 2.5 task/s, elapsed: 161s, ETA:    37s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 407/500, 2.5 task/s, elapsed: 161s, ETA:    37s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 408/500, 2.5 task/s, elapsed: 161s, ETA:    36s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 409/500, 2.5 task/s, elapsed: 162s, ETA:    36s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 410/500, 2.5 task/s, elapsed: 162s, ETA:    36s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 411/500, 2.5 task/s, elapsed: 163s, ETA:    35s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 412/500, 2.5 task/s, elapsed: 163s, ETA:    35s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 413/500, 2.5 task/s, elapsed: 163s, ETA:    34s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 414/500, 2.5 task/s, elapsed: 164s, ETA:    34s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 415/500, 2.5 task/s, elapsed: 164s, ETA:    34s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 416/500, 2.5 task/s, elapsed: 165s, ETA:    33s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 417/500, 2.5 task/s, elapsed: 165s, ETA:    33s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 418/500, 2.5 task/s, elapsed: 165s, ETA:    32s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 419/500, 2.5 task/s, elapsed: 166s, ETA:    32s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 420/500, 2.5 task/s, elapsed: 166s, ETA:    32s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 421/500, 2.5 task/s, elapsed: 167s, ETA:    31s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 422/500, 2.5 task/s, elapsed: 167s, ETA:    31s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 423/500, 2.5 task/s, elapsed: 167s, ETA:    30s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 424/500, 2.5 task/s, elapsed: 168s, ETA:    30s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 425/500, 2.5 task/s, elapsed: 168s, ETA:    30s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 426/500, 2.5 task/s, elapsed: 168s, ETA:    29s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 427/500, 2.5 task/s, elapsed: 169s, ETA:    29s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 428/500, 2.5 task/s, elapsed: 169s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 429/500, 2.5 task/s, elapsed: 170s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 430/500, 2.5 task/s, elapsed: 170s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 431/500, 2.5 task/s, elapsed: 170s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 432/500, 2.5 task/s, elapsed: 171s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 433/500, 2.5 task/s, elapsed: 171s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 434/500, 2.5 task/s, elapsed: 172s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 435/500, 2.5 task/s, elapsed: 172s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 436/500, 2.5 task/s, elapsed: 172s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 437/500, 2.5 task/s, elapsed: 173s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 438/500, 2.5 task/s, elapsed: 173s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 439/500, 2.5 task/s, elapsed: 174s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 440/500, 2.5 task/s, elapsed: 174s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 441/500, 2.5 task/s, elapsed: 174s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 442/500, 2.5 task/s, elapsed: 175s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 443/500, 2.5 task/s, elapsed: 175s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 444/500, 2.5 task/s, elapsed: 176s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 445/500, 2.5 task/s, elapsed: 176s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 446/500, 2.5 task/s, elapsed: 176s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 447/500, 2.5 task/s, elapsed: 177s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 448/500, 2.5 task/s, elapsed: 177s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 449/500, 2.5 task/s, elapsed: 177s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 450/500, 2.5 task/s, elapsed: 178s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 451/500, 2.5 task/s, elapsed: 178s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 452/500, 2.5 task/s, elapsed: 179s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 453/500, 2.5 task/s, elapsed: 179s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 454/500, 2.5 task/s, elapsed: 179s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 455/500, 2.5 task/s, elapsed: 180s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 456/500, 2.5 task/s, elapsed: 180s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 457/500, 2.5 task/s, elapsed: 181s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 458/500, 2.5 task/s, elapsed: 181s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 459/500, 2.5 task/s, elapsed: 181s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 460/500, 2.5 task/s, elapsed: 182s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 461/500, 2.5 task/s, elapsed: 182s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 462/500, 2.5 task/s, elapsed: 183s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 463/500, 2.5 task/s, elapsed: 183s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 464/500, 2.5 task/s, elapsed: 183s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 465/500, 2.5 task/s, elapsed: 184s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 466/500, 2.5 task/s, elapsed: 184s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 467/500, 2.5 task/s, elapsed: 185s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 468/500, 2.5 task/s, elapsed: 185s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 469/500, 2.5 task/s, elapsed: 185s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 470/500, 2.5 task/s, elapsed: 186s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 471/500, 2.5 task/s, elapsed: 186s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 472/500, 2.5 task/s, elapsed: 187s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 473/500, 2.5 task/s, elapsed: 187s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 474/500, 2.5 task/s, elapsed: 187s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 475/500, 2.5 task/s, elapsed: 188s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 476/500, 2.5 task/s, elapsed: 188s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 477/500, 2.5 task/s, elapsed: 189s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 478/500, 2.5 task/s, elapsed: 189s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 479/500, 2.5 task/s, elapsed: 189s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 480/500, 2.5 task/s, elapsed: 190s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 481/500, 2.5 task/s, elapsed: 190s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 482/500, 2.5 task/s, elapsed: 191s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 483/500, 2.5 task/s, elapsed: 191s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 484/500, 2.5 task/s, elapsed: 191s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 485/500, 2.5 task/s, elapsed: 192s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 486/500, 2.5 task/s, elapsed: 192s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 487/500, 2.5 task/s, elapsed: 193s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 488/500, 2.5 task/s, elapsed: 193s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 489/500, 2.5 task/s, elapsed: 194s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 490/500, 2.5 task/s, elapsed: 194s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 491/500, 2.5 task/s, elapsed: 194s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 492/500, 2.5 task/s, elapsed: 195s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 493/500, 2.5 task/s, elapsed: 195s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 494/500, 2.5 task/s, elapsed: 196s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 495/500, 2.5 task/s, elapsed: 196s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 496/500, 2.5 task/s, elapsed: 196s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 497/500, 2.5 task/s, elapsed: 197s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 498/500, 2.5 task/s, elapsed: 197s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 499/500, 2.5 task/s, elapsed: 198s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 500/500, 2.5 task/s, elapsed: 198s, ETA:     0s2022-04-18 17:04:25,629 - mmseg - INFO - per class results:
2022-04-18 17:04:25,632 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|      road     | 37.66 | 100.0 |
|    sidewalk   |  0.0  |  0.0  |
|    building   |  0.0  |  0.0  |
|      wall     |  0.0  |  0.0  |
|     fence     |  0.0  |  0.0  |
|      pole     |  0.0  |  0.0  |
| traffic light |  0.0  |  0.0  |
|  traffic sign |  0.0  |  0.0  |
|   vegetation  |  0.0  |  0.0  |
|    terrain    |  0.0  |  0.0  |
|      sky      |  0.0  |  0.0  |
|     person    |  0.0  |  0.0  |
|     rider     |  0.0  |  0.0  |
|      car      |  0.0  |  0.0  |
|     truck     |  0.0  |  0.0  |
|      bus      |  0.0  |  0.0  |
|     train     |  0.0  |  0.0  |
|   motorcycle  |  0.0  |  0.0  |
|    bicycle    |  0.0  |  0.0  |
+---------------+-------+-------+
2022-04-18 17:04:25,632 - mmseg - INFO - Summary:
2022-04-18 17:04:25,632 - mmseg - INFO - 
+-------+------+------+
|  aAcc | mIoU | mAcc |
+-------+------+------+
| 37.66 | 1.98 | 5.26 |
+-------+------+------+
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:04:25,643 - mmseg - INFO - Exp name: 220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02
2022-04-18 17:04:25,643 - mmseg - INFO - Iter [500/40000]	lr: 5.400e-05, eta: 1 day, 9:32:36, time: 3.342, data_time: 0.018, memory: 9779, aAcc: 0.3766, mIoU: 0.0198, mAcc: 0.0526, IoU.road: 0.3766, IoU.sidewalk: 0.0000, IoU.building: 0.0000, IoU.wall: 0.0000, IoU.fence: 0.0000, IoU.pole: 0.0000, IoU.traffic light: 0.0000, IoU.traffic sign: 0.0000, IoU.vegetation: 0.0000, IoU.terrain: 0.0000, IoU.sky: 0.0000, IoU.person: 0.0000, IoU.rider: 0.0000, IoU.car: 0.0000, IoU.truck: 0.0000, IoU.bus: 0.0000, IoU.train: 0.0000, IoU.motorcycle: 0.0000, IoU.bicycle: 0.0000, Acc.road: 1.0000, Acc.sidewalk: 0.0000, Acc.building: 0.0000, Acc.wall: 0.0000, Acc.fence: 0.0000, Acc.pole: 0.0000, Acc.traffic light: 0.0000, Acc.traffic sign: 0.0000, Acc.vegetation: 0.0000, Acc.terrain: 0.0000, Acc.sky: 0.0000, Acc.person: 0.0000, Acc.rider: 0.0000, Acc.car: 0.0000, Acc.truck: 0.0000, Acc.bus: 0.0000, Acc.train: 0.0000, Acc.motorcycle: 0.0000, Acc.bicycle: 0.0000, decode.loss_seg: nan, decode.acc_seg: 21.6380, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 63.3544
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:04:40,572 - mmseg - INFO - Iter [8850/40000]	lr: 4.673e-05, eta: 13:19:28, time: 1.539, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1462, decode.acc_seg: 89.7553, src.loss_imnet_feat_dist: 0.1177, mix.decode.loss_seg: 0.1615, mix.decode.acc_seg: 88.0403
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:05:50,446 - mmseg - INFO - Iter [8900/40000]	lr: 4.665e-05, eta: 13:17:46, time: 1.397, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1466, decode.acc_seg: 89.9628, src.loss_imnet_feat_dist: 0.1138, mix.decode.loss_seg: 0.1621, mix.decode.acc_seg: 87.3048
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:07:01,823 - mmseg - INFO - Iter [8950/40000]	lr: 4.658e-05, eta: 13:16:10, time: 1.428, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1417, decode.acc_seg: 90.5872, src.loss_imnet_feat_dist: 0.1194, mix.decode.loss_seg: 0.1481, mix.decode.acc_seg: 89.3094
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:08:13,833 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-18 17:08:13,834 - mmseg - INFO - Iter [9000/40000]	lr: 4.650e-05, eta: 13:14:36, time: 1.440, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1432, decode.acc_seg: 90.0518, src.loss_imnet_feat_dist: 0.1118, mix.decode.loss_seg: 0.1643, mix.decode.acc_seg: 88.5478
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:09:25,976 - mmseg - INFO - Iter [9050/40000]	lr: 4.643e-05, eta: 13:13:03, time: 1.443, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1365, decode.acc_seg: 89.5707, src.loss_imnet_feat_dist: 0.1112, mix.decode.loss_seg: 0.1503, mix.decode.acc_seg: 89.2790
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:10:37,360 - mmseg - INFO - Iter [9100/40000]	lr: 4.635e-05, eta: 13:11:27, time: 1.428, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1432, decode.acc_seg: 89.7296, src.loss_imnet_feat_dist: 0.1162, mix.decode.loss_seg: 0.1722, mix.decode.acc_seg: 88.2060
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:11:53,916 - mmseg - INFO - Iter [9150/40000]	lr: 4.628e-05, eta: 13:10:09, time: 1.531, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1494, decode.acc_seg: 89.9451, src.loss_imnet_feat_dist: 0.1089, mix.decode.loss_seg: 0.1638, mix.decode.acc_seg: 89.2361
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:13:07,756 - mmseg - INFO - Iter [9200/40000]	lr: 4.620e-05, eta: 13:08:43, time: 1.477, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1504, decode.acc_seg: 89.6442, src.loss_imnet_feat_dist: 0.1139, mix.decode.loss_seg: 0.1742, mix.decode.acc_seg: 87.2834
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:14:22,555 - mmseg - INFO - Iter [9250/40000]	lr: 4.613e-05, eta: 13:07:19, time: 1.496, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1356, decode.acc_seg: 90.7002, src.loss_imnet_feat_dist: 0.1123, mix.decode.loss_seg: 0.1607, mix.decode.acc_seg: 89.4338
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:15:32,887 - mmseg - INFO - Iter [9300/40000]	lr: 4.605e-05, eta: 13:05:41, time: 1.407, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1444, decode.acc_seg: 89.2732, src.loss_imnet_feat_dist: 0.1184, mix.decode.loss_seg: 0.1549, mix.decode.acc_seg: 88.2683
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:16:49,453 - mmseg - INFO - Iter [9350/40000]	lr: 4.598e-05, eta: 13:04:23, time: 1.531, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1444, decode.acc_seg: 89.7460, src.loss_imnet_feat_dist: 0.1137, mix.decode.loss_seg: 0.1649, mix.decode.acc_seg: 87.3550
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:18:06,491 - mmseg - INFO - Iter [9400/40000]	lr: 4.590e-05, eta: 13:03:07, time: 1.541, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1367, decode.acc_seg: 90.8209, src.loss_imnet_feat_dist: 0.1158, mix.decode.loss_seg: 0.1483, mix.decode.acc_seg: 89.8444
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:18:25,407 - mmseg - INFO - Iter [4250/40000]	lr: 5.363e-05, eta: 1 day, 9:52:58, time: 3.361, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.2901, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.9600
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:21:14,167 - mmseg - INFO - Iter [4300/40000]	lr: 5.355e-05, eta: 1 day, 9:49:52, time: 3.375, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 24.6345, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.0960
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:24:01,030 - mmseg - INFO - Iter [4350/40000]	lr: 5.348e-05, eta: 1 day, 9:46:31, time: 3.337, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.2936, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.0707
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:24:30,955 - mmseg - INFO - Iter [9650/40000]	lr: 4.553e-05, eta: 12:56:45, time: 1.571, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1346, decode.acc_seg: 90.8198, src.loss_imnet_feat_dist: 0.1124, mix.decode.loss_seg: 0.1425, mix.decode.acc_seg: 89.2943
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:25:47,254 - mmseg - INFO - Iter [9700/40000]	lr: 4.545e-05, eta: 12:55:27, time: 1.526, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1279, decode.acc_seg: 90.6720, src.loss_imnet_feat_dist: 0.1121, mix.decode.loss_seg: 0.1484, mix.decode.acc_seg: 88.4199
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:27:06,385 - mmseg - INFO - Iter [9750/40000]	lr: 4.538e-05, eta: 12:54:18, time: 1.583, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1255, decode.acc_seg: 90.3268, src.loss_imnet_feat_dist: 0.1108, mix.decode.loss_seg: 0.1549, mix.decode.acc_seg: 88.8171
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:28:21,310 - mmseg - INFO - Iter [9800/40000]	lr: 4.530e-05, eta: 12:52:55, time: 1.499, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1456, decode.acc_seg: 90.6975, src.loss_imnet_feat_dist: 0.1140, mix.decode.loss_seg: 0.1595, mix.decode.acc_seg: 88.9763
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:29:40,051 - mmseg - INFO - Iter [9850/40000]	lr: 4.523e-05, eta: 12:51:44, time: 1.575, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1358, decode.acc_seg: 90.6417, src.loss_imnet_feat_dist: 0.1114, mix.decode.loss_seg: 0.1701, mix.decode.acc_seg: 88.3813
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:30:57,762 - mmseg - INFO - Iter [9900/40000]	lr: 4.515e-05, eta: 12:50:30, time: 1.554, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1361, decode.acc_seg: 90.4405, src.loss_imnet_feat_dist: 0.1116, mix.decode.loss_seg: 0.1593, mix.decode.acc_seg: 88.2712
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:32:16,285 - mmseg - INFO - Iter [9950/40000]	lr: 4.508e-05, eta: 12:49:19, time: 1.570, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1408, decode.acc_seg: 90.7179, src.loss_imnet_feat_dist: 0.1179, mix.decode.loss_seg: 0.1392, mix.decode.acc_seg: 89.4215
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:33:33,651 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-18 17:33:33,651 - mmseg - INFO - Iter [10000/40000]	lr: 4.500e-05, eta: 12:48:04, time: 1.547, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1360, decode.acc_seg: 90.0772, src.loss_imnet_feat_dist: 0.1137, mix.decode.loss_seg: 0.1560, mix.decode.acc_seg: 87.8745
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:34:51,853 - mmseg - INFO - Iter [10050/40000]	lr: 4.493e-05, eta: 12:46:51, time: 1.564, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1649, decode.acc_seg: 89.2543, src.loss_imnet_feat_dist: 0.1152, mix.decode.loss_seg: 0.2065, mix.decode.acc_seg: 87.1066
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:36:08,140 - mmseg - INFO - Iter [10100/40000]	lr: 4.485e-05, eta: 12:45:33, time: 1.526, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1573, decode.acc_seg: 89.5555, src.loss_imnet_feat_dist: 0.1153, mix.decode.loss_seg: 0.2024, mix.decode.acc_seg: 86.7324
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:37:24,834 - mmseg - INFO - Iter [10150/40000]	lr: 4.478e-05, eta: 12:44:15, time: 1.534, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1549, decode.acc_seg: 89.1231, src.loss_imnet_feat_dist: 0.1109, mix.decode.loss_seg: 0.1833, mix.decode.acc_seg: 87.3725
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:38:38,420 - mmseg - INFO - Iter [10200/40000]	lr: 4.470e-05, eta: 12:42:49, time: 1.472, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1622, decode.acc_seg: 89.3431, src.loss_imnet_feat_dist: 0.1238, mix.decode.loss_seg: 0.1981, mix.decode.acc_seg: 86.3081
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:39:57,583 - mmseg - INFO - Iter [10250/40000]	lr: 4.463e-05, eta: 12:41:39, time: 1.583, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1895, decode.acc_seg: 88.4389, src.loss_imnet_feat_dist: 0.1300, mix.decode.loss_seg: 0.1995, mix.decode.acc_seg: 86.7425
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:41:11,059 - mmseg - INFO - Iter [10300/40000]	lr: 4.455e-05, eta: 12:40:13, time: 1.470, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1528, decode.acc_seg: 89.8375, src.loss_imnet_feat_dist: 0.1160, mix.decode.loss_seg: 0.1726, mix.decode.acc_seg: 88.4566
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:42:24,343 - mmseg - INFO - Iter [10350/40000]	lr: 4.448e-05, eta: 12:38:46, time: 1.466, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1473, decode.acc_seg: 90.4168, src.loss_imnet_feat_dist: 0.1187, mix.decode.loss_seg: 0.1826, mix.decode.acc_seg: 87.9956
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:43:41,752 - mmseg - INFO - Iter [10400/40000]	lr: 4.440e-05, eta: 12:37:31, time: 1.548, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1457, decode.acc_seg: 90.0266, src.loss_imnet_feat_dist: 0.1116, mix.decode.loss_seg: 0.1600, mix.decode.acc_seg: 87.9446
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:45:00,583 - mmseg - INFO - Iter [10450/40000]	lr: 4.433e-05, eta: 12:36:20, time: 1.577, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1338, decode.acc_seg: 90.7185, src.loss_imnet_feat_dist: 0.1127, mix.decode.loss_seg: 0.1543, mix.decode.acc_seg: 88.8067
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:46:17,094 - mmseg - INFO - Iter [10500/40000]	lr: 4.425e-05, eta: 12:35:02, time: 1.530, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1381, decode.acc_seg: 89.1550, src.loss_imnet_feat_dist: 0.1140, mix.decode.loss_seg: 0.1647, mix.decode.acc_seg: 88.1447
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:47:34,838 - mmseg - INFO - Iter [10550/40000]	lr: 4.418e-05, eta: 12:33:48, time: 1.555, data_time: 0.013, memory: 9636, decode.loss_seg: 0.2251, decode.acc_seg: 88.2910, src.loss_imnet_feat_dist: 0.1207, mix.decode.loss_seg: 0.2059, mix.decode.acc_seg: 87.0481
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:48:49,216 - mmseg - INFO - Iter [10600/40000]	lr: 4.410e-05, eta: 12:32:25, time: 1.488, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1421, decode.acc_seg: 89.8074, src.loss_imnet_feat_dist: 0.1245, mix.decode.loss_seg: 0.1621, mix.decode.acc_seg: 88.0050
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:50:01,386 - mmseg - INFO - Iter [10650/40000]	lr: 4.403e-05, eta: 12:30:55, time: 1.443, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1517, decode.acc_seg: 89.4290, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1767, mix.decode.acc_seg: 87.0681
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:51:13,874 - mmseg - INFO - Iter [10700/40000]	lr: 4.395e-05, eta: 12:29:27, time: 1.450, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1524, decode.acc_seg: 89.6300, src.loss_imnet_feat_dist: 0.1134, mix.decode.loss_seg: 0.1664, mix.decode.acc_seg: 87.9326
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:52:27,963 - mmseg - INFO - Iter [10750/40000]	lr: 4.388e-05, eta: 12:28:03, time: 1.482, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1394, decode.acc_seg: 90.7369, src.loss_imnet_feat_dist: 0.1175, mix.decode.loss_seg: 0.1629, mix.decode.acc_seg: 88.8347
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:53:45,239 - mmseg - INFO - Iter [10800/40000]	lr: 4.380e-05, eta: 12:26:48, time: 1.546, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1490, decode.acc_seg: 89.9492, src.loss_imnet_feat_dist: 0.1169, mix.decode.loss_seg: 0.1653, mix.decode.acc_seg: 88.4712
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:54:47,653 - mmseg - INFO - Iter [4900/40000]	lr: 5.265e-05, eta: 1 day, 9:11:46, time: 3.349, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.4564, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.7944
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 17:57:38,294 - mmseg - INFO - Iter [4950/40000]	lr: 5.258e-05, eta: 1 day, 9:08:58, time: 3.413, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.1873, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.6828
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:00:26,469 - mmseg - INFO - Exp name: 220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02
2022-04-18 18:00:26,469 - mmseg - INFO - Iter [5000/40000]	lr: 5.250e-05, eta: 1 day, 9:05:54, time: 3.364, data_time: 0.019, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.3879, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.1850
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:01:23,845 - mmseg - INFO - Iter [11100/40000]	lr: 4.335e-05, eta: 12:19:03, time: 1.519, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1252, decode.acc_seg: 91.2752, src.loss_imnet_feat_dist: 0.1147, mix.decode.loss_seg: 0.1459, mix.decode.acc_seg: 90.0496
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:02:42,813 - mmseg - INFO - Iter [11150/40000]	lr: 4.328e-05, eta: 12:17:52, time: 1.579, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1451, decode.acc_seg: 89.5641, src.loss_imnet_feat_dist: 0.1153, mix.decode.loss_seg: 0.1562, mix.decode.acc_seg: 87.9185
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:04:00,184 - mmseg - INFO - Iter [11200/40000]	lr: 4.320e-05, eta: 12:16:37, time: 1.547, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1510, decode.acc_seg: 89.7023, src.loss_imnet_feat_dist: 0.1137, mix.decode.loss_seg: 0.1618, mix.decode.acc_seg: 88.8696
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:05:14,832 - mmseg - INFO - Iter [11250/40000]	lr: 4.313e-05, eta: 12:15:15, time: 1.493, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1450, decode.acc_seg: 90.1228, src.loss_imnet_feat_dist: 0.1253, mix.decode.loss_seg: 0.1744, mix.decode.acc_seg: 87.9379
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:06:25,769 - mmseg - INFO - Iter [11300/40000]	lr: 4.305e-05, eta: 12:13:43, time: 1.419, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1359, decode.acc_seg: 90.7002, src.loss_imnet_feat_dist: 0.1136, mix.decode.loss_seg: 0.1508, mix.decode.acc_seg: 90.0410
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:07:38,194 - mmseg - INFO - Iter [11350/40000]	lr: 4.298e-05, eta: 12:12:16, time: 1.448, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1253, decode.acc_seg: 90.1945, src.loss_imnet_feat_dist: 0.1130, mix.decode.loss_seg: 0.1648, mix.decode.acc_seg: 89.1052
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:08:55,844 - mmseg - INFO - Iter [11400/40000]	lr: 4.290e-05, eta: 12:11:02, time: 1.553, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1265, decode.acc_seg: 89.1451, src.loss_imnet_feat_dist: 0.1079, mix.decode.loss_seg: 0.1418, mix.decode.acc_seg: 88.1970
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:10:12,710 - mmseg - INFO - Iter [11450/40000]	lr: 4.283e-05, eta: 12:09:45, time: 1.537, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1369, decode.acc_seg: 90.1159, src.loss_imnet_feat_dist: 0.1180, mix.decode.loss_seg: 0.1823, mix.decode.acc_seg: 87.8974
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:11:29,189 - mmseg - INFO - Iter [11500/40000]	lr: 4.275e-05, eta: 12:08:28, time: 1.530, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1346, decode.acc_seg: 89.9612, src.loss_imnet_feat_dist: 0.1154, mix.decode.loss_seg: 0.1594, mix.decode.acc_seg: 88.6893
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:12:45,531 - mmseg - INFO - Iter [11550/40000]	lr: 4.268e-05, eta: 12:07:11, time: 1.527, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1439, decode.acc_seg: 89.3944, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1824, mix.decode.acc_seg: 86.5783
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:13:56,872 - mmseg - INFO - Iter [11600/40000]	lr: 4.260e-05, eta: 12:05:41, time: 1.427, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1621, decode.acc_seg: 88.5837, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1754, mix.decode.acc_seg: 86.3760
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:14:26,300 - mmseg - INFO - Iter [5250/40000]	lr: 5.213e-05, eta: 1 day, 8:50:28, time: 3.387, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.3809, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.7473
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:17:14,232 - mmseg - INFO - Iter [5300/40000]	lr: 5.205e-05, eta: 1 day, 8:47:23, time: 3.359, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.3032, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.3719
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:20:02,424 - mmseg - INFO - Iter [5350/40000]	lr: 5.198e-05, eta: 1 day, 8:44:21, time: 3.364, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.4699, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.1819
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:20:16,571 - mmseg - INFO - Iter [11850/40000]	lr: 4.223e-05, eta: 11:59:09, time: 1.593, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1263, decode.acc_seg: 91.2405, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1541, mix.decode.acc_seg: 88.6463
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:21:33,966 - mmseg - INFO - Iter [11900/40000]	lr: 4.215e-05, eta: 11:57:54, time: 1.548, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1418, decode.acc_seg: 90.0417, src.loss_imnet_feat_dist: 0.1083, mix.decode.loss_seg: 0.1537, mix.decode.acc_seg: 88.7934
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:22:51,433 - mmseg - INFO - Iter [11950/40000]	lr: 4.208e-05, eta: 11:56:40, time: 1.549, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1355, decode.acc_seg: 89.6485, src.loss_imnet_feat_dist: 0.1098, mix.decode.loss_seg: 0.1675, mix.decode.acc_seg: 87.1150
[                                                  ] 0/500, elapsed: 0s, ETA:
[                                 ] 1/500, 1.7 task/s, elapsed: 1s, ETA:   293s
[                                 ] 2/500, 2.6 task/s, elapsed: 1s, ETA:   190s
[                                 ] 3/500, 3.3 task/s, elapsed: 1s, ETA:   150s
[                                 ] 4/500, 3.8 task/s, elapsed: 1s, ETA:   130s
[                                 ] 5/500, 4.2 task/s, elapsed: 1s, ETA:   118s
[                                 ] 6/500, 4.5 task/s, elapsed: 1s, ETA:   111s
[                                 ] 7/500, 4.7 task/s, elapsed: 1s, ETA:   105s
[                                 ] 8/500, 4.9 task/s, elapsed: 2s, ETA:   100s
[                                 ] 9/500, 5.1 task/s, elapsed: 2s, ETA:    97s
[                                ] 10/500, 5.2 task/s, elapsed: 2s, ETA:    94s
[                                ] 11/500, 5.4 task/s, elapsed: 2s, ETA:    91s
[                                ] 12/500, 5.5 task/s, elapsed: 2s, ETA:    90s
[                                ] 13/500, 5.6 task/s, elapsed: 2s, ETA:    88s
[                                ] 14/500, 5.6 task/s, elapsed: 2s, ETA:    86s
[                                ] 15/500, 5.7 task/s, elapsed: 3s, ETA:    85s
[>                               ] 16/500, 5.8 task/s, elapsed: 3s, ETA:    84s
[>                               ] 17/500, 5.8 task/s, elapsed: 3s, ETA:    83s
[>                               ] 18/500, 5.9 task/s, elapsed: 3s, ETA:    82s
[>                               ] 19/500, 5.9 task/s, elapsed: 3s, ETA:    82s
[>                               ] 20/500, 5.9 task/s, elapsed: 3s, ETA:    81s
[>                               ] 21/500, 6.0 task/s, elapsed: 4s, ETA:    80s
[>                               ] 22/500, 6.0 task/s, elapsed: 4s, ETA:    80s
[>                               ] 23/500, 6.0 task/s, elapsed: 4s, ETA:    79s
[>                               ] 24/500, 6.1 task/s, elapsed: 4s, ETA:    79s
[>                               ] 25/500, 6.1 task/s, elapsed: 4s, ETA:    78s
[>                               ] 26/500, 6.1 task/s, elapsed: 4s, ETA:    78s
[>                               ] 27/500, 6.1 task/s, elapsed: 4s, ETA:    77s
[>                               ] 28/500, 6.1 task/s, elapsed: 5s, ETA:    77s
[>                               ] 29/500, 6.2 task/s, elapsed: 5s, ETA:    76s
[>                               ] 30/500, 6.2 task/s, elapsed: 5s, ETA:    76s
[>                               ] 31/500, 6.2 task/s, elapsed: 5s, ETA:    76s
[>>                              ] 32/500, 6.2 task/s, elapsed: 5s, ETA:    75s
[>>                              ] 33/500, 6.2 task/s, elapsed: 5s, ETA:    75s
[>>                              ] 34/500, 6.3 task/s, elapsed: 5s, ETA:    75s
[>>                              ] 35/500, 6.3 task/s, elapsed: 6s, ETA:    74s
[>>                              ] 36/500, 6.3 task/s, elapsed: 6s, ETA:    74s
[>>                              ] 37/500, 6.3 task/s, elapsed: 6s, ETA:    74s
[>>                              ] 38/500, 6.3 task/s, elapsed: 6s, ETA:    74s
[>>                              ] 39/500, 6.3 task/s, elapsed: 6s, ETA:    73s
[>>                              ] 40/500, 6.3 task/s, elapsed: 6s, ETA:    73s
[>>                              ] 41/500, 6.3 task/s, elapsed: 7s, ETA:    73s
[>>                              ] 42/500, 6.3 task/s, elapsed: 7s, ETA:    73s
[>>                              ] 43/500, 6.3 task/s, elapsed: 7s, ETA:    72s
[>>                              ] 44/500, 6.3 task/s, elapsed: 7s, ETA:    72s
[>>                              ] 45/500, 6.3 task/s, elapsed: 7s, ETA:    72s
[>>                              ] 46/500, 6.3 task/s, elapsed: 7s, ETA:    72s
[>>>                             ] 47/500, 6.3 task/s, elapsed: 7s, ETA:    72s
[>>>                             ] 48/500, 6.3 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 49/500, 6.3 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 50/500, 6.4 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 51/500, 6.4 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 52/500, 6.4 task/s, elapsed: 8s, ETA:    70s
[>>>                             ] 53/500, 6.4 task/s, elapsed: 8s, ETA:    70s
[>>>                             ] 54/500, 6.4 task/s, elapsed: 8s, ETA:    70s
[>>>                             ] 55/500, 6.4 task/s, elapsed: 9s, ETA:    70s
[>>>                             ] 56/500, 6.4 task/s, elapsed: 9s, ETA:    70s
[>>>                             ] 57/500, 6.4 task/s, elapsed: 9s, ETA:    69s
[>>>                             ] 58/500, 6.4 task/s, elapsed: 9s, ETA:    69s
[>>>                             ] 59/500, 6.4 task/s, elapsed: 9s, ETA:    69s
[>>>                             ] 60/500, 6.4 task/s, elapsed: 9s, ETA:    69s
[>>>                            ] 61/500, 6.4 task/s, elapsed: 10s, ETA:    69s
[>>>                            ] 62/500, 6.4 task/s, elapsed: 10s, ETA:    68s
[>>>                            ] 63/500, 6.4 task/s, elapsed: 10s, ETA:    68s
[>>>                            ] 64/500, 6.4 task/s, elapsed: 10s, ETA:    68s
[>>>>                           ] 65/500, 6.4 task/s, elapsed: 10s, ETA:    68s
[>>>>                           ] 66/500, 6.4 task/s, elapsed: 10s, ETA:    68s
[>>>>                           ] 67/500, 6.4 task/s, elapsed: 10s, ETA:    67s
[>>>>                           ] 68/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 69/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 70/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 71/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 72/500, 6.5 task/s, elapsed: 11s, ETA:    66s
[>>>>                           ] 73/500, 6.5 task/s, elapsed: 11s, ETA:    66s
[>>>>                           ] 74/500, 6.5 task/s, elapsed: 11s, ETA:    66s
[>>>>                           ] 75/500, 6.5 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 76/500, 6.5 task/s, elapsed: 12s, ETA:    65s
[>>>>                           ] 77/500, 6.5 task/s, elapsed: 12s, ETA:    65s
[>>>>                           ] 78/500, 6.5 task/s, elapsed: 12s, ETA:    65s
[>>>>                           ] 79/500, 6.5 task/s, elapsed: 12s, ETA:    65s
[>>>>                           ] 80/500, 6.5 task/s, elapsed: 12s, ETA:    65s
[>>>>>                          ] 81/500, 6.5 task/s, elapsed: 12s, ETA:    65s
[>>>>>                          ] 82/500, 6.5 task/s, elapsed: 13s, ETA:    64s
[>>>>>                          ] 83/500, 6.5 task/s, elapsed: 13s, ETA:    64s
[>>>>>                          ] 84/500, 6.5 task/s, elapsed: 13s, ETA:    64s
[>>>>>                          ] 85/500, 6.5 task/s, elapsed: 13s, ETA:    64s
[>>>>>                          ] 86/500, 6.5 task/s, elapsed: 13s, ETA:    63s
[>>>>>                          ] 87/500, 6.5 task/s, elapsed: 13s, ETA:    63s
[>>>>>                          ] 88/500, 6.5 task/s, elapsed: 13s, ETA:    63s
[>>>>>                          ] 89/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 90/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 91/500, 6.5 task/s, elapsed: 14s, ETA:    62s
[>>>>>                          ] 92/500, 6.6 task/s, elapsed: 14s, ETA:    62s
[>>>>>                          ] 93/500, 6.6 task/s, elapsed: 14s, ETA:    62s
[>>>>>                          ] 94/500, 6.6 task/s, elapsed: 14s, ETA:    62s
[>>>>>                          ] 95/500, 6.6 task/s, elapsed: 14s, ETA:    62s
[>>>>>                          ] 96/500, 6.6 task/s, elapsed: 15s, ETA:    62s
[>>>>>>                         ] 97/500, 6.6 task/s, elapsed: 15s, ETA:    61s
[>>>>>>                         ] 98/500, 6.6 task/s, elapsed: 15s, ETA:    61s
[>>>>>>                         ] 99/500, 6.6 task/s, elapsed: 15s, ETA:    61s
[>>>>>>                        ] 100/500, 6.6 task/s, elapsed: 15s, ETA:    61s
[>>>>>>                        ] 101/500, 6.6 task/s, elapsed: 15s, ETA:    61s
[>>>>>>                        ] 102/500, 6.6 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 103/500, 6.6 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 104/500, 6.6 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 105/500, 6.6 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 106/500, 6.6 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 107/500, 6.6 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 108/500, 6.6 task/s, elapsed: 16s, ETA:    59s
[>>>>>>                        ] 109/500, 6.6 task/s, elapsed: 16s, ETA:    59s
[>>>>>>                        ] 110/500, 6.6 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 111/500, 6.6 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 112/500, 6.6 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 113/500, 6.6 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 114/500, 6.6 task/s, elapsed: 17s, ETA:    58s
[>>>>>>                        ] 115/500, 6.6 task/s, elapsed: 17s, ETA:    58s
[>>>>>>                        ] 116/500, 6.6 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 117/500, 6.6 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 118/500, 6.6 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 119/500, 6.6 task/s, elapsed: 18s, ETA:    57s
[>>>>>>>                       ] 120/500, 6.6 task/s, elapsed: 18s, ETA:    57s
[>>>>>>>                       ] 121/500, 6.6 task/s, elapsed: 18s, ETA:    57s
[>>>>>>>                       ] 122/500, 6.6 task/s, elapsed: 18s, ETA:    57s
[>>>>>>>                       ] 123/500, 6.6 task/s, elapsed: 19s, ETA:    57s
[>>>>>>>                       ] 124/500, 6.6 task/s, elapsed: 19s, ETA:    57s
[>>>>>>>                       ] 125/500, 6.6 task/s, elapsed: 19s, ETA:    56s
[>>>>>>>                       ] 126/500, 6.6 task/s, elapsed: 19s, ETA:    56s
[>>>>>>>                       ] 127/500, 6.6 task/s, elapsed: 19s, ETA:    56s
[>>>>>>>                       ] 128/500, 6.6 task/s, elapsed: 19s, ETA:    56s
[>>>>>>>                       ] 129/500, 6.6 task/s, elapsed: 19s, ETA:    56s
[>>>>>>>                       ] 130/500, 6.6 task/s, elapsed: 20s, ETA:    56s
[>>>>>>>                       ] 131/500, 6.7 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>                       ] 132/500, 6.7 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>                       ] 133/500, 6.7 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>>                      ] 134/500, 6.7 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>>                      ] 135/500, 6.7 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>>                      ] 136/500, 6.7 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>>                      ] 137/500, 6.7 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 138/500, 6.7 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 139/500, 6.7 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 140/500, 6.7 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 141/500, 6.7 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 142/500, 6.7 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 143/500, 6.7 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 144/500, 6.7 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 145/500, 6.7 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 146/500, 6.7 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 147/500, 6.7 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 148/500, 6.7 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 149/500, 6.7 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>>                     ] 150/500, 6.7 task/s, elapsed: 22s, ETA:    52s
[>>>>>>>>>                     ] 151/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 152/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 153/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 154/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 155/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 156/500, 6.7 task/s, elapsed: 23s, ETA:    51s
[>>>>>>>>>                     ] 157/500, 6.7 task/s, elapsed: 23s, ETA:    51s
[>>>>>>>>>                     ] 158/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 159/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 160/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 161/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 162/500, 6.7 task/s, elapsed: 24s, ETA:    50s
[>>>>>>>>>                     ] 163/500, 6.7 task/s, elapsed: 24s, ETA:    50s
[>>>>>>>>>                     ] 164/500, 6.7 task/s, elapsed: 24s, ETA:    50s
[>>>>>>>>>                     ] 165/500, 6.7 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>                     ] 166/500, 6.7 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>>                    ] 167/500, 6.7 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>>                    ] 168/500, 6.7 task/s, elapsed: 25s, ETA:    49s
[>>>>>>>>>>                    ] 169/500, 6.7 task/s, elapsed: 25s, ETA:    49s
[>>>>>>>>>>                    ] 170/500, 6.7 task/s, elapsed: 25s, ETA:    49s
[>>>>>>>>>>                    ] 171/500, 6.7 task/s, elapsed: 25s, ETA:    49s
[>>>>>>>>>>                    ] 172/500, 6.7 task/s, elapsed: 26s, ETA:    49s
[>>>>>>>>>>                    ] 173/500, 6.7 task/s, elapsed: 26s, ETA:    49s
[>>>>>>>>>>                    ] 174/500, 6.7 task/s, elapsed: 26s, ETA:    49s
[>>>>>>>>>>                    ] 175/500, 6.7 task/s, elapsed: 26s, ETA:    48s
[>>>>>>>>>>                    ] 176/500, 6.7 task/s, elapsed: 26s, ETA:    48s
[>>>>>>>>>>                    ] 177/500, 6.7 task/s, elapsed: 26s, ETA:    48s
[>>>>>>>>>>                    ] 178/500, 6.7 task/s, elapsed: 26s, ETA:    48s
[>>>>>>>>>>                    ] 179/500, 6.7 task/s, elapsed: 27s, ETA:    48s
[>>>>>>>>>>                    ] 180/500, 6.7 task/s, elapsed: 27s, ETA:    48s
[>>>>>>>>>>                    ] 181/500, 6.7 task/s, elapsed: 27s, ETA:    47s
[>>>>>>>>>>                    ] 182/500, 6.7 task/s, elapsed: 27s, ETA:    47s
[>>>>>>>>>>                    ] 183/500, 6.7 task/s, elapsed: 27s, ETA:    47s
[>>>>>>>>>>>                   ] 184/500, 6.7 task/s, elapsed: 27s, ETA:    47s
[>>>>>>>>>>>                   ] 185/500, 6.7 task/s, elapsed: 27s, ETA:    47s
[>>>>>>>>>>>                   ] 186/500, 6.7 task/s, elapsed: 28s, ETA:    47s
[>>>>>>>>>>>                   ] 187/500, 6.7 task/s, elapsed: 28s, ETA:    47s
[>>>>>>>>>>>                   ] 188/500, 6.7 task/s, elapsed: 28s, ETA:    46s
[>>>>>>>>>>>                   ] 189/500, 6.7 task/s, elapsed: 28s, ETA:    46s
[>>>>>>>>>>>                   ] 190/500, 6.7 task/s, elapsed: 28s, ETA:    46s
[>>>>>>>>>>>                   ] 191/500, 6.7 task/s, elapsed: 28s, ETA:    46s
[>>>>>>>>>>>                   ] 192/500, 6.7 task/s, elapsed: 29s, ETA:    46s
[>>>>>>>>>>>                   ] 193/500, 6.7 task/s, elapsed: 29s, ETA:    46s
[>>>>>>>>>>>                   ] 194/500, 6.7 task/s, elapsed: 29s, ETA:    45s
[>>>>>>>>>>>                   ] 195/500, 6.7 task/s, elapsed: 29s, ETA:    45s
[>>>>>>>>>>>                   ] 196/500, 6.7 task/s, elapsed: 29s, ETA:    45s
[>>>>>>>>>>>                   ] 197/500, 6.7 task/s, elapsed: 29s, ETA:    45s
[>>>>>>>>>>>                   ] 198/500, 6.7 task/s, elapsed: 29s, ETA:    45s
[>>>>>>>>>>>                   ] 199/500, 6.7 task/s, elapsed: 30s, ETA:    45s
[>>>>>>>>>>>>                  ] 200/500, 6.7 task/s, elapsed: 30s, ETA:    45s
[>>>>>>>>>>>>                  ] 201/500, 6.7 task/s, elapsed: 30s, ETA:    44s
[>>>>>>>>>>>>                  ] 202/500, 6.7 task/s, elapsed: 30s, ETA:    44s
[>>>>>>>>>>>>                  ] 203/500, 6.7 task/s, elapsed: 30s, ETA:    44s
[>>>>>>>>>>>>                  ] 204/500, 6.7 task/s, elapsed: 30s, ETA:    44s
[>>>>>>>>>>>>                  ] 205/500, 6.7 task/s, elapsed: 30s, ETA:    44s
[>>>>>>>>>>>>                  ] 206/500, 6.7 task/s, elapsed: 31s, ETA:    44s
[>>>>>>>>>>>>                  ] 207/500, 6.7 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 208/500, 6.7 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 209/500, 6.7 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 210/500, 6.7 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 211/500, 6.7 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 212/500, 6.7 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 213/500, 6.7 task/s, elapsed: 32s, ETA:    43s
[>>>>>>>>>>>>                  ] 214/500, 6.7 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>                  ] 215/500, 6.7 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>                  ] 216/500, 6.7 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>>                 ] 217/500, 6.7 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>>                 ] 218/500, 6.7 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>>                 ] 219/500, 6.8 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>>                 ] 220/500, 6.8 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 221/500, 6.8 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 222/500, 6.8 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 223/500, 6.8 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 224/500, 6.8 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 225/500, 6.8 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 226/500, 6.8 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 227/500, 6.8 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 228/500, 6.8 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 229/500, 6.8 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 230/500, 6.8 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 231/500, 6.8 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 232/500, 6.8 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 233/500, 6.8 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>>                ] 234/500, 6.8 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 235/500, 6.8 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 236/500, 6.8 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 237/500, 6.8 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 238/500, 6.8 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 239/500, 6.8 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 240/500, 6.8 task/s, elapsed: 35s, ETA:    38s
[>>>>>>>>>>>>>>                ] 241/500, 6.8 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 242/500, 6.8 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 243/500, 6.8 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 244/500, 6.8 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 245/500, 6.8 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 246/500, 6.8 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 247/500, 6.8 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>                ] 248/500, 6.8 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>                ] 249/500, 6.8 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 250/500, 6.8 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 251/500, 6.8 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 252/500, 6.8 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 253/500, 6.8 task/s, elapsed: 37s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 254/500, 6.8 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 255/500, 6.8 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 256/500, 6.8 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 257/500, 6.8 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 258/500, 6.8 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 259/500, 6.8 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 260/500, 6.8 task/s, elapsed: 38s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 261/500, 6.8 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 262/500, 6.8 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 263/500, 6.8 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 264/500, 6.8 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 265/500, 6.8 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 266/500, 6.8 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 267/500, 6.8 task/s, elapsed: 39s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 268/500, 6.8 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 269/500, 6.8 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 270/500, 6.8 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 271/500, 6.8 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 272/500, 6.8 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 273/500, 6.8 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 274/500, 6.8 task/s, elapsed: 40s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 275/500, 6.8 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 276/500, 6.8 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 277/500, 6.8 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 278/500, 6.8 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 279/500, 6.8 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 280/500, 6.8 task/s, elapsed: 41s, ETA:    32s
[>>>>>>>>>>>>>>>>              ] 281/500, 6.8 task/s, elapsed: 41s, ETA:    32s
[>>>>>>>>>>>>>>>>              ] 282/500, 6.8 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>              ] 283/500, 6.8 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 284/500, 6.8 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 285/500, 6.8 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 286/500, 6.8 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 287/500, 6.8 task/s, elapsed: 42s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 288/500, 6.8 task/s, elapsed: 42s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 289/500, 6.8 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 290/500, 6.8 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 291/500, 6.8 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 292/500, 6.8 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 293/500, 6.8 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 294/500, 6.8 task/s, elapsed: 43s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 295/500, 6.8 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 296/500, 6.8 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 297/500, 6.8 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 298/500, 6.8 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 299/500, 6.8 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 300/500, 6.8 task/s, elapsed: 44s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 301/500, 6.8 task/s, elapsed: 44s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 302/500, 6.8 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 303/500, 6.8 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 304/500, 6.8 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 305/500, 6.8 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 306/500, 6.8 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 307/500, 6.8 task/s, elapsed: 45s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 308/500, 6.8 task/s, elapsed: 45s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 309/500, 6.8 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 310/500, 6.8 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 311/500, 6.8 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 312/500, 6.8 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 313/500, 6.8 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 314/500, 6.8 task/s, elapsed: 46s, ETA:    27s
[>>>>>>>>>>>>>>>>>>            ] 315/500, 6.8 task/s, elapsed: 46s, ETA:    27s
[>>>>>>>>>>>>>>>>>>            ] 316/500, 6.8 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 317/500, 6.8 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 318/500, 6.8 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 319/500, 6.8 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 320/500, 6.8 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 321/500, 6.8 task/s, elapsed: 47s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 322/500, 6.8 task/s, elapsed: 47s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 323/500, 6.8 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 324/500, 6.8 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 325/500, 6.8 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 326/500, 6.8 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 327/500, 6.8 task/s, elapsed: 48s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 328/500, 6.8 task/s, elapsed: 48s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 329/500, 6.8 task/s, elapsed: 48s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 330/500, 6.8 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 331/500, 6.8 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 332/500, 6.8 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 333/500, 6.8 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 334/500, 6.8 task/s, elapsed: 49s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 335/500, 6.8 task/s, elapsed: 49s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 336/500, 6.8 task/s, elapsed: 49s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 337/500, 6.8 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 338/500, 6.8 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 339/500, 6.8 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 340/500, 6.8 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 341/500, 6.8 task/s, elapsed: 50s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 342/500, 6.8 task/s, elapsed: 50s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 343/500, 6.8 task/s, elapsed: 50s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 344/500, 6.8 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 345/500, 6.8 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 346/500, 6.8 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 347/500, 6.8 task/s, elapsed: 51s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>          ] 348/500, 6.8 task/s, elapsed: 51s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>          ] 349/500, 6.8 task/s, elapsed: 51s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 350/500, 6.8 task/s, elapsed: 51s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 351/500, 6.8 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 352/500, 6.8 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 353/500, 6.8 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 354/500, 6.8 task/s, elapsed: 52s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 355/500, 6.8 task/s, elapsed: 52s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 356/500, 6.8 task/s, elapsed: 52s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 357/500, 6.8 task/s, elapsed: 52s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 358/500, 6.8 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 359/500, 6.8 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 360/500, 6.8 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 361/500, 6.8 task/s, elapsed: 53s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 362/500, 6.8 task/s, elapsed: 53s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 363/500, 6.8 task/s, elapsed: 53s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 364/500, 6.8 task/s, elapsed: 53s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 365/500, 6.8 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 366/500, 6.8 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 367/500, 6.8 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 368/500, 6.8 task/s, elapsed: 54s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 369/500, 6.8 task/s, elapsed: 54s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 370/500, 6.8 task/s, elapsed: 54s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 371/500, 6.8 task/s, elapsed: 54s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 372/500, 6.8 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 373/500, 6.8 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 374/500, 6.8 task/s, elapsed: 55s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 375/500, 6.8 task/s, elapsed: 55s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 376/500, 6.8 task/s, elapsed: 55s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 377/500, 6.8 task/s, elapsed: 55s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 378/500, 6.8 task/s, elapsed: 55s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 379/500, 6.8 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 380/500, 6.8 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 381/500, 6.8 task/s, elapsed: 56s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>        ] 382/500, 6.8 task/s, elapsed: 56s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>        ] 383/500, 6.8 task/s, elapsed: 56s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 384/500, 6.8 task/s, elapsed: 56s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 385/500, 6.8 task/s, elapsed: 56s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 386/500, 6.8 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 387/500, 6.8 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 388/500, 6.8 task/s, elapsed: 57s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 389/500, 6.8 task/s, elapsed: 57s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 390/500, 6.8 task/s, elapsed: 57s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 391/500, 6.8 task/s, elapsed: 57s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 392/500, 6.8 task/s, elapsed: 57s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 393/500, 6.8 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 394/500, 6.8 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 395/500, 6.8 task/s, elapsed: 58s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 396/500, 6.8 task/s, elapsed: 58s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 397/500, 6.8 task/s, elapsed: 58s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 398/500, 6.8 task/s, elapsed: 58s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 399/500, 6.8 task/s, elapsed: 58s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 400/500, 6.8 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 401/500, 6.8 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 402/500, 6.8 task/s, elapsed: 59s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 403/500, 6.8 task/s, elapsed: 59s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 404/500, 6.8 task/s, elapsed: 59s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 405/500, 6.8 task/s, elapsed: 59s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 406/500, 6.8 task/s, elapsed: 59s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 407/500, 6.8 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 408/500, 6.8 task/s, elapsed: 60s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 409/500, 6.8 task/s, elapsed: 60s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 410/500, 6.8 task/s, elapsed: 60s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 411/500, 6.8 task/s, elapsed: 60s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 412/500, 6.8 task/s, elapsed: 60s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 413/500, 6.8 task/s, elapsed: 60s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 414/500, 6.8 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 415/500, 6.8 task/s, elapsed: 61s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 416/500, 6.8 task/s, elapsed: 61s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 417/500, 6.8 task/s, elapsed: 61s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 418/500, 6.8 task/s, elapsed: 61s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 419/500, 6.8 task/s, elapsed: 61s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 420/500, 6.8 task/s, elapsed: 61s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 421/500, 6.8 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 422/500, 6.8 task/s, elapsed: 62s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 423/500, 6.8 task/s, elapsed: 62s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 424/500, 6.8 task/s, elapsed: 62s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 425/500, 6.8 task/s, elapsed: 62s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 426/500, 6.8 task/s, elapsed: 62s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 427/500, 6.8 task/s, elapsed: 62s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 428/500, 6.8 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 429/500, 6.8 task/s, elapsed: 63s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 430/500, 6.8 task/s, elapsed: 63s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 431/500, 6.8 task/s, elapsed: 63s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 432/500, 6.8 task/s, elapsed: 63s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 433/500, 6.8 task/s, elapsed: 63s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 434/500, 6.8 task/s, elapsed: 63s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 435/500, 6.8 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 436/500, 6.8 task/s, elapsed: 64s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 437/500, 6.8 task/s, elapsed: 64s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 438/500, 6.8 task/s, elapsed: 64s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 439/500, 6.8 task/s, elapsed: 64s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 440/500, 6.8 task/s, elapsed: 64s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 441/500, 6.8 task/s, elapsed: 64s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 442/500, 6.8 task/s, elapsed: 65s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 443/500, 6.8 task/s, elapsed: 65s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 444/500, 6.8 task/s, elapsed: 65s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 445/500, 6.8 task/s, elapsed: 65s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 446/500, 6.8 task/s, elapsed: 65s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 447/500, 6.8 task/s, elapsed: 65s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 448/500, 6.8 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 449/500, 6.8 task/s, elapsed: 66s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 450/500, 6.8 task/s, elapsed: 66s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 451/500, 6.8 task/s, elapsed: 66s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 452/500, 6.8 task/s, elapsed: 66s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 453/500, 6.8 task/s, elapsed: 66s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 454/500, 6.8 task/s, elapsed: 66s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 455/500, 6.8 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 456/500, 6.8 task/s, elapsed: 67s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 457/500, 6.8 task/s, elapsed: 67s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 458/500, 6.8 task/s, elapsed: 67s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 459/500, 6.8 task/s, elapsed: 67s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 460/500, 6.8 task/s, elapsed: 67s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 461/500, 6.8 task/s, elapsed: 67s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 462/500, 6.8 task/s, elapsed: 68s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 463/500, 6.8 task/s, elapsed: 68s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 464/500, 6.8 task/s, elapsed: 68s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 465/500, 6.8 task/s, elapsed: 68s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 466/500, 6.8 task/s, elapsed: 68s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 467/500, 6.8 task/s, elapsed: 68s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 468/500, 6.8 task/s, elapsed: 69s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 469/500, 6.8 task/s, elapsed: 69s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 470/500, 6.8 task/s, elapsed: 69s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 471/500, 6.8 task/s, elapsed: 69s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 472/500, 6.8 task/s, elapsed: 69s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 473/500, 6.8 task/s, elapsed: 69s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 474/500, 6.8 task/s, elapsed: 69s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 475/500, 6.8 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 476/500, 6.8 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 477/500, 6.8 task/s, elapsed: 70s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 478/500, 6.8 task/s, elapsed: 70s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 479/500, 6.8 task/s, elapsed: 70s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 480/500, 6.8 task/s, elapsed: 70s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 481/500, 6.8 task/s, elapsed: 70s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 482/500, 6.8 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 483/500, 6.8 task/s, elapsed: 71s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 484/500, 6.8 task/s, elapsed: 71s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 485/500, 6.8 task/s, elapsed: 71s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 486/500, 6.8 task/s, elapsed: 71s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 487/500, 6.8 task/s, elapsed: 71s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 488/500, 6.8 task/s, elapsed: 71s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 489/500, 6.8 task/s, elapsed: 72s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 490/500, 6.8 task/s, elapsed: 72s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 491/500, 6.8 task/s, elapsed: 72s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 492/500, 6.8 task/s, elapsed: 72s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 493/500, 6.8 task/s, elapsed: 72s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 494/500, 6.8 task/s, elapsed: 72s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 495/500, 6.8 task/s, elapsed: 72s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 496/500, 6.8 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 497/500, 6.8 task/s, elapsed: 73s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 498/500, 6.8 task/s, elapsed: 73s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 499/500, 6.8 task/s, elapsed: 73s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 500/500, 6.8 task/s, elapsed: 73s, ETA:     0s                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             2022-04-18 18:26:05,167 - mmseg - INFO - per class results:
2022-04-18 18:26:05,169 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|      road     |  85.8 | 88.45 |
|    sidewalk   |  42.2 |  78.3 |
|    building   | 86.46 | 93.95 |
|      wall     | 23.86 | 27.24 |
|     fence     |  5.62 |  6.24 |
|      pole     | 45.49 | 54.41 |
| traffic light | 51.61 | 65.99 |
|  traffic sign | 50.84 | 62.47 |
|   vegetation  | 85.59 | 94.48 |
|    terrain    |  0.0  |  0.0  |
|      sky      | 87.27 | 99.29 |
|     person    | 70.45 |  83.0 |
|     rider     | 41.78 | 64.91 |
|      car      | 86.61 | 97.16 |
|     truck     |  0.0  |  0.0  |
|      bus      | 55.34 | 74.37 |
|     train     |  0.0  |  0.0  |
|   motorcycle  |  55.7 | 70.19 |
|    bicycle    | 61.96 | 78.55 |
+---------------+-------+-------+
2022-04-18 18:26:05,169 - mmseg - INFO - Summary:
2022-04-18 18:26:05,170 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 87.87 | 49.29 | 59.95 |
+-------+-------+-------+
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:26:05,181 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-18 18:26:05,181 - mmseg - INFO - Iter [500/40000]	lr: 4.200e-05, eta: 11:55:23, time: 1.534, data_time: 0.013, memory: 9636, aAcc: 0.8787, mIoU: 0.4929, mAcc: 0.5995, IoU.road: 0.8580, IoU.sidewalk: 0.4220, IoU.building: 0.8646, IoU.wall: 0.2386, IoU.fence: 0.0562, IoU.pole: 0.4549, IoU.traffic light: 0.5161, IoU.traffic sign: 0.5084, IoU.vegetation: 0.8559, IoU.terrain: 0.0000, IoU.sky: 0.8727, IoU.person: 0.7045, IoU.rider: 0.4178, IoU.car: 0.8661, IoU.truck: 0.0000, IoU.bus: 0.5534, IoU.train: 0.0000, IoU.motorcycle: 0.5570, IoU.bicycle: 0.6196, Acc.road: 0.8845, Acc.sidewalk: 0.7830, Acc.building: 0.9395, Acc.wall: 0.2724, Acc.fence: 0.0624, Acc.pole: 0.5441, Acc.traffic light: 0.6599, Acc.traffic sign: 0.6247, Acc.vegetation: 0.9448, Acc.terrain: 0.0000, Acc.sky: 0.9929, Acc.person: 0.8300, Acc.rider: 0.6491, Acc.car: 0.9716, Acc.truck: 0.0000, Acc.bus: 0.7437, Acc.train: 0.0000, Acc.motorcycle: 0.7019, Acc.bicycle: 0.7855, decode.loss_seg: 0.1336, decode.acc_seg: 90.2864, src.loss_imnet_feat_dist: 0.1211, mix.decode.loss_seg: 0.1405, mix.decode.acc_seg: 88.3787
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:27:19,952 - mmseg - INFO - Iter [12050/40000]	lr: 4.193e-05, eta: 11:58:34, time: 3.837, data_time: 2.356, memory: 9636, decode.loss_seg: 0.1225, decode.acc_seg: 89.6218, src.loss_imnet_feat_dist: 0.1166, mix.decode.loss_seg: 0.1580, mix.decode.acc_seg: 87.7162
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:28:37,277 - mmseg - INFO - Iter [12100/40000]	lr: 4.185e-05, eta: 11:57:17, time: 1.546, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1262, decode.acc_seg: 89.9058, src.loss_imnet_feat_dist: 0.1107, mix.decode.loss_seg: 0.1493, mix.decode.acc_seg: 88.6124
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:29:56,588 - mmseg - INFO - Iter [12150/40000]	lr: 4.178e-05, eta: 11:56:05, time: 1.586, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1432, decode.acc_seg: 89.8412, src.loss_imnet_feat_dist: 0.1102, mix.decode.loss_seg: 0.1677, mix.decode.acc_seg: 88.4064
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:31:13,165 - mmseg - INFO - Iter [12200/40000]	lr: 4.170e-05, eta: 11:54:46, time: 1.532, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1367, decode.acc_seg: 90.6402, src.loss_imnet_feat_dist: 0.1153, mix.decode.loss_seg: 0.1435, mix.decode.acc_seg: 88.4816
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:32:29,261 - mmseg - INFO - Iter [12250/40000]	lr: 4.163e-05, eta: 11:53:27, time: 1.522, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1347, decode.acc_seg: 89.9968, src.loss_imnet_feat_dist: 0.1089, mix.decode.loss_seg: 0.1491, mix.decode.acc_seg: 88.8887
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:33:45,954 - mmseg - INFO - Iter [12300/40000]	lr: 4.155e-05, eta: 11:52:09, time: 1.534, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1322, decode.acc_seg: 90.1581, src.loss_imnet_feat_dist: 0.1133, mix.decode.loss_seg: 0.1561, mix.decode.acc_seg: 88.2669
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:34:58,782 - mmseg - INFO - Iter [12350/40000]	lr: 4.148e-05, eta: 11:50:42, time: 1.457, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1438, decode.acc_seg: 89.8662, src.loss_imnet_feat_dist: 0.1087, mix.decode.loss_seg: 0.1523, mix.decode.acc_seg: 89.2423
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:36:10,159 - mmseg - INFO - Iter [12400/40000]	lr: 4.140e-05, eta: 11:49:12, time: 1.428, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1230, decode.acc_seg: 91.0633, src.loss_imnet_feat_dist: 0.1139, mix.decode.loss_seg: 0.1447, mix.decode.acc_seg: 88.8825
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:37:22,887 - mmseg - INFO - Iter [12450/40000]	lr: 4.133e-05, eta: 11:47:45, time: 1.455, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1252, decode.acc_seg: 89.8665, src.loss_imnet_feat_dist: 0.1096, mix.decode.loss_seg: 0.1685, mix.decode.acc_seg: 87.5834
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:38:37,875 - mmseg - INFO - Iter [12500/40000]	lr: 4.125e-05, eta: 11:46:24, time: 1.500, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1494, decode.acc_seg: 89.7776, src.loss_imnet_feat_dist: 0.1154, mix.decode.loss_seg: 0.1718, mix.decode.acc_seg: 87.8991
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:39:39,252 - mmseg - INFO - Iter [5700/40000]	lr: 5.145e-05, eta: 1 day, 8:23:08, time: 3.322, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.9286, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.9346
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:42:26,653 - mmseg - INFO - Iter [5750/40000]	lr: 5.138e-05, eta: 1 day, 8:20:03, time: 3.348, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.1502, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.1719
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:45:14,332 - mmseg - INFO - Iter [5800/40000]	lr: 5.130e-05, eta: 1 day, 8:17:00, time: 3.354, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.3739, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.5690
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:46:14,265 - mmseg - INFO - Iter [12800/40000]	lr: 4.080e-05, eta: 11:38:29, time: 1.487, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1357, decode.acc_seg: 89.8539, src.loss_imnet_feat_dist: 0.1112, mix.decode.loss_seg: 0.1505, mix.decode.acc_seg: 87.9070
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:47:30,049 - mmseg - INFO - Iter [12850/40000]	lr: 4.073e-05, eta: 11:37:09, time: 1.516, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1374, decode.acc_seg: 89.5330, src.loss_imnet_feat_dist: 0.1114, mix.decode.loss_seg: 0.1436, mix.decode.acc_seg: 88.1747
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:48:46,555 - mmseg - INFO - Iter [12900/40000]	lr: 4.065e-05, eta: 11:35:51, time: 1.530, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1254, decode.acc_seg: 90.2867, src.loss_imnet_feat_dist: 0.1063, mix.decode.loss_seg: 0.1642, mix.decode.acc_seg: 88.6076
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:50:05,850 - mmseg - INFO - Iter [12950/40000]	lr: 4.058e-05, eta: 11:34:38, time: 1.586, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1338, decode.acc_seg: 90.7729, src.loss_imnet_feat_dist: 0.1102, mix.decode.loss_seg: 0.1569, mix.decode.acc_seg: 89.0010
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:51:22,933 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-18 18:51:22,933 - mmseg - INFO - Iter [13000/40000]	lr: 4.050e-05, eta: 11:33:22, time: 1.542, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1300, decode.acc_seg: 90.2545, src.loss_imnet_feat_dist: 0.1114, mix.decode.loss_seg: 0.1507, mix.decode.acc_seg: 88.6158
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:52:38,379 - mmseg - INFO - Iter [13050/40000]	lr: 4.043e-05, eta: 11:32:01, time: 1.509, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1474, decode.acc_seg: 89.5470, src.loss_imnet_feat_dist: 0.1166, mix.decode.loss_seg: 0.1507, mix.decode.acc_seg: 89.2674
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:53:54,796 - mmseg - INFO - Iter [13100/40000]	lr: 4.035e-05, eta: 11:30:43, time: 1.528, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1530, decode.acc_seg: 89.5169, src.loss_imnet_feat_dist: 0.1231, mix.decode.loss_seg: 0.2025, mix.decode.acc_seg: 86.7905
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:55:13,212 - mmseg - INFO - Iter [13150/40000]	lr: 4.028e-05, eta: 11:29:29, time: 1.568, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1384, decode.acc_seg: 89.6290, src.loss_imnet_feat_dist: 0.1154, mix.decode.loss_seg: 0.1723, mix.decode.acc_seg: 87.3539
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:56:27,071 - mmseg - INFO - Iter [13200/40000]	lr: 4.020e-05, eta: 11:28:05, time: 1.477, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1583, decode.acc_seg: 88.4297, src.loss_imnet_feat_dist: 0.1201, mix.decode.loss_seg: 0.1647, mix.decode.acc_seg: 87.1189
                                                                                                                                           /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:57:39,820 - mmseg - INFO - Iter [13250/40000]	lr: 4.013e-05, eta: 11:26:40, time: 1.455, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1502, decode.acc_seg: 89.3880, src.loss_imnet_feat_dist: 0.1188, mix.decode.loss_seg: 0.1699, mix.decode.acc_seg: 87.4337
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 18:58:52,025 - mmseg - INFO - Iter [13300/40000]	lr: 4.005e-05, eta: 11:25:13, time: 1.444, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1418, decode.acc_seg: 90.3261, src.loss_imnet_feat_dist: 0.1133, mix.decode.loss_seg: 0.1642, mix.decode.acc_seg: 88.4504
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:00:05,040 - mmseg - INFO - Iter [13350/40000]	lr: 3.998e-05, eta: 11:23:48, time: 1.460, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1355, decode.acc_seg: 90.2426, src.loss_imnet_feat_dist: 0.1169, mix.decode.loss_seg: 0.1609, mix.decode.acc_seg: 88.7170
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:01:16,233 - mmseg - INFO - Iter [13400/40000]	lr: 3.990e-05, eta: 11:22:20, time: 1.424, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1335, decode.acc_seg: 90.0646, src.loss_imnet_feat_dist: 0.1159, mix.decode.loss_seg: 0.1455, mix.decode.acc_seg: 88.4506
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:02:29,090 - mmseg - INFO - Iter [13450/40000]	lr: 3.983e-05, eta: 11:20:54, time: 1.457, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1245, decode.acc_seg: 90.6805, src.loss_imnet_feat_dist: 0.1152, mix.decode.loss_seg: 0.1346, mix.decode.acc_seg: 89.5467
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:03:47,026 - mmseg - INFO - Iter [13500/40000]	lr: 3.975e-05, eta: 11:19:39, time: 1.559, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1130, decode.acc_seg: 91.0389, src.loss_imnet_feat_dist: 0.1047, mix.decode.loss_seg: 0.1282, mix.decode.acc_seg: 88.8807
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:04:52,301 - mmseg - INFO - Iter [6150/40000]	lr: 5.078e-05, eta: 1 day, 7:56:07, time: 3.363, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.6185, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.0623
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:07:38,774 - mmseg - INFO - Iter [6200/40000]	lr: 5.070e-05, eta: 1 day, 7:52:59, time: 3.329, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.3528, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.8855
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:10:28,031 - mmseg - INFO - Iter [6250/40000]	lr: 5.063e-05, eta: 1 day, 7:50:07, time: 3.385, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.1573, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 63.4724
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:13:16,066 - mmseg - INFO - Iter [6300/40000]	lr: 5.055e-05, eta: 1 day, 7:47:08, time: 3.361, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.4462, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.6353
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:16:04,601 - mmseg - INFO - Iter [6350/40000]	lr: 5.048e-05, eta: 1 day, 7:44:11, time: 3.371, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.8834, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.4938
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:16:35,215 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-18 19:16:35,216 - mmseg - INFO - Iter [14000/40000]	lr: 3.900e-05, eta: 11:06:48, time: 1.539, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1268, decode.acc_seg: 90.7070, src.loss_imnet_feat_dist: 0.1119, mix.decode.loss_seg: 0.1356, mix.decode.acc_seg: 89.6199
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:17:50,330 - mmseg - INFO - Iter [14050/40000]	lr: 3.893e-05, eta: 11:05:27, time: 1.502, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1290, decode.acc_seg: 89.8605, src.loss_imnet_feat_dist: 0.1054, mix.decode.loss_seg: 0.1444, mix.decode.acc_seg: 88.6744
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:19:06,692 - mmseg - INFO - Iter [14100/40000]	lr: 3.885e-05, eta: 11:04:09, time: 1.527, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1357, decode.acc_seg: 89.3961, src.loss_imnet_feat_dist: 0.1098, mix.decode.loss_seg: 0.1488, mix.decode.acc_seg: 88.4327
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:20:24,290 - mmseg - INFO - Iter [14150/40000]	lr: 3.878e-05, eta: 11:02:54, time: 1.552, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1178, decode.acc_seg: 90.8648, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1532, mix.decode.acc_seg: 88.5519
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:21:41,752 - mmseg - INFO - Iter [14200/40000]	lr: 3.870e-05, eta: 11:01:38, time: 1.549, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1314, decode.acc_seg: 90.5712, src.loss_imnet_feat_dist: 0.1141, mix.decode.loss_seg: 0.1383, mix.decode.acc_seg: 89.3075
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:22:57,987 - mmseg - INFO - Iter [14250/40000]	lr: 3.863e-05, eta: 11:00:20, time: 1.525, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1253, decode.acc_seg: 90.5364, src.loss_imnet_feat_dist: 0.1042, mix.decode.loss_seg: 0.1492, mix.decode.acc_seg: 88.2332
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:24:11,551 - mmseg - INFO - Iter [14300/40000]	lr: 3.855e-05, eta: 10:58:57, time: 1.471, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1322, decode.acc_seg: 90.2974, src.loss_imnet_feat_dist: 0.1109, mix.decode.loss_seg: 0.1460, mix.decode.acc_seg: 89.3601
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:25:29,832 - mmseg - INFO - Iter [14350/40000]	lr: 3.848e-05, eta: 10:57:42, time: 1.566, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1303, decode.acc_seg: 90.3824, src.loss_imnet_feat_dist: 0.1118, mix.decode.loss_seg: 0.1554, mix.decode.acc_seg: 88.4834
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:26:43,395 - mmseg - INFO - Iter [14400/40000]	lr: 3.840e-05, eta: 10:56:19, time: 1.471, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1353, decode.acc_seg: 89.6097, src.loss_imnet_feat_dist: 0.1091, mix.decode.loss_seg: 0.1387, mix.decode.acc_seg: 87.9627
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:28:03,001 - mmseg - INFO - Iter [14450/40000]	lr: 3.833e-05, eta: 10:55:07, time: 1.592, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1363, decode.acc_seg: 90.0529, src.loss_imnet_feat_dist: 0.1106, mix.decode.loss_seg: 0.1566, mix.decode.acc_seg: 88.5042
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:29:19,137 - mmseg - INFO - Iter [14500/40000]	lr: 3.825e-05, eta: 10:53:49, time: 1.523, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1204, decode.acc_seg: 90.8749, src.loss_imnet_feat_dist: 0.1072, mix.decode.loss_seg: 0.1620, mix.decode.acc_seg: 88.4894
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:30:37,809 - mmseg - INFO - Iter [14550/40000]	lr: 3.818e-05, eta: 10:52:35, time: 1.573, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1352, decode.acc_seg: 90.1199, src.loss_imnet_feat_dist: 0.1111, mix.decode.loss_seg: 0.1518, mix.decode.acc_seg: 88.6739
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:31:53,431 - mmseg - INFO - Iter [14600/40000]	lr: 3.810e-05, eta: 10:51:16, time: 1.512, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1239, decode.acc_seg: 89.7478, src.loss_imnet_feat_dist: 0.1017, mix.decode.loss_seg: 0.1499, mix.decode.acc_seg: 87.9458
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:33:06,648 - mmseg - INFO - Iter [14650/40000]	lr: 3.803e-05, eta: 10:49:52, time: 1.464, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1202, decode.acc_seg: 90.8454, src.loss_imnet_feat_dist: 0.0989, mix.decode.loss_seg: 0.1414, mix.decode.acc_seg: 89.3849
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:34:23,642 - mmseg - INFO - Iter [14700/40000]	lr: 3.795e-05, eta: 10:48:36, time: 1.540, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1270, decode.acc_seg: 90.4546, src.loss_imnet_feat_dist: 0.1055, mix.decode.loss_seg: 0.1418, mix.decode.acc_seg: 88.7618
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:35:40,430 - mmseg - INFO - Iter [14750/40000]	lr: 3.788e-05, eta: 10:47:18, time: 1.536, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1456, decode.acc_seg: 89.6177, src.loss_imnet_feat_dist: 0.1144, mix.decode.loss_seg: 0.1774, mix.decode.acc_seg: 87.5843
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:36:58,353 - mmseg - INFO - Iter [14800/40000]	lr: 3.780e-05, eta: 10:46:03, time: 1.558, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1255, decode.acc_seg: 90.3574, src.loss_imnet_feat_dist: 0.1120, mix.decode.loss_seg: 0.1534, mix.decode.acc_seg: 88.6603
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:38:17,953 - mmseg - INFO - Iter [14850/40000]	lr: 3.773e-05, eta: 10:44:51, time: 1.592, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1167, decode.acc_seg: 90.8200, src.loss_imnet_feat_dist: 0.1126, mix.decode.loss_seg: 0.1396, mix.decode.acc_seg: 89.9515
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:39:30,343 - mmseg - INFO - Iter [14900/40000]	lr: 3.765e-05, eta: 10:43:26, time: 1.448, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1231, decode.acc_seg: 90.5306, src.loss_imnet_feat_dist: 0.1171, mix.decode.loss_seg: 0.1295, mix.decode.acc_seg: 89.3358
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:40:49,441 - mmseg - INFO - Iter [14950/40000]	lr: 3.758e-05, eta: 10:42:13, time: 1.582, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1361, decode.acc_seg: 90.8652, src.loss_imnet_feat_dist: 0.1120, mix.decode.loss_seg: 0.1647, mix.decode.acc_seg: 87.9024
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:42:05,215 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-18 19:42:05,215 - mmseg - INFO - Iter [15000/40000]	lr: 3.750e-05, eta: 10:40:54, time: 1.515, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1298, decode.acc_seg: 89.7814, src.loss_imnet_feat_dist: 0.1080, mix.decode.loss_seg: 0.1376, mix.decode.acc_seg: 87.9476
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:43:22,239 - mmseg - INFO - Iter [15050/40000]	lr: 3.743e-05, eta: 10:39:38, time: 1.540, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1284, decode.acc_seg: 90.6655, src.loss_imnet_feat_dist: 0.1123, mix.decode.loss_seg: 0.1523, mix.decode.acc_seg: 88.2558
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:44:39,544 - mmseg - INFO - Iter [15100/40000]	lr: 3.735e-05, eta: 10:38:21, time: 1.546, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1260, decode.acc_seg: 89.7677, src.loss_imnet_feat_dist: 0.1113, mix.decode.loss_seg: 0.1378, mix.decode.acc_seg: 88.2364
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:45:58,649 - mmseg - INFO - Iter [15150/40000]	lr: 3.728e-05, eta: 10:37:08, time: 1.582, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1214, decode.acc_seg: 90.5363, src.loss_imnet_feat_dist: 0.1118, mix.decode.loss_seg: 0.1309, mix.decode.acc_seg: 88.9712
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:47:15,008 - mmseg - INFO - Iter [15200/40000]	lr: 3.720e-05, eta: 10:35:50, time: 1.527, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1271, decode.acc_seg: 90.5869, src.loss_imnet_feat_dist: 0.1114, mix.decode.loss_seg: 0.1550, mix.decode.acc_seg: 88.5365
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:48:27,548 - mmseg - INFO - Iter [15250/40000]	lr: 3.713e-05, eta: 10:34:26, time: 1.451, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1356, decode.acc_seg: 90.4745, src.loss_imnet_feat_dist: 0.1138, mix.decode.loss_seg: 0.1486, mix.decode.acc_seg: 88.9601
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:49:40,801 - mmseg - INFO - Iter [15300/40000]	lr: 3.705e-05, eta: 10:33:03, time: 1.465, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1252, decode.acc_seg: 90.4895, src.loss_imnet_feat_dist: 0.1116, mix.decode.loss_seg: 0.1527, mix.decode.acc_seg: 88.3127
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:50:59,878 - mmseg - INFO - Iter [15350/40000]	lr: 3.698e-05, eta: 10:31:50, time: 1.582, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1259, decode.acc_seg: 89.8467, src.loss_imnet_feat_dist: 0.1047, mix.decode.loss_seg: 0.1618, mix.decode.acc_seg: 87.9051
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:52:13,946 - mmseg - INFO - Iter [15400/40000]	lr: 3.690e-05, eta: 10:30:29, time: 1.481, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1971, decode.acc_seg: 90.0164, src.loss_imnet_feat_dist: 0.1134, mix.decode.loss_seg: 0.1826, mix.decode.acc_seg: 87.8344
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:53:30,809 - mmseg - INFO - Iter [15450/40000]	lr: 3.683e-05, eta: 10:29:12, time: 1.537, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1943, decode.acc_seg: 88.0510, src.loss_imnet_feat_dist: 0.1228, mix.decode.loss_seg: 0.2171, mix.decode.acc_seg: 84.9933
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:54:43,822 - mmseg - INFO - Iter [15500/40000]	lr: 3.675e-05, eta: 10:27:49, time: 1.460, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1307, decode.acc_seg: 91.1555, src.loss_imnet_feat_dist: 0.1152, mix.decode.loss_seg: 0.1704, mix.decode.acc_seg: 88.1568
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:55:19,225 - mmseg - INFO - Iter [7050/40000]	lr: 4.943e-05, eta: 1 day, 7:02:51, time: 3.362, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.2996, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.6045
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 19:58:05,214 - mmseg - INFO - Iter [7100/40000]	lr: 4.935e-05, eta: 1 day, 6:59:45, time: 3.320, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 24.1817, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 65.9975
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:00:53,946 - mmseg - INFO - Iter [7150/40000]	lr: 4.928e-05, eta: 1 day, 6:56:51, time: 3.375, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.9610, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.1871
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:03:37,970 - mmseg - INFO - Iter [15850/40000]	lr: 3.623e-05, eta: 10:18:44, time: 1.592, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1341, decode.acc_seg: 90.6829, src.loss_imnet_feat_dist: 0.1093, mix.decode.loss_seg: 0.1552, mix.decode.acc_seg: 89.2153
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:04:55,454 - mmseg - INFO - Iter [15900/40000]	lr: 3.615e-05, eta: 10:17:29, time: 1.550, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1247, decode.acc_seg: 89.1854, src.loss_imnet_feat_dist: 0.1085, mix.decode.loss_seg: 0.1471, mix.decode.acc_seg: 88.6252
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:06:13,963 - mmseg - INFO - Iter [15950/40000]	lr: 3.608e-05, eta: 10:16:14, time: 1.570, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1373, decode.acc_seg: 90.3232, src.loss_imnet_feat_dist: 0.1065, mix.decode.loss_seg: 0.1437, mix.decode.acc_seg: 89.2496
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [                                                  ] 0/500, elapsed: 0s, ETA:
[                                 ] 1/500, 1.5 task/s, elapsed: 1s, ETA:   329s
[                                 ] 2/500, 2.4 task/s, elapsed: 1s, ETA:   206s
[                                 ] 3/500, 3.1 task/s, elapsed: 1s, ETA:   161s
[                                 ] 4/500, 3.6 task/s, elapsed: 1s, ETA:   139s
[                                 ] 5/500, 4.0 task/s, elapsed: 1s, ETA:   125s
[                                 ] 6/500, 4.2 task/s, elapsed: 1s, ETA:   117s
[                                 ] 7/500, 4.5 task/s, elapsed: 2s, ETA:   110s
[                                 ] 8/500, 4.7 task/s, elapsed: 2s, ETA:   105s
[                                 ] 9/500, 4.9 task/s, elapsed: 2s, ETA:   101s
[                                ] 10/500, 5.0 task/s, elapsed: 2s, ETA:    98s
[                                ] 11/500, 5.1 task/s, elapsed: 2s, ETA:    95s
[                                ] 12/500, 5.2 task/s, elapsed: 2s, ETA:    93s
[                                ] 13/500, 5.3 task/s, elapsed: 2s, ETA:    91s
[                                ] 14/500, 5.4 task/s, elapsed: 3s, ETA:    89s
[                                ] 15/500, 5.5 task/s, elapsed: 3s, ETA:    88s
[>                               ] 16/500, 5.6 task/s, elapsed: 3s, ETA:    87s
[>                               ] 17/500, 5.6 task/s, elapsed: 3s, ETA:    86s
[>                               ] 18/500, 5.7 task/s, elapsed: 3s, ETA:    85s
[>                               ] 19/500, 5.7 task/s, elapsed: 3s, ETA:    84s
[>                               ] 20/500, 5.7 task/s, elapsed: 3s, ETA:    84s
[>                               ] 21/500, 5.8 task/s, elapsed: 4s, ETA:    83s
[>                               ] 22/500, 5.8 task/s, elapsed: 4s, ETA:    82s
[>                               ] 23/500, 5.9 task/s, elapsed: 4s, ETA:    81s
[>                               ] 24/500, 5.9 task/s, elapsed: 4s, ETA:    81s
[>                               ] 25/500, 5.9 task/s, elapsed: 4s, ETA:    80s
[>                               ] 26/500, 5.9 task/s, elapsed: 4s, ETA:    80s
[>                               ] 27/500, 6.0 task/s, elapsed: 5s, ETA:    79s
[>                               ] 28/500, 6.0 task/s, elapsed: 5s, ETA:    79s
[>                               ] 29/500, 6.0 task/s, elapsed: 5s, ETA:    79s
[>                               ] 30/500, 6.0 task/s, elapsed: 5s, ETA:    78s
[>                               ] 31/500, 6.0 task/s, elapsed: 5s, ETA:    78s
[>>                              ] 32/500, 6.1 task/s, elapsed: 5s, ETA:    77s
[>>                              ] 33/500, 6.1 task/s, elapsed: 5s, ETA:    77s
[>>                              ] 34/500, 6.1 task/s, elapsed: 6s, ETA:    77s
[>>                              ] 35/500, 6.1 task/s, elapsed: 6s, ETA:    76s
[>>                              ] 36/500, 6.1 task/s, elapsed: 6s, ETA:    76s
[>>                              ] 37/500, 6.1 task/s, elapsed: 6s, ETA:    76s
[>>                              ] 38/500, 6.1 task/s, elapsed: 6s, ETA:    75s
[>>                              ] 39/500, 6.2 task/s, elapsed: 6s, ETA:    75s
[>>                              ] 40/500, 6.2 task/s, elapsed: 6s, ETA:    75s
[>>                              ] 41/500, 6.2 task/s, elapsed: 7s, ETA:    74s
[>>                              ] 42/500, 6.2 task/s, elapsed: 7s, ETA:    74s
[>>                              ] 43/500, 6.2 task/s, elapsed: 7s, ETA:    74s
[>>                              ] 44/500, 6.2 task/s, elapsed: 7s, ETA:    73s
[>>                              ] 45/500, 6.2 task/s, elapsed: 7s, ETA:    73s
[>>                              ] 46/500, 6.2 task/s, elapsed: 7s, ETA:    73s
[>>>                             ] 47/500, 6.2 task/s, elapsed: 8s, ETA:    73s
[>>>                             ] 48/500, 6.2 task/s, elapsed: 8s, ETA:    72s
[>>>                             ] 49/500, 6.3 task/s, elapsed: 8s, ETA:    72s
[>>>                             ] 50/500, 6.3 task/s, elapsed: 8s, ETA:    72s
[>>>                             ] 51/500, 6.3 task/s, elapsed: 8s, ETA:    72s
[>>>                             ] 52/500, 6.3 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 53/500, 6.3 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 54/500, 6.3 task/s, elapsed: 9s, ETA:    71s
[>>>                             ] 55/500, 6.3 task/s, elapsed: 9s, ETA:    71s
[>>>                             ] 56/500, 6.3 task/s, elapsed: 9s, ETA:    71s
[>>>                             ] 57/500, 6.3 task/s, elapsed: 9s, ETA:    70s
[>>>                             ] 58/500, 6.3 task/s, elapsed: 9s, ETA:    70s
[>>>                             ] 59/500, 6.3 task/s, elapsed: 9s, ETA:    70s
[>>>                            ] 60/500, 6.3 task/s, elapsed: 10s, ETA:    70s
[>>>                            ] 61/500, 6.3 task/s, elapsed: 10s, ETA:    69s
[>>>                            ] 62/500, 6.3 task/s, elapsed: 10s, ETA:    69s
[>>>                            ] 63/500, 6.3 task/s, elapsed: 10s, ETA:    69s
[>>>                            ] 64/500, 6.3 task/s, elapsed: 10s, ETA:    69s
[>>>>                           ] 65/500, 6.4 task/s, elapsed: 10s, ETA:    68s
[>>>>                           ] 66/500, 6.4 task/s, elapsed: 10s, ETA:    68s
[>>>>                           ] 67/500, 6.4 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 68/500, 6.4 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 69/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 70/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 71/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 72/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 73/500, 6.4 task/s, elapsed: 11s, ETA:    66s
[>>>>                           ] 74/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 75/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 76/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 77/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 78/500, 6.5 task/s, elapsed: 12s, ETA:    65s
[>>>>                           ] 79/500, 6.5 task/s, elapsed: 12s, ETA:    65s
[>>>>                           ] 80/500, 6.5 task/s, elapsed: 12s, ETA:    65s
[>>>>>                          ] 81/500, 6.5 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 82/500, 6.5 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 83/500, 6.5 task/s, elapsed: 13s, ETA:    64s
[>>>>>                          ] 84/500, 6.5 task/s, elapsed: 13s, ETA:    64s
[>>>>>                          ] 85/500, 6.5 task/s, elapsed: 13s, ETA:    64s
[>>>>>                          ] 86/500, 6.5 task/s, elapsed: 13s, ETA:    64s
[>>>>>                          ] 87/500, 6.5 task/s, elapsed: 13s, ETA:    64s
[>>>>>                          ] 88/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 89/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 90/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 91/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 92/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 93/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 94/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 95/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>                          ] 96/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>>                         ] 97/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>>                         ] 98/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>>                         ] 99/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>>                        ] 100/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>>                        ] 101/500, 6.5 task/s, elapsed: 16s, ETA:    62s
[>>>>>>                        ] 102/500, 6.5 task/s, elapsed: 16s, ETA:    61s
[>>>>>>                        ] 103/500, 6.5 task/s, elapsed: 16s, ETA:    61s
[>>>>>>                        ] 104/500, 6.5 task/s, elapsed: 16s, ETA:    61s
[>>>>>>                        ] 105/500, 6.5 task/s, elapsed: 16s, ETA:    61s
[>>>>>>                        ] 106/500, 6.5 task/s, elapsed: 16s, ETA:    61s
[>>>>>>                        ] 107/500, 6.5 task/s, elapsed: 17s, ETA:    61s
[>>>>>>                        ] 108/500, 6.5 task/s, elapsed: 17s, ETA:    61s
[>>>>>>                        ] 109/500, 6.5 task/s, elapsed: 17s, ETA:    60s
[>>>>>>                        ] 110/500, 6.5 task/s, elapsed: 17s, ETA:    60s
[>>>>>>                        ] 111/500, 6.5 task/s, elapsed: 17s, ETA:    60s
[>>>>>>                        ] 112/500, 6.5 task/s, elapsed: 17s, ETA:    60s
[>>>>>>                        ] 113/500, 6.5 task/s, elapsed: 17s, ETA:    60s
[>>>>>>                        ] 114/500, 6.5 task/s, elapsed: 18s, ETA:    60s
[>>>>>>                        ] 115/500, 6.5 task/s, elapsed: 18s, ETA:    60s
[>>>>>>                        ] 116/500, 6.5 task/s, elapsed: 18s, ETA:    59s
[>>>>>>>                       ] 117/500, 6.5 task/s, elapsed: 18s, ETA:    59s
[>>>>>>>                       ] 118/500, 6.5 task/s, elapsed: 18s, ETA:    59s
[>>>>>>>                       ] 119/500, 6.5 task/s, elapsed: 18s, ETA:    59s
[>>>>>>>                       ] 120/500, 6.5 task/s, elapsed: 19s, ETA:    59s
[>>>>>>>                       ] 121/500, 6.5 task/s, elapsed: 19s, ETA:    59s
[>>>>>>>                       ] 122/500, 6.5 task/s, elapsed: 19s, ETA:    58s
[>>>>>>>                       ] 123/500, 6.5 task/s, elapsed: 19s, ETA:    58s
[>>>>>>>                       ] 124/500, 6.5 task/s, elapsed: 19s, ETA:    58s
[>>>>>>>                       ] 125/500, 6.5 task/s, elapsed: 19s, ETA:    58s
[>>>>>>>                       ] 126/500, 6.5 task/s, elapsed: 19s, ETA:    58s
[>>>>>>>                       ] 127/500, 6.5 task/s, elapsed: 20s, ETA:    58s
[>>>>>>>                       ] 128/500, 6.5 task/s, elapsed: 20s, ETA:    58s
[>>>>>>>                       ] 129/500, 6.5 task/s, elapsed: 20s, ETA:    57s
[>>>>>>>                       ] 130/500, 6.5 task/s, elapsed: 20s, ETA:    57s
[>>>>>>>                       ] 131/500, 6.5 task/s, elapsed: 20s, ETA:    57s
[>>>>>>>                       ] 132/500, 6.5 task/s, elapsed: 20s, ETA:    57s
[>>>>>>>                       ] 133/500, 6.5 task/s, elapsed: 21s, ETA:    57s
[>>>>>>>>                      ] 134/500, 6.5 task/s, elapsed: 21s, ETA:    57s
[>>>>>>>>                      ] 135/500, 6.4 task/s, elapsed: 21s, ETA:    57s
[>>>>>>>>                      ] 136/500, 6.5 task/s, elapsed: 21s, ETA:    56s
[>>>>>>>>                      ] 137/500, 6.5 task/s, elapsed: 21s, ETA:    56s
[>>>>>>>>                      ] 138/500, 6.5 task/s, elapsed: 21s, ETA:    56s
[>>>>>>>>                      ] 139/500, 6.5 task/s, elapsed: 22s, ETA:    56s
[>>>>>>>>                      ] 140/500, 6.5 task/s, elapsed: 22s, ETA:    56s
[>>>>>>>>                      ] 141/500, 6.5 task/s, elapsed: 22s, ETA:    56s
[>>>>>>>>                      ] 142/500, 6.4 task/s, elapsed: 22s, ETA:    56s
[>>>>>>>>                      ] 143/500, 6.4 task/s, elapsed: 22s, ETA:    55s
[>>>>>>>>                      ] 144/500, 6.4 task/s, elapsed: 22s, ETA:    55s
[>>>>>>>>                      ] 145/500, 6.4 task/s, elapsed: 23s, ETA:    55s
[>>>>>>>>                      ] 146/500, 6.4 task/s, elapsed: 23s, ETA:    55s
[>>>>>>>>                      ] 147/500, 6.4 task/s, elapsed: 23s, ETA:    55s
[>>>>>>>>                      ] 148/500, 6.4 task/s, elapsed: 23s, ETA:    55s
[>>>>>>>>                      ] 149/500, 6.4 task/s, elapsed: 23s, ETA:    55s
[>>>>>>>>>                     ] 150/500, 6.4 task/s, elapsed: 23s, ETA:    54s
[>>>>>>>>>                     ] 151/500, 6.4 task/s, elapsed: 23s, ETA:    54s
[>>>>>>>>>                     ] 152/500, 6.4 task/s, elapsed: 24s, ETA:    54s
[>>>>>>>>>                     ] 153/500, 6.4 task/s, elapsed: 24s, ETA:    54s
[>>>>>>>>>                     ] 154/500, 6.4 task/s, elapsed: 24s, ETA:    54s
[>>>>>>>>>                     ] 155/500, 6.4 task/s, elapsed: 24s, ETA:    54s
[>>>>>>>>>                     ] 156/500, 6.4 task/s, elapsed: 24s, ETA:    53s
[>>>>>>>>>                     ] 157/500, 6.4 task/s, elapsed: 24s, ETA:    53s
[>>>>>>>>>                     ] 158/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 159/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 160/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 161/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 162/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 163/500, 6.4 task/s, elapsed: 25s, ETA:    52s
[>>>>>>>>>                     ] 164/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>                     ] 165/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>                     ] 166/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>>                    ] 167/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>>                    ] 168/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>>                    ] 169/500, 6.4 task/s, elapsed: 26s, ETA:    51s
[>>>>>>>>>>                    ] 170/500, 6.4 task/s, elapsed: 26s, ETA:    51s
[>>>>>>>>>>                    ] 171/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 172/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 173/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 174/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 175/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 176/500, 6.4 task/s, elapsed: 27s, ETA:    50s
[>>>>>>>>>>                    ] 177/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 178/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 179/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 180/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 181/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 182/500, 6.4 task/s, elapsed: 28s, ETA:    49s
[>>>>>>>>>>                    ] 183/500, 6.4 task/s, elapsed: 28s, ETA:    49s
[>>>>>>>>>>>                   ] 184/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 185/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 186/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 187/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 188/500, 6.4 task/s, elapsed: 29s, ETA:    48s
[>>>>>>>>>>>                   ] 189/500, 6.4 task/s, elapsed: 29s, ETA:    48s
[>>>>>>>>>>>                   ] 190/500, 6.5 task/s, elapsed: 29s, ETA:    48s
[>>>>>>>>>>>                   ] 191/500, 6.5 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 192/500, 6.5 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 193/500, 6.5 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 194/500, 6.5 task/s, elapsed: 30s, ETA:    47s
[>>>>>>>>>>>                   ] 195/500, 6.5 task/s, elapsed: 30s, ETA:    47s
[>>>>>>>>>>>                   ] 196/500, 6.5 task/s, elapsed: 30s, ETA:    47s
[>>>>>>>>>>>                   ] 197/500, 6.5 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>                   ] 198/500, 6.5 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>                   ] 199/500, 6.5 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>>                  ] 200/500, 6.5 task/s, elapsed: 31s, ETA:    46s
[>>>>>>>>>>>>                  ] 201/500, 6.5 task/s, elapsed: 31s, ETA:    46s
[>>>>>>>>>>>>                  ] 202/500, 6.5 task/s, elapsed: 31s, ETA:    46s
[>>>>>>>>>>>>                  ] 203/500, 6.5 task/s, elapsed: 31s, ETA:    46s
[>>>>>>>>>>>>                  ] 204/500, 6.5 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 205/500, 6.5 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 206/500, 6.5 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 207/500, 6.5 task/s, elapsed: 32s, ETA:    45s
[>>>>>>>>>>>>                  ] 208/500, 6.5 task/s, elapsed: 32s, ETA:    45s
[>>>>>>>>>>>>                  ] 209/500, 6.5 task/s, elapsed: 32s, ETA:    45s
[>>>>>>>>>>>>                  ] 210/500, 6.5 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 211/500, 6.5 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 212/500, 6.5 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 213/500, 6.5 task/s, elapsed: 33s, ETA:    44s
[>>>>>>>>>>>>                  ] 214/500, 6.4 task/s, elapsed: 33s, ETA:    44s
[>>>>>>>>>>>>                  ] 215/500, 6.4 task/s, elapsed: 33s, ETA:    44s
[>>>>>>>>>>>>                  ] 216/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 217/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 218/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 219/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 220/500, 6.4 task/s, elapsed: 34s, ETA:    43s
[>>>>>>>>>>>>>                 ] 221/500, 6.4 task/s, elapsed: 34s, ETA:    43s
[>>>>>>>>>>>>>                 ] 222/500, 6.4 task/s, elapsed: 34s, ETA:    43s
[>>>>>>>>>>>>>                 ] 223/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 224/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 225/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 226/500, 6.5 task/s, elapsed: 35s, ETA:    42s
[>>>>>>>>>>>>>                 ] 227/500, 6.5 task/s, elapsed: 35s, ETA:    42s
[>>>>>>>>>>>>>                 ] 228/500, 6.5 task/s, elapsed: 35s, ETA:    42s
[>>>>>>>>>>>>>                 ] 229/500, 6.5 task/s, elapsed: 35s, ETA:    42s
[>>>>>>>>>>>>>                 ] 230/500, 6.5 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>                 ] 231/500, 6.5 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>                 ] 232/500, 6.5 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>                 ] 233/500, 6.5 task/s, elapsed: 36s, ETA:    41s
[>>>>>>>>>>>>>>                ] 234/500, 6.5 task/s, elapsed: 36s, ETA:    41s
[>>>>>>>>>>>>>>                ] 235/500, 6.5 task/s, elapsed: 36s, ETA:    41s
[>>>>>>>>>>>>>>                ] 236/500, 6.5 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 237/500, 6.5 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 238/500, 6.5 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 239/500, 6.5 task/s, elapsed: 37s, ETA:    40s
[>>>>>>>>>>>>>>                ] 240/500, 6.4 task/s, elapsed: 37s, ETA:    40s
[>>>>>>>>>>>>>>                ] 241/500, 6.4 task/s, elapsed: 37s, ETA:    40s
[>>>>>>>>>>>>>>                ] 242/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 243/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 244/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 245/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 246/500, 6.4 task/s, elapsed: 38s, ETA:    39s
[>>>>>>>>>>>>>>                ] 247/500, 6.4 task/s, elapsed: 38s, ETA:    39s
[>>>>>>>>>>>>>>                ] 248/500, 6.4 task/s, elapsed: 38s, ETA:    39s
[>>>>>>>>>>>>>>                ] 249/500, 6.4 task/s, elapsed: 39s, ETA:    39s
[>>>>>>>>>>>>>>>               ] 250/500, 6.4 task/s, elapsed: 39s, ETA:    39s
[>>>>>>>>>>>>>>>               ] 251/500, 6.5 task/s, elapsed: 39s, ETA:    39s
[>>>>>>>>>>>>>>>               ] 252/500, 6.5 task/s, elapsed: 39s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 253/500, 6.5 task/s, elapsed: 39s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 254/500, 6.5 task/s, elapsed: 39s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 255/500, 6.5 task/s, elapsed: 39s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 256/500, 6.5 task/s, elapsed: 40s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 257/500, 6.5 task/s, elapsed: 40s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 258/500, 6.5 task/s, elapsed: 40s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 259/500, 6.5 task/s, elapsed: 40s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 260/500, 6.5 task/s, elapsed: 40s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 261/500, 6.5 task/s, elapsed: 40s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 262/500, 6.5 task/s, elapsed: 41s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 263/500, 6.5 task/s, elapsed: 41s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 264/500, 6.5 task/s, elapsed: 41s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 265/500, 6.5 task/s, elapsed: 41s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 266/500, 6.5 task/s, elapsed: 41s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 267/500, 6.5 task/s, elapsed: 41s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 268/500, 6.5 task/s, elapsed: 41s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 269/500, 6.5 task/s, elapsed: 42s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 270/500, 6.5 task/s, elapsed: 42s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 271/500, 6.5 task/s, elapsed: 42s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 272/500, 6.5 task/s, elapsed: 42s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 273/500, 6.5 task/s, elapsed: 42s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 274/500, 6.5 task/s, elapsed: 42s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 275/500, 6.5 task/s, elapsed: 42s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 276/500, 6.5 task/s, elapsed: 43s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 277/500, 6.5 task/s, elapsed: 43s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 278/500, 6.5 task/s, elapsed: 43s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 279/500, 6.5 task/s, elapsed: 43s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 280/500, 6.5 task/s, elapsed: 43s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 281/500, 6.5 task/s, elapsed: 43s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 282/500, 6.5 task/s, elapsed: 43s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 283/500, 6.5 task/s, elapsed: 44s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 284/500, 6.5 task/s, elapsed: 44s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 285/500, 6.5 task/s, elapsed: 44s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 286/500, 6.5 task/s, elapsed: 44s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 287/500, 6.5 task/s, elapsed: 44s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 288/500, 6.5 task/s, elapsed: 44s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 289/500, 6.5 task/s, elapsed: 44s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 290/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 291/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 292/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 293/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 294/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 295/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 296/500, 6.5 task/s, elapsed: 45s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 297/500, 6.5 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 298/500, 6.5 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 299/500, 6.5 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>>            ] 300/500, 6.5 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>>            ] 301/500, 6.5 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>>            ] 302/500, 6.5 task/s, elapsed: 46s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 303/500, 6.5 task/s, elapsed: 46s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 304/500, 6.5 task/s, elapsed: 47s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 305/500, 6.5 task/s, elapsed: 47s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 306/500, 6.5 task/s, elapsed: 47s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 307/500, 6.5 task/s, elapsed: 47s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 308/500, 6.5 task/s, elapsed: 47s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 309/500, 6.5 task/s, elapsed: 47s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 310/500, 6.5 task/s, elapsed: 47s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 311/500, 6.5 task/s, elapsed: 48s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 312/500, 6.5 task/s, elapsed: 48s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 313/500, 6.5 task/s, elapsed: 48s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 314/500, 6.5 task/s, elapsed: 48s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 315/500, 6.5 task/s, elapsed: 48s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 316/500, 6.5 task/s, elapsed: 48s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 317/500, 6.5 task/s, elapsed: 48s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 318/500, 6.5 task/s, elapsed: 49s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 319/500, 6.5 task/s, elapsed: 49s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 320/500, 6.5 task/s, elapsed: 49s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 321/500, 6.5 task/s, elapsed: 49s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 322/500, 6.5 task/s, elapsed: 49s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 323/500, 6.6 task/s, elapsed: 49s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 324/500, 6.6 task/s, elapsed: 49s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 325/500, 6.6 task/s, elapsed: 50s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 326/500, 6.6 task/s, elapsed: 50s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 327/500, 6.6 task/s, elapsed: 50s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 328/500, 6.6 task/s, elapsed: 50s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 329/500, 6.6 task/s, elapsed: 50s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 330/500, 6.6 task/s, elapsed: 50s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 331/500, 6.6 task/s, elapsed: 50s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 332/500, 6.6 task/s, elapsed: 51s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 333/500, 6.6 task/s, elapsed: 51s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 334/500, 6.6 task/s, elapsed: 51s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 335/500, 6.6 task/s, elapsed: 51s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 336/500, 6.6 task/s, elapsed: 51s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 337/500, 6.6 task/s, elapsed: 51s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 338/500, 6.6 task/s, elapsed: 51s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 339/500, 6.6 task/s, elapsed: 52s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 340/500, 6.6 task/s, elapsed: 52s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 341/500, 6.6 task/s, elapsed: 52s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 342/500, 6.6 task/s, elapsed: 52s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 343/500, 6.6 task/s, elapsed: 52s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 344/500, 6.6 task/s, elapsed: 52s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 345/500, 6.6 task/s, elapsed: 52s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 346/500, 6.6 task/s, elapsed: 53s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 347/500, 6.6 task/s, elapsed: 53s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 348/500, 6.6 task/s, elapsed: 53s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 349/500, 6.6 task/s, elapsed: 53s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>         ] 350/500, 6.6 task/s, elapsed: 53s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>         ] 351/500, 6.6 task/s, elapsed: 53s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>         ] 352/500, 6.6 task/s, elapsed: 53s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 353/500, 6.6 task/s, elapsed: 54s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 354/500, 6.6 task/s, elapsed: 54s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 355/500, 6.6 task/s, elapsed: 54s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 356/500, 6.6 task/s, elapsed: 54s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 357/500, 6.6 task/s, elapsed: 54s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 358/500, 6.6 task/s, elapsed: 54s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 359/500, 6.6 task/s, elapsed: 54s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 360/500, 6.6 task/s, elapsed: 55s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 361/500, 6.6 task/s, elapsed: 55s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 362/500, 6.6 task/s, elapsed: 55s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 363/500, 6.6 task/s, elapsed: 55s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 364/500, 6.6 task/s, elapsed: 55s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 365/500, 6.6 task/s, elapsed: 55s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 366/500, 6.6 task/s, elapsed: 55s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 367/500, 6.6 task/s, elapsed: 56s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 368/500, 6.6 task/s, elapsed: 56s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 369/500, 6.6 task/s, elapsed: 56s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 370/500, 6.6 task/s, elapsed: 56s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 371/500, 6.6 task/s, elapsed: 56s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 372/500, 6.6 task/s, elapsed: 56s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 373/500, 6.6 task/s, elapsed: 57s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 374/500, 6.6 task/s, elapsed: 57s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 375/500, 6.6 task/s, elapsed: 57s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 376/500, 6.6 task/s, elapsed: 57s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 377/500, 6.6 task/s, elapsed: 57s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 378/500, 6.6 task/s, elapsed: 57s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 379/500, 6.6 task/s, elapsed: 57s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 380/500, 6.6 task/s, elapsed: 57s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 381/500, 6.6 task/s, elapsed: 58s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 382/500, 6.6 task/s, elapsed: 58s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 383/500, 6.6 task/s, elapsed: 58s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 384/500, 6.6 task/s, elapsed: 58s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 385/500, 6.6 task/s, elapsed: 58s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 386/500, 6.6 task/s, elapsed: 58s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 387/500, 6.6 task/s, elapsed: 58s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 388/500, 6.6 task/s, elapsed: 59s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 389/500, 6.6 task/s, elapsed: 59s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 390/500, 6.6 task/s, elapsed: 59s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 391/500, 6.6 task/s, elapsed: 59s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 392/500, 6.6 task/s, elapsed: 59s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 393/500, 6.6 task/s, elapsed: 59s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 394/500, 6.6 task/s, elapsed: 60s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 395/500, 6.6 task/s, elapsed: 60s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 396/500, 6.6 task/s, elapsed: 60s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 397/500, 6.6 task/s, elapsed: 60s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 398/500, 6.6 task/s, elapsed: 60s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 399/500, 6.6 task/s, elapsed: 60s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 400/500, 6.6 task/s, elapsed: 60s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 401/500, 6.6 task/s, elapsed: 61s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 402/500, 6.6 task/s, elapsed: 61s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 403/500, 6.6 task/s, elapsed: 61s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 404/500, 6.6 task/s, elapsed: 61s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 405/500, 6.6 task/s, elapsed: 61s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 406/500, 6.6 task/s, elapsed: 61s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 407/500, 6.6 task/s, elapsed: 61s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 408/500, 6.6 task/s, elapsed: 62s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 409/500, 6.6 task/s, elapsed: 62s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 410/500, 6.6 task/s, elapsed: 62s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 411/500, 6.6 task/s, elapsed: 62s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 412/500, 6.6 task/s, elapsed: 62s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 413/500, 6.6 task/s, elapsed: 62s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 414/500, 6.6 task/s, elapsed: 62s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 415/500, 6.6 task/s, elapsed: 63s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 416/500, 6.6 task/s, elapsed: 63s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 417/500, 6.6 task/s, elapsed: 63s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 418/500, 6.6 task/s, elapsed: 63s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 419/500, 6.6 task/s, elapsed: 63s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 420/500, 6.6 task/s, elapsed: 63s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 421/500, 6.6 task/s, elapsed: 63s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 422/500, 6.6 task/s, elapsed: 64s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 423/500, 6.6 task/s, elapsed: 64s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 424/500, 6.6 task/s, elapsed: 64s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 425/500, 6.6 task/s, elapsed: 64s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 426/500, 6.6 task/s, elapsed: 64s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 427/500, 6.6 task/s, elapsed: 64s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 428/500, 6.6 task/s, elapsed: 64s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 429/500, 6.6 task/s, elapsed: 65s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 430/500, 6.6 task/s, elapsed: 65s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 431/500, 6.6 task/s, elapsed: 65s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 432/500, 6.6 task/s, elapsed: 65s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 433/500, 6.6 task/s, elapsed: 65s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 434/500, 6.6 task/s, elapsed: 65s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 435/500, 6.6 task/s, elapsed: 65s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 436/500, 6.6 task/s, elapsed: 66s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 437/500, 6.6 task/s, elapsed: 66s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 438/500, 6.7 task/s, elapsed: 66s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 439/500, 6.7 task/s, elapsed: 66s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 440/500, 6.7 task/s, elapsed: 66s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 441/500, 6.7 task/s, elapsed: 66s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 442/500, 6.7 task/s, elapsed: 66s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 443/500, 6.7 task/s, elapsed: 67s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 444/500, 6.6 task/s, elapsed: 67s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 445/500, 6.6 task/s, elapsed: 67s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 446/500, 6.6 task/s, elapsed: 67s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 447/500, 6.6 task/s, elapsed: 67s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 448/500, 6.6 task/s, elapsed: 67s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 449/500, 6.7 task/s, elapsed: 68s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 450/500, 6.7 task/s, elapsed: 68s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 451/500, 6.7 task/s, elapsed: 68s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 452/500, 6.7 task/s, elapsed: 68s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 453/500, 6.6 task/s, elapsed: 68s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 454/500, 6.6 task/s, elapsed: 68s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 455/500, 6.6 task/s, elapsed: 68s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 456/500, 6.6 task/s, elapsed: 69s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 457/500, 6.6 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 458/500, 6.6 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 459/500, 6.6 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 460/500, 6.6 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 461/500, 6.6 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 462/500, 6.6 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 463/500, 6.6 task/s, elapsed: 70s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 464/500, 6.6 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 465/500, 6.6 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 466/500, 6.7 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 467/500, 6.6 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 468/500, 6.7 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 469/500, 6.7 task/s, elapsed: 71s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 470/500, 6.7 task/s, elapsed: 71s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 471/500, 6.7 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 472/500, 6.7 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 473/500, 6.7 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 474/500, 6.7 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 475/500, 6.7 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 476/500, 6.7 task/s, elapsed: 72s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 477/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 478/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 479/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 480/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 481/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 482/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 483/500, 6.7 task/s, elapsed: 73s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 484/500, 6.7 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 485/500, 6.7 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 486/500, 6.7 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 487/500, 6.7 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 488/500, 6.7 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 489/500, 6.7 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 490/500, 6.7 task/s, elapsed: 74s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 491/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 492/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 493/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 494/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 495/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 496/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 497/500, 6.7 task/s, elapsed: 75s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 498/500, 6.7 task/s, elapsed: 75s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 499/500, 6.7 task/s, elapsed: 75s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 500/500, 6.7 task/s, elapsed: 75s, ETA:     0s2022-04-18 20:09:28,761 - mmseg - INFO - per class results:
2022-04-18 20:09:28,763 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|      road     | 86.06 | 89.06 |
|    sidewalk   | 41.75 | 75.65 |
|    building   | 86.81 | 94.84 |
|      wall     | 27.74 | 31.24 |
|     fence     |  5.83 |  6.47 |
|      pole     | 48.08 | 58.73 |
| traffic light | 51.95 | 67.51 |
|  traffic sign | 51.41 | 66.93 |
|   vegetation  | 84.57 | 92.63 |
|    terrain    |  0.0  |  0.0  |
|      sky      | 83.63 | 98.98 |
|     person    | 72.86 | 84.89 |
|     rider     | 43.45 | 56.69 |
|      car      | 86.38 | 96.82 |
|     truck     |  0.0  |  0.0  |
|      bus      | 52.18 | 69.53 |
|     train     |  0.0  |  0.0  |
|   motorcycle  | 47.97 | 54.79 |
|    bicycle    | 62.59 | 74.99 |
+---------------+-------+-------+
2022-04-18 20:09:28,763 - mmseg - INFO - Summary:
2022-04-18 20:09:28,763 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 87.88 | 49.12 | 58.93 |
+-------+-------+-------+
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:09:28,778 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-18 20:09:28,778 - mmseg - INFO - Iter [500/40000]	lr: 3.600e-05, eta: 10:14:55, time: 1.509, data_time: 0.013, memory: 9636, aAcc: 0.8788, mIoU: 0.4912, mAcc: 0.5893, IoU.road: 0.8606, IoU.sidewalk: 0.4175, IoU.building: 0.8681, IoU.wall: 0.2774, IoU.fence: 0.0583, IoU.pole: 0.4808, IoU.traffic light: 0.5195, IoU.traffic sign: 0.5141, IoU.vegetation: 0.8457, IoU.terrain: 0.0000, IoU.sky: 0.8363, IoU.person: 0.7286, IoU.rider: 0.4345, IoU.car: 0.8638, IoU.truck: 0.0000, IoU.bus: 0.5218, IoU.train: 0.0000, IoU.motorcycle: 0.4797, IoU.bicycle: 0.6259, Acc.road: 0.8906, Acc.sidewalk: 0.7565, Acc.building: 0.9484, Acc.wall: 0.3124, Acc.fence: 0.0647, Acc.pole: 0.5873, Acc.traffic light: 0.6751, Acc.traffic sign: 0.6693, Acc.vegetation: 0.9263, Acc.terrain: 0.0000, Acc.sky: 0.9898, Acc.person: 0.8489, Acc.rider: 0.5669, Acc.car: 0.9682, Acc.truck: 0.0000, Acc.bus: 0.6953, Acc.train: 0.0000, Acc.motorcycle: 0.5479, Acc.bicycle: 0.7499, decode.loss_seg: 0.1319, decode.acc_seg: 89.9399, src.loss_imnet_feat_dist: 0.1088, mix.decode.loss_seg: 0.1452, mix.decode.acc_seg: 88.8261
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:10:47,528 - mmseg - INFO - Iter [16050/40000]	lr: 3.593e-05, eta: 10:16:39, time: 3.962, data_time: 2.403, memory: 9636, decode.loss_seg: 0.1252, decode.acc_seg: 90.6526, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1397, mix.decode.acc_seg: 88.9776
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:12:03,340 - mmseg - INFO - Iter [16100/40000]	lr: 3.585e-05, eta: 10:15:20, time: 1.516, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1267, decode.acc_seg: 90.5084, src.loss_imnet_feat_dist: 0.1118, mix.decode.loss_seg: 0.1319, mix.decode.acc_seg: 89.4234
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:13:17,263 - mmseg - INFO - Iter [16150/40000]	lr: 3.578e-05, eta: 10:13:58, time: 1.478, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1246, decode.acc_seg: 90.0852, src.loss_imnet_feat_dist: 0.1017, mix.decode.loss_seg: 0.1470, mix.decode.acc_seg: 88.8182
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:14:34,199 - mmseg - INFO - Iter [16200/40000]	lr: 3.570e-05, eta: 10:12:40, time: 1.539, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1226, decode.acc_seg: 90.4569, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1329, mix.decode.acc_seg: 89.2085
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:15:54,295 - mmseg - INFO - Iter [16250/40000]	lr: 3.563e-05, eta: 10:11:27, time: 1.602, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1145, decode.acc_seg: 91.3404, src.loss_imnet_feat_dist: 0.1084, mix.decode.loss_seg: 0.1261, mix.decode.acc_seg: 89.6140
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:17:10,970 - mmseg - INFO - Iter [16300/40000]	lr: 3.555e-05, eta: 10:10:09, time: 1.533, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1299, decode.acc_seg: 90.4706, src.loss_imnet_feat_dist: 0.1117, mix.decode.loss_seg: 0.1504, mix.decode.acc_seg: 89.0684
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:18:28,920 - mmseg - INFO - Iter [16350/40000]	lr: 3.548e-05, eta: 10:08:53, time: 1.559, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1316, decode.acc_seg: 90.6144, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1526, mix.decode.acc_seg: 88.1150
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:19:46,597 - mmseg - INFO - Iter [16400/40000]	lr: 3.540e-05, eta: 10:07:36, time: 1.554, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1234, decode.acc_seg: 90.7657, src.loss_imnet_feat_dist: 0.1065, mix.decode.loss_seg: 0.1379, mix.decode.acc_seg: 88.9967
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:21:06,535 - mmseg - INFO - Iter [16450/40000]	lr: 3.533e-05, eta: 10:06:23, time: 1.599, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1249, decode.acc_seg: 91.0681, src.loss_imnet_feat_dist: 0.1124, mix.decode.loss_seg: 0.1346, mix.decode.acc_seg: 89.8785
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:22:21,783 - mmseg - INFO - Iter [16500/40000]	lr: 3.525e-05, eta: 10:05:03, time: 1.505, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1240, decode.acc_seg: 90.7231, src.loss_imnet_feat_dist: 0.1088, mix.decode.loss_seg: 0.1383, mix.decode.acc_seg: 88.9734
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:23:16,227 - mmseg - INFO - Iter [7550/40000]	lr: 4.868e-05, eta: 1 day, 6:33:13, time: 3.381, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.8038, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.9441
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:26:02,426 - mmseg - INFO - Iter [7600/40000]	lr: 4.860e-05, eta: 1 day, 6:30:10, time: 3.324, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.0510, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.2933
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:28:51,079 - mmseg - INFO - Iter [7650/40000]	lr: 4.853e-05, eta: 1 day, 6:27:17, time: 3.373, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.1010, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.7310
05
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:29:52,391 - mmseg - INFO - Iter [16800/40000]	lr: 3.480e-05, eta: 9:57:01, time: 1.447, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1208, decode.acc_seg: 91.2193, src.loss_imnet_feat_dist: 0.1091, mix.decode.loss_seg: 0.1588, mix.decode.acc_seg: 89.2854
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:31:10,141 - mmseg - INFO - Iter [16850/40000]	lr: 3.473e-05, eta: 9:55:45, time: 1.555, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1226, decode.acc_seg: 90.6313, src.loss_imnet_feat_dist: 0.1075, mix.decode.loss_seg: 0.1393, mix.decode.acc_seg: 89.5733
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:32:23,515 - mmseg - INFO - Iter [16900/40000]	lr: 3.465e-05, eta: 9:54:23, time: 1.467, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1225, decode.acc_seg: 90.0189, src.loss_imnet_feat_dist: 0.1023, mix.decode.loss_seg: 0.1342, mix.decode.acc_seg: 88.7484
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:33:41,802 - mmseg - INFO - Iter [16950/40000]	lr: 3.458e-05, eta: 9:53:07, time: 1.566, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1156, decode.acc_seg: 90.6836, src.loss_imnet_feat_dist: 0.1085, mix.decode.loss_seg: 0.1358, mix.decode.acc_seg: 88.3694
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:34:57,120 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-18 20:34:57,120 - mmseg - INFO - Iter [17000/40000]	lr: 3.450e-05, eta: 9:51:47, time: 1.506, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1192, decode.acc_seg: 90.3810, src.loss_imnet_feat_dist: 0.1058, mix.decode.loss_seg: 0.1547, mix.decode.acc_seg: 87.7292
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:36:16,213 - mmseg - INFO - Iter [17050/40000]	lr: 3.443e-05, eta: 9:50:33, time: 1.582, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1235, decode.acc_seg: 89.9164, src.loss_imnet_feat_dist: 0.1094, mix.decode.loss_seg: 0.1481, mix.decode.acc_seg: 88.2272
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:37:13,063 - mmseg - INFO - Iter [7800/40000]	lr: 4.830e-05, eta: 1 day, 6:18:22, time: 3.306, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.4541, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.5695
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:40:02,691 - mmseg - INFO - Iter [7850/40000]	lr: 4.823e-05, eta: 1 day, 6:15:34, time: 3.393, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.6562, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.9112
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:42:50,239 - mmseg - INFO - Iter [7900/40000]	lr: 4.815e-05, eta: 1 day, 6:12:37, time: 3.351, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.6808, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.9484
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:45:37,948 - mmseg - INFO - Iter [7950/40000]	lr: 4.808e-05, eta: 1 day, 6:09:40, time: 3.354, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.7292, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.4205
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [                                                  ] 0/500, elapsed: 0s, ETA:
[                                 ] 1/500, 1.4 task/s, elapsed: 1s, ETA:   347s
[                                 ] 2/500, 1.8 task/s, elapsed: 1s, ETA:   269s
[                                 ] 3/500, 2.0 task/s, elapsed: 1s, ETA:   243s
[                                 ] 4/500, 2.2 task/s, elapsed: 2s, ETA:   230s
[                                 ] 5/500, 2.2 task/s, elapsed: 2s, ETA:   222s
[                                 ] 6/500, 2.3 task/s, elapsed: 3s, ETA:   216s
[                                 ] 7/500, 2.3 task/s, elapsed: 3s, ETA:   212s
[                                 ] 8/500, 2.4 task/s, elapsed: 3s, ETA:   209s
[                                 ] 9/500, 2.4 task/s, elapsed: 4s, ETA:   207s
[                                ] 10/500, 2.4 task/s, elapsed: 4s, ETA:   204s
[                                ] 11/500, 2.4 task/s, elapsed: 5s, ETA:   203s
[                                ] 12/500, 2.4 task/s, elapsed: 5s, ETA:   201s
[                                ] 13/500, 2.4 task/s, elapsed: 5s, ETA:   200s
[                                ] 14/500, 2.4 task/s, elapsed: 6s, ETA:   199s
[                                ] 15/500, 2.5 task/s, elapsed: 6s, ETA:   197s
[>                               ] 16/500, 2.5 task/s, elapsed: 6s, ETA:   196s
[>                               ] 17/500, 2.5 task/s, elapsed: 7s, ETA:   195s
[>                               ] 18/500, 2.5 task/s, elapsed: 7s, ETA:   195s
[>                               ] 19/500, 2.5 task/s, elapsed: 8s, ETA:   194s
[>                               ] 20/500, 2.5 task/s, elapsed: 8s, ETA:   193s
[>                               ] 21/500, 2.5 task/s, elapsed: 8s, ETA:   192s
[>                               ] 22/500, 2.5 task/s, elapsed: 9s, ETA:   191s
[>                               ] 23/500, 2.5 task/s, elapsed: 9s, ETA:   191s
[>                              ] 24/500, 2.5 task/s, elapsed: 10s, ETA:   190s
[>                              ] 25/500, 2.5 task/s, elapsed: 10s, ETA:   189s
[>                              ] 26/500, 2.5 task/s, elapsed: 10s, ETA:   189s
[>                              ] 27/500, 2.5 task/s, elapsed: 11s, ETA:   188s
[>                              ] 28/500, 2.5 task/s, elapsed: 11s, ETA:   188s
[>                              ] 29/500, 2.5 task/s, elapsed: 12s, ETA:   187s
[>                              ] 30/500, 2.5 task/s, elapsed: 12s, ETA:   187s
[>                              ] 31/500, 2.5 task/s, elapsed: 12s, ETA:   186s
[>                              ] 32/500, 2.5 task/s, elapsed: 13s, ETA:   185s
[>>                             ] 33/500, 2.5 task/s, elapsed: 13s, ETA:   185s
[>>                             ] 34/500, 2.5 task/s, elapsed: 13s, ETA:   184s
[>>                             ] 35/500, 2.5 task/s, elapsed: 14s, ETA:   184s
[>>                             ] 36/500, 2.5 task/s, elapsed: 14s, ETA:   183s
[>>                             ] 37/500, 2.5 task/s, elapsed: 15s, ETA:   183s
[>>                             ] 38/500, 2.5 task/s, elapsed: 15s, ETA:   182s
[>>                             ] 39/500, 2.5 task/s, elapsed: 15s, ETA:   182s
[>>                             ] 40/500, 2.5 task/s, elapsed: 16s, ETA:   181s
[>>                             ] 41/500, 2.5 task/s, elapsed: 16s, ETA:   181s
[>>                             ] 42/500, 2.5 task/s, elapsed: 17s, ETA:   180s
[>>                             ] 43/500, 2.5 task/s, elapsed: 17s, ETA:   180s
[>>                             ] 44/500, 2.5 task/s, elapsed: 17s, ETA:   179s
[>>                             ] 45/500, 2.5 task/s, elapsed: 18s, ETA:   179s
[>>                             ] 46/500, 2.5 task/s, elapsed: 18s, ETA:   179s
[>>                             ] 47/500, 2.5 task/s, elapsed: 18s, ETA:   178s
[>>                             ] 48/500, 2.5 task/s, elapsed: 19s, ETA:   178s
[>>>                            ] 49/500, 2.5 task/s, elapsed: 19s, ETA:   177s
[>>>                            ] 50/500, 2.5 task/s, elapsed: 20s, ETA:   177s
[>>>                            ] 51/500, 2.5 task/s, elapsed: 20s, ETA:   176s
[>>>                            ] 52/500, 2.5 task/s, elapsed: 20s, ETA:   176s
[>>>                            ] 53/500, 2.5 task/s, elapsed: 21s, ETA:   175s
[>>>                            ] 54/500, 2.5 task/s, elapsed: 21s, ETA:   175s
[>>>                            ] 55/500, 2.5 task/s, elapsed: 22s, ETA:   175s
[>>>                            ] 56/500, 2.6 task/s, elapsed: 22s, ETA:   174s
[>>>                            ] 57/500, 2.6 task/s, elapsed: 22s, ETA:   174s
[>>>                            ] 58/500, 2.6 task/s, elapsed: 23s, ETA:   173s
[>>>                            ] 59/500, 2.6 task/s, elapsed: 23s, ETA:   173s
[>>>                            ] 60/500, 2.6 task/s, elapsed: 24s, ETA:   172s
[>>>                            ] 61/500, 2.6 task/s, elapsed: 24s, ETA:   172s
[>>>                            ] 62/500, 2.6 task/s, elapsed: 24s, ETA:   172s
[>>>                            ] 63/500, 2.6 task/s, elapsed: 25s, ETA:   171s
[>>>                            ] 64/500, 2.6 task/s, elapsed: 25s, ETA:   171s
[>>>>                           ] 65/500, 2.6 task/s, elapsed: 25s, ETA:   170s
[>>>>                           ] 66/500, 2.6 task/s, elapsed: 26s, ETA:   170s
[>>>>                           ] 67/500, 2.6 task/s, elapsed: 26s, ETA:   169s
[>>>>                           ] 68/500, 2.6 task/s, elapsed: 27s, ETA:   169s
[>>>>                           ] 69/500, 2.6 task/s, elapsed: 27s, ETA:   169s
[>>>>                           ] 70/500, 2.6 task/s, elapsed: 27s, ETA:   168s
[>>>>                           ] 71/500, 2.6 task/s, elapsed: 28s, ETA:   168s
[>>>>                           ] 72/500, 2.6 task/s, elapsed: 28s, ETA:   167s
[>>>>                           ] 73/500, 2.6 task/s, elapsed: 29s, ETA:   167s
[>>>>                           ] 74/500, 2.6 task/s, elapsed: 29s, ETA:   166s
[>>>>                           ] 75/500, 2.6 task/s, elapsed: 29s, ETA:   166s
[>>>>                           ] 76/500, 2.6 task/s, elapsed: 30s, ETA:   166s
[>>>>                           ] 77/500, 2.6 task/s, elapsed: 30s, ETA:   165s
[>>>>                           ] 78/500, 2.6 task/s, elapsed: 30s, ETA:   165s
[>>>>                           ] 79/500, 2.6 task/s, elapsed: 31s, ETA:   164s
[>>>>                           ] 80/500, 2.6 task/s, elapsed: 31s, ETA:   164s
[>>>>>                          ] 81/500, 2.6 task/s, elapsed: 32s, ETA:   164s
[>>>>>                          ] 82/500, 2.6 task/s, elapsed: 32s, ETA:   163s
[>>>>>                          ] 83/500, 2.6 task/s, elapsed: 32s, ETA:   163s
[>>>>>                          ] 84/500, 2.6 task/s, elapsed: 33s, ETA:   162s
[>>>>>                          ] 85/500, 2.6 task/s, elapsed: 33s, ETA:   162s
[>>>>>                          ] 86/500, 2.6 task/s, elapsed: 34s, ETA:   162s
[>>>>>                          ] 87/500, 2.6 task/s, elapsed: 34s, ETA:   161s
[>>>>>                          ] 88/500, 2.6 task/s, elapsed: 34s, ETA:   161s
[>>>>>                          ] 89/500, 2.6 task/s, elapsed: 35s, ETA:   160s
[>>>>>                          ] 90/500, 2.6 task/s, elapsed: 35s, ETA:   160s
[>>>>>                          ] 91/500, 2.6 task/s, elapsed: 35s, ETA:   160s
[>>>>>                          ] 92/500, 2.6 task/s, elapsed: 36s, ETA:   159s
[>>>>>                          ] 93/500, 2.6 task/s, elapsed: 36s, ETA:   159s
[>>>>>                          ] 94/500, 2.6 task/s, elapsed: 37s, ETA:   159s
[>>>>>                          ] 95/500, 2.6 task/s, elapsed: 37s, ETA:   158s
[>>>>>                          ] 96/500, 2.6 task/s, elapsed: 38s, ETA:   158s
[>>>>>>                         ] 97/500, 2.6 task/s, elapsed: 38s, ETA:   158s
[>>>>>>                         ] 98/500, 2.5 task/s, elapsed: 38s, ETA:   158s
[>>>>>>                         ] 99/500, 2.5 task/s, elapsed: 39s, ETA:   158s
[>>>>>>                        ] 100/500, 2.5 task/s, elapsed: 39s, ETA:   157s
[>>>>>>                        ] 101/500, 2.5 task/s, elapsed: 40s, ETA:   157s
[>>>>>>                        ] 102/500, 2.5 task/s, elapsed: 40s, ETA:   157s
[>>>>>>                        ] 103/500, 2.5 task/s, elapsed: 41s, ETA:   157s
[>>>>>>                        ] 104/500, 2.5 task/s, elapsed: 41s, ETA:   156s
[>>>>>>                        ] 105/500, 2.5 task/s, elapsed: 42s, ETA:   156s
[>>>>>>                        ] 106/500, 2.5 task/s, elapsed: 42s, ETA:   156s
[>>>>>>                        ] 107/500, 2.5 task/s, elapsed: 42s, ETA:   156s
[>>>>>>                        ] 108/500, 2.5 task/s, elapsed: 43s, ETA:   155s
[>>>>>>                        ] 109/500, 2.5 task/s, elapsed: 43s, ETA:   155s
[>>>>>>                        ] 110/500, 2.5 task/s, elapsed: 44s, ETA:   155s
[>>>>>>                        ] 111/500, 2.5 task/s, elapsed: 44s, ETA:   155s
[>>>>>>                        ] 112/500, 2.5 task/s, elapsed: 45s, ETA:   154s
[>>>>>>                        ] 113/500, 2.5 task/s, elapsed: 45s, ETA:   154s
[>>>>>>                        ] 114/500, 2.5 task/s, elapsed: 45s, ETA:   154s
[>>>>>>                        ] 115/500, 2.5 task/s, elapsed: 46s, ETA:   153s
[>>>>>>                        ] 116/500, 2.5 task/s, elapsed: 46s, ETA:   153s
[>>>>>>>                       ] 117/500, 2.5 task/s, elapsed: 47s, ETA:   153s
[>>>>>>>                       ] 118/500, 2.5 task/s, elapsed: 47s, ETA:   152s
[>>>>>>>                       ] 119/500, 2.5 task/s, elapsed: 47s, ETA:   152s
[>>>>>>>                       ] 120/500, 2.5 task/s, elapsed: 48s, ETA:   152s
[>>>>>>>                       ] 121/500, 2.5 task/s, elapsed: 48s, ETA:   151s
[>>>>>>>                       ] 122/500, 2.5 task/s, elapsed: 49s, ETA:   151s
[>>>>>>>                       ] 123/500, 2.5 task/s, elapsed: 49s, ETA:   150s
[>>>>>>>                       ] 124/500, 2.5 task/s, elapsed: 49s, ETA:   150s
[>>>>>>>                       ] 125/500, 2.5 task/s, elapsed: 50s, ETA:   149s
[>>>>>>>                       ] 126/500, 2.5 task/s, elapsed: 50s, ETA:   149s
[>>>>>>>                       ] 127/500, 2.5 task/s, elapsed: 51s, ETA:   149s
[>>>>>>>                       ] 128/500, 2.5 task/s, elapsed: 51s, ETA:   148s
[>>>>>>>                       ] 129/500, 2.5 task/s, elapsed: 51s, ETA:   148s
[>>>>>>>                       ] 130/500, 2.5 task/s, elapsed: 52s, ETA:   147s
[>>>>>>>                       ] 131/500, 2.5 task/s, elapsed: 52s, ETA:   147s
[>>>>>>>                       ] 132/500, 2.5 task/s, elapsed: 53s, ETA:   146s
[>>>>>>>                       ] 133/500, 2.5 task/s, elapsed: 53s, ETA:   146s
[>>>>>>>>                      ] 134/500, 2.5 task/s, elapsed: 53s, ETA:   146s
[>>>>>>>>                      ] 135/500, 2.5 task/s, elapsed: 54s, ETA:   145s
[>>>>>>>>                      ] 136/500, 2.5 task/s, elapsed: 54s, ETA:   145s
[>>>>>>>>                      ] 137/500, 2.5 task/s, elapsed: 54s, ETA:   144s
[>>>>>>>>                      ] 138/500, 2.5 task/s, elapsed: 55s, ETA:   144s
[>>>>>>>>                      ] 139/500, 2.5 task/s, elapsed: 55s, ETA:   143s
[>>>>>>>>                      ] 140/500, 2.5 task/s, elapsed: 56s, ETA:   143s
[>>>>>>>>                      ] 141/500, 2.5 task/s, elapsed: 56s, ETA:   143s
[>>>>>>>>                      ] 142/500, 2.5 task/s, elapsed: 56s, ETA:   142s
[>>>>>>>>                      ] 143/500, 2.5 task/s, elapsed: 57s, ETA:   142s
[>>>>>>>>                      ] 144/500, 2.5 task/s, elapsed: 57s, ETA:   141s
[>>>>>>>>                      ] 145/500, 2.5 task/s, elapsed: 58s, ETA:   141s
[>>>>>>>>                      ] 146/500, 2.5 task/s, elapsed: 58s, ETA:   140s
[>>>>>>>>                      ] 147/500, 2.5 task/s, elapsed: 58s, ETA:   140s
[>>>>>>>>                      ] 148/500, 2.5 task/s, elapsed: 59s, ETA:   140s
[>>>>>>>>                      ] 149/500, 2.5 task/s, elapsed: 59s, ETA:   139s
[>>>>>>>>>                     ] 150/500, 2.5 task/s, elapsed: 59s, ETA:   139s
[>>>>>>>>>                     ] 151/500, 2.5 task/s, elapsed: 60s, ETA:   138s
[>>>>>>>>>                     ] 152/500, 2.5 task/s, elapsed: 60s, ETA:   138s
[>>>>>>>>>                     ] 153/500, 2.5 task/s, elapsed: 61s, ETA:   138s
[>>>>>>>>>                     ] 154/500, 2.5 task/s, elapsed: 61s, ETA:   137s
[>>>>>>>>>                     ] 155/500, 2.5 task/s, elapsed: 61s, ETA:   137s
[>>>>>>>>>                     ] 156/500, 2.5 task/s, elapsed: 62s, ETA:   136s
[>>>>>>>>>                     ] 157/500, 2.5 task/s, elapsed: 62s, ETA:   136s
[>>>>>>>>>                     ] 158/500, 2.5 task/s, elapsed: 63s, ETA:   135s
[>>>>>>>>>                     ] 159/500, 2.5 task/s, elapsed: 63s, ETA:   135s
[>>>>>>>>>                     ] 160/500, 2.5 task/s, elapsed: 63s, ETA:   135s
[>>>>>>>>>                     ] 161/500, 2.5 task/s, elapsed: 64s, ETA:   134s
[>>>>>>>>>                     ] 162/500, 2.5 task/s, elapsed: 64s, ETA:   134s
[>>>>>>>>>                     ] 163/500, 2.5 task/s, elapsed: 65s, ETA:   133s
[>>>>>>>>>                     ] 164/500, 2.5 task/s, elapsed: 65s, ETA:   133s
[>>>>>>>>>                     ] 165/500, 2.5 task/s, elapsed: 65s, ETA:   133s
[>>>>>>>>>                     ] 166/500, 2.5 task/s, elapsed: 66s, ETA:   132s
[>>>>>>>>>>                    ] 167/500, 2.5 task/s, elapsed: 66s, ETA:   132s
[>>>>>>>>>>                    ] 168/500, 2.5 task/s, elapsed: 66s, ETA:   131s
[>>>>>>>>>>                    ] 169/500, 2.5 task/s, elapsed: 67s, ETA:   131s
[>>>>>>>>>>                    ] 170/500, 2.5 task/s, elapsed: 67s, ETA:   131s
[>>>>>>>>>>                    ] 171/500, 2.5 task/s, elapsed: 68s, ETA:   130s
[>>>>>>>>>>                    ] 172/500, 2.5 task/s, elapsed: 68s, ETA:   130s
[>>>>>>>>>>                    ] 173/500, 2.5 task/s, elapsed: 68s, ETA:   129s
[>>>>>>>>>>                    ] 174/500, 2.5 task/s, elapsed: 69s, ETA:   129s
[>>>>>>>>>>                    ] 175/500, 2.5 task/s, elapsed: 69s, ETA:   128s
[>>>>>>>>>>                    ] 176/500, 2.5 task/s, elapsed: 70s, ETA:   128s
[>>>>>>>>>>                    ] 177/500, 2.5 task/s, elapsed: 70s, ETA:   128s
[>>>>>>>>>>                    ] 178/500, 2.5 task/s, elapsed: 70s, ETA:   127s
[>>>>>>>>>>                    ] 179/500, 2.5 task/s, elapsed: 71s, ETA:   127s
[>>>>>>>>>>                    ] 180/500, 2.5 task/s, elapsed: 71s, ETA:   126s
[>>>>>>>>>>                    ] 181/500, 2.5 task/s, elapsed: 71s, ETA:   126s
[>>>>>>>>>>                    ] 182/500, 2.5 task/s, elapsed: 72s, ETA:   126s
[>>>>>>>>>>                    ] 183/500, 2.5 task/s, elapsed: 72s, ETA:   125s
[>>>>>>>>>>>                   ] 184/500, 2.5 task/s, elapsed: 73s, ETA:   125s
[>>>>>>>>>>>                   ] 185/500, 2.5 task/s, elapsed: 73s, ETA:   124s
[>>>>>>>>>>>                   ] 186/500, 2.5 task/s, elapsed: 73s, ETA:   124s
[>>>>>>>>>>>                   ] 187/500, 2.5 task/s, elapsed: 74s, ETA:   124s
[>>>>>>>>>>>                   ] 188/500, 2.5 task/s, elapsed: 74s, ETA:   123s
[>>>>>>>>>>>                   ] 189/500, 2.5 task/s, elapsed: 75s, ETA:   123s
[>>>>>>>>>>>                   ] 190/500, 2.5 task/s, elapsed: 75s, ETA:   122s
[>>>>>>>>>>>                   ] 191/500, 2.5 task/s, elapsed: 75s, ETA:   122s
[>>>>>>>>>>>                   ] 192/500, 2.5 task/s, elapsed: 76s, ETA:   122s
[>>>>>>>>>>>                   ] 193/500, 2.5 task/s, elapsed: 76s, ETA:   121s
[>>>>>>>>>>>                   ] 194/500, 2.5 task/s, elapsed: 77s, ETA:   121s
[>>>>>>>>>>>                   ] 195/500, 2.5 task/s, elapsed: 77s, ETA:   120s
[>>>>>>>>>>>                   ] 196/500, 2.5 task/s, elapsed: 77s, ETA:   120s
[>>>>>>>>>>>                   ] 197/500, 2.5 task/s, elapsed: 78s, ETA:   119s
[>>>>>>>>>>>                   ] 198/500, 2.5 task/s, elapsed: 78s, ETA:   119s
[>>>>>>>>>>>                   ] 199/500, 2.5 task/s, elapsed: 78s, ETA:   119s
[>>>>>>>>>>>>                  ] 200/500, 2.5 task/s, elapsed: 79s, ETA:   118s
[>>>>>>>>>>>>                  ] 201/500, 2.5 task/s, elapsed: 79s, ETA:   118s
[>>>>>>>>>>>>                  ] 202/500, 2.5 task/s, elapsed: 80s, ETA:   117s
[>>>>>>>>>>>>                  ] 203/500, 2.5 task/s, elapsed: 80s, ETA:   117s
[>>>>>>>>>>>>                  ] 204/500, 2.5 task/s, elapsed: 80s, ETA:   117s
[>>>>>>>>>>>>                  ] 205/500, 2.5 task/s, elapsed: 81s, ETA:   116s
[>>>>>>>>>>>>                  ] 206/500, 2.5 task/s, elapsed: 81s, ETA:   116s
[>>>>>>>>>>>>                  ] 207/500, 2.5 task/s, elapsed: 82s, ETA:   115s
[>>>>>>>>>>>>                  ] 208/500, 2.5 task/s, elapsed: 82s, ETA:   115s
[>>>>>>>>>>>>                  ] 209/500, 2.5 task/s, elapsed: 82s, ETA:   115s
[>>>>>>>>>>>>                  ] 210/500, 2.5 task/s, elapsed: 83s, ETA:   114s
[>>>>>>>>>>>>                  ] 211/500, 2.5 task/s, elapsed: 83s, ETA:   114s
[>>>>>>>>>>>>                  ] 212/500, 2.5 task/s, elapsed: 83s, ETA:   113s
[>>>>>>>>>>>>                  ] 213/500, 2.5 task/s, elapsed: 84s, ETA:   113s
[>>>>>>>>>>>>                  ] 214/500, 2.5 task/s, elapsed: 84s, ETA:   113s
[>>>>>>>>>>>>                  ] 215/500, 2.5 task/s, elapsed: 85s, ETA:   112s
[>>>>>>>>>>>>                  ] 216/500, 2.5 task/s, elapsed: 85s, ETA:   112s
[>>>>>>>>>>>>>                 ] 217/500, 2.5 task/s, elapsed: 85s, ETA:   111s
[>>>>>>>>>>>>>                 ] 218/500, 2.5 task/s, elapsed: 86s, ETA:   111s
[>>>>>>>>>>>>>                 ] 219/500, 2.5 task/s, elapsed: 86s, ETA:   111s
[>>>>>>>>>>>>>                 ] 220/500, 2.5 task/s, elapsed: 87s, ETA:   110s
[>>>>>>>>>>>>>                 ] 221/500, 2.5 task/s, elapsed: 87s, ETA:   110s
[>>>>>>>>>>>>>                 ] 222/500, 2.5 task/s, elapsed: 87s, ETA:   109s
[>>>>>>>>>>>>>                 ] 223/500, 2.5 task/s, elapsed: 88s, ETA:   109s
[>>>>>>>>>>>>>                 ] 224/500, 2.5 task/s, elapsed: 88s, ETA:   109s
[>>>>>>>>>>>>>                 ] 225/500, 2.5 task/s, elapsed: 89s, ETA:   108s
[>>>>>>>>>>>>>                 ] 226/500, 2.5 task/s, elapsed: 89s, ETA:   108s
[>>>>>>>>>>>>>                 ] 227/500, 2.5 task/s, elapsed: 89s, ETA:   107s
[>>>>>>>>>>>>>                 ] 228/500, 2.5 task/s, elapsed: 90s, ETA:   107s
[>>>>>>>>>>>>>                 ] 229/500, 2.5 task/s, elapsed: 90s, ETA:   107s
[>>>>>>>>>>>>>                 ] 230/500, 2.5 task/s, elapsed: 90s, ETA:   106s
[>>>>>>>>>>>>>                 ] 231/500, 2.5 task/s, elapsed: 91s, ETA:   106s
[>>>>>>>>>>>>>                 ] 232/500, 2.5 task/s, elapsed: 91s, ETA:   105s
[>>>>>>>>>>>>>                 ] 233/500, 2.5 task/s, elapsed: 92s, ETA:   105s
[>>>>>>>>>>>>>>                ] 234/500, 2.5 task/s, elapsed: 92s, ETA:   105s
[>>>>>>>>>>>>>>                ] 235/500, 2.5 task/s, elapsed: 92s, ETA:   104s
[>>>>>>>>>>>>>>                ] 236/500, 2.5 task/s, elapsed: 93s, ETA:   104s
[>>>>>>>>>>>>>>                ] 237/500, 2.5 task/s, elapsed: 93s, ETA:   103s
[>>>>>>>>>>>>>>                ] 238/500, 2.5 task/s, elapsed: 94s, ETA:   103s
[>>>>>>>>>>>>>>                ] 239/500, 2.5 task/s, elapsed: 94s, ETA:   103s
[>>>>>>>>>>>>>>                ] 240/500, 2.5 task/s, elapsed: 94s, ETA:   102s
[>>>>>>>>>>>>>>                ] 241/500, 2.5 task/s, elapsed: 95s, ETA:   102s
[>>>>>>>>>>>>>>                ] 242/500, 2.5 task/s, elapsed: 95s, ETA:   101s
[>>>>>>>>>>>>>>                ] 243/500, 2.5 task/s, elapsed: 96s, ETA:   101s
[>>>>>>>>>>>>>>                ] 244/500, 2.5 task/s, elapsed: 96s, ETA:   101s
[>>>>>>>>>>>>>>                ] 245/500, 2.5 task/s, elapsed: 96s, ETA:   100s
[>>>>>>>>>>>>>>                ] 246/500, 2.5 task/s, elapsed: 97s, ETA:   100s
[>>>>>>>>>>>>>>                ] 247/500, 2.5 task/s, elapsed: 97s, ETA:   100s
[>>>>>>>>>>>>>>                ] 248/500, 2.5 task/s, elapsed: 98s, ETA:    99s
[>>>>>>>>>>>>>>                ] 249/500, 2.5 task/s, elapsed: 98s, ETA:    99s
[>>>>>>>>>>>>>>>               ] 250/500, 2.5 task/s, elapsed: 98s, ETA:    98s
[>>>>>>>>>>>>>>>               ] 251/500, 2.5 task/s, elapsed: 99s, ETA:    98s
[>>>>>>>>>>>>>>>               ] 252/500, 2.5 task/s, elapsed: 99s, ETA:    98s
[>>>>>>>>>>>>>>               ] 253/500, 2.5 task/s, elapsed: 100s, ETA:    97s
[>>>>>>>>>>>>>>               ] 254/500, 2.5 task/s, elapsed: 100s, ETA:    97s
[>>>>>>>>>>>>>>               ] 255/500, 2.5 task/s, elapsed: 100s, ETA:    96s
[>>>>>>>>>>>>>>               ] 256/500, 2.5 task/s, elapsed: 101s, ETA:    96s
[>>>>>>>>>>>>>>               ] 257/500, 2.5 task/s, elapsed: 101s, ETA:    96s
[>>>>>>>>>>>>>>               ] 258/500, 2.5 task/s, elapsed: 101s, ETA:    95s
[>>>>>>>>>>>>>>>              ] 259/500, 2.5 task/s, elapsed: 102s, ETA:    95s
[>>>>>>>>>>>>>>>              ] 260/500, 2.5 task/s, elapsed: 102s, ETA:    94s/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:50:08,655 - mmseg - INFO - Iter [17600/40000]	lr: 3.360e-05, eta: 9:36:02, time: 1.479, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1239, decode.acc_seg: 90.2612, src.loss_imnet_feat_dist: 0.1093, mix.decode.loss_seg: 0.1456, mix.decode.acc_seg: 88.4930
sk/s, elapsed: 105s, ETA:    91s
[>>>>>>>>>>>>>>>              ] 269/500, 2.5 task/s, elapsed: 106s, ETA:    91s
[>>>>>>>>>>>>>>>              ] 270/500, 2.5 task/s, elapsed: 106s, ETA:    90s
[>>>>>>>>>>>>>>>              ] 271/500, 2.5 task/s, elapsed: 107s, ETA:    90s
[>>>>>>>>>>>>>>>              ] 272/500, 2.5 task/s, elapsed: 107s, ETA:    90s
[>>>>>>>>>>>>>>>              ] 273/500, 2.5 task/s, elapsed: 107s, ETA:    89s
[>>>>>>>>>>>>>>>              ] 274/500, 2.5 task/s, elapsed: 108s, ETA:    89s
[>>>>>>>>>>>>>>>              ] 275/500, 2.5 task/s, elapsed: 108s, ETA:    88s
[>>>>>>>>>>>>>>>>             ] 276/500, 2.5 task/s, elapsed: 109s, ETA:    88s
[>>>>>>>>>>>>>>>>             ] 277/500, 2.5 task/s, elapsed: 109s, ETA:    88s
[>>>>>>>>>>>>>>>>             ] 278/500, 2.5 task/s, elapsed: 109s, ETA:    87s
[>>>>>>>>>>>>>>>>             ] 279/500, 2.5 task/s, elapsed: 110s, ETA:    87s
[>>>>>>>>>>>>>>>>             ] 280/500, 2.5 task/s, elapsed: 110s, ETA:    87s
[>>>>>>>>>>>>>>>>             ] 281/500, 2.5 task/s, elapsed: 111s, ETA:    86s
[>>>>>>>>>>>>>>>>             ] 282/500, 2.5 task/s, elapsed: 111s, ETA:    86s
[>>>>>>>>>>>>>>>>             ] 283/500, 2.5 task/s, elapsed: 111s, ETA:    85s
[>>>>>>>>>>>>>>>>             ] 284/500, 2.5 task/s, elapsed: 112s, ETA:    85s
[>>>>>>>>>>>>>>>>             ] 285/500, 2.5 task/s, elapsed: 112s, ETA:    85s
[>>>>>>>>>>>>>>>>             ] 286/500, 2.5 task/s, elapsed: 112s, ETA:    84s
[>>>>>>>>>>>>>>>>             ] 287/500, 2.5 task/s, elapsed: 113s, ETA:    84s
[>>>>>>>>>>>>>>>>             ] 288/500, 2.5 task/s, elapsed: 113s, ETA:    83s
[>>>>>>>>>>>>>>>>             ] 289/500, 2.5 task/s, elapsed: 114s, ETA:    83s
[>>>>>>>>>>>>>>>>             ] 290/500, 2.5 task/s, elapsed: 114s, ETA:    83s
[>>>>>>>>>>>>>>>>             ] 291/500, 2.5 task/s, elapsed: 114s, ETA:    82s
[>>>>>>>>>>>>>>>>             ] 292/500, 2.5 task/s, elapsed: 115s, ETA:    82s
[>>>>>>>>>>>>>>>>             ] 293/500, 2.5 task/s, elapsed: 115s, ETA:    81s
[>>>>>>>>>>>>>>>>>            ] 294/500, 2.5 task/s, elapsed: 116s, ETA:    81s
[>>>>>>>>>>>>>>>>>            ] 295/500, 2.5 task/s, elapsed: 116s, ETA:    81s
[>>>>>>>>>>>>>>>>>            ] 296/500, 2.5 task/s, elapsed: 116s, ETA:    80s
[>>>>>>>>>>>>>>>>>            ] 297/500, 2.5 task/s, elapsed: 117s, ETA:    80s
[>>>>>>>>>>>>>>>>>            ] 298/500, 2.5 task/s, elapsed: 117s, ETA:    79s
[>>>>>>>>>>>>>>>>>            ] 299/500, 2.5 task/s, elapsed: 118s, ETA:    79s
[>>>>>>>>>>>>>>>>>            ] 300/500, 2.5 task/s, elapsed: 118s, ETA:    79s
[>>>>>>>>>>>>>>>>>            ] 301/500, 2.5 task/s, elapsed: 118s, ETA:    78s
[>>>>>>>>>>>>>>>>>            ] 302/500, 2.5 task/s, elapsed: 119s, ETA:    78s
[>>>>>>>>>>>>>>>>>            ] 303/500, 2.5 task/s, elapsed: 119s, ETA:    77s
[>>>>>>>>>>>>>>>>>            ] 304/500, 2.5 task/s, elapsed: 119s, ETA:    77s
[>>>>>>>>>>>>>>>>>            ] 305/500, 2.5 task/s, elapsed: 120s, ETA:    77s
[>>>>>>>>>>>>>>>>>            ] 306/500, 2.5 task/s, elapsed: 120s, ETA:    76s
[>>>>>>>>>>>>>>>>>            ] 307/500, 2.5 task/s, elapsed: 121s, ETA:    76s
[>>>>>>>>>>>>>>>>>            ] 308/500, 2.5 task/s, elapsed: 121s, ETA:    75s
[>>>>>>>>>>>>>>>>>            ] 309/500, 2.5 task/s, elapsed: 121s, ETA:    75s
[>>>>>>>>>>>>>>>>>            ] 310/500, 2.5 task/s, elapsed: 122s, ETA:    75s
[>>>>>>>>>>>>>>>>>>           ] 311/500, 2.5 task/s, elapsed: 122s, ETA:    74s
[>>>>>>>>>>>>>>>>>>           ] 312/500, 2.5 task/s, elapsed: 123s, ETA:    74s
[>>>>>>>>>>>>>>>>>>           ] 313/500, 2.5 task/s, elapsed: 123s, ETA:    73s
[>>>>>>>>>>>>>>>>>>           ] 314/500, 2.5 task/s, elapsed: 123s, ETA:    73s
[>>>>>>>>>>>>>>>>>>           ] 315/500, 2.5 task/s, elapsed: 124s, ETA:    73s
[>>>>>>>>>>>>>>>>>>           ] 316/500, 2.5 task/s, elapsed: 124s, ETA:    72s
[>>>>>>>>>>>>>>>>>>           ] 317/500, 2.5 task/s, elapsed: 125s, ETA:    72s
[>>>>>>>>>>>>>>>>>>           ] 318/500, 2.5 task/s, elapsed: 125s, ETA:    71s
[>>>>>>>>>>>>>>>>>>           ] 319/500, 2.5 task/s, elapsed: 125s, ETA:    71s
[>>>>>>>>>>>>>>>>>>           ] 320/500, 2.5 task/s, elapsed: 126s, ETA:    71s
[>>>>>>>>>>>>>>>>>>           ] 321/500, 2.5 task/s, elapsed: 126s, ETA:    70s
[>>>>>>>>>>>>>>>>>>           ] 322/500, 2.5 task/s, elapsed: 126s, ETA:    70s
[>>>>>>>>>>>>>>>>>>           ] 323/500, 2.5 task/s, elapsed: 127s, ETA:    70s
[>>>>>>>>>>>>>>>>>>           ] 324/500, 2.5 task/s, elapsed: 127s, ETA:    69s
[>>>>>>>>>>>>>>>>>>           ] 325/500, 2.5 task/s, elapsed: 128s, ETA:    69s
[>>>>>>>>>>>>>>>>>>           ] 326/500, 2.5 task/s, elapsed: 128s, ETA:    68s
[>>>>>>>>>>>>>>>>>>           ] 327/500, 2.5 task/s, elapsed: 128s, ETA:    68s
[>>>>>>>>>>>>>>>>>>>          ] 328/500, 2.5 task/s, elapsed: 129s, ETA:    68s
[>>>>>>>>>>>>>>>>>>>          ] 329/500, 2.5 task/s, elapsed: 129s, ETA:    67s
[>>>>>>>>>>>>>>>>>>>          ] 330/500, 2.5 task/s, elapsed: 130s, ETA:    67s
[>>>>>>>>>>>>>>>>>>>          ] 331/500, 2.5 task/s, elapsed: 130s, ETA:    66s
[>>>>>>>>>>>>>>>>>>>          ] 332/500, 2.5 task/s, elapsed: 130s, ETA:    66s
[>>>>>>>>>>>>>>>>>>>          ] 333/500, 2.5 task/s, elapsed: 131s, ETA:    66s
[>>>>>>>>>>>>>>>>>>>          ] 334/500, 2.5 task/s, elapsed: 131s, ETA:    65s
[>>>>>>>>>>>>>>>>>>>          ] 335/500, 2.5 task/s, elapsed: 132s, ETA:    65s
[>>>>>>>>>>>>>>>>>>>          ] 336/500, 2.5 task/s, elapsed: 132s, ETA:    64s
[>>>>>>>>>>>>>>>>>>>          ] 337/500, 2.5 task/s, elapsed: 132s, ETA:    64s
[>>>>>>>>>>>>>>>>>>>          ] 338/500, 2.5 task/s, elapsed: 133s, ETA:    64s
[>>>>>>>>>>>>>>>>>>>          ] 339/500, 2.5 task/s, elapsed: 133s, ETA:    63s
[>>>>>>>>>>>>>>>>>>>          ] 340/500, 2.5 task/s, elapsed: 134s, ETA:    63s
[>>>>>>>>>>>>>>>>>>>          ] 341/500, 2.5 task/s, elapsed: 134s, ETA:    62s
[>>>>>>>>>>>>>>>>>>>          ] 342/500, 2.5 task/s, elapsed: 134s, ETA:    62s
[>>>>>>>>>>>>>>>>>>>          ] 343/500, 2.5 task/s, elapsed: 135s, ETA:    62s
[>>>>>>>>>>>>>>>>>>>          ] 344/500, 2.5 task/s, elapsed: 135s, ETA:    61s
[>>>>>>>>>>>>>>>>>>>>         ] 345/500, 2.5 task/s, elapsed: 136s, ETA:    61s
[>>>>>>>>>>>>>>>>>>>>         ] 346/500, 2.5 task/s, elapsed: 136s, ETA:    61s
[>>>>>>>>>>>>>>>>>>>>         ] 347/500, 2.5 task/s, elapsed: 136s, ETA:    60s
[>>>>>>>>>>>>>>>>>>>>         ] 348/500, 2.5 task/s, elapsed: 137s, ETA:    60s
[>>>>>>>>>>>>>>>>>>>>         ] 349/500, 2.5 task/s, elapsed: 137s, ETA:    59s
[>>>>>>>>>>>>>>>>>>>>         ] 350/500, 2.5 task/s, elapsed: 138s, ETA:    59s
[>>>>>>>>>>>>>>>>>>>>         ] 351/500, 2.5 task/s, elapsed: 138s, ETA:    59s
[>>>>>>>>>>>>>>>>>>>>         ] 352/500, 2.5 task/s, elapsed: 138s, ETA:    58s
[>>>>>>>>>>>>>>>>>>>>         ] 353/500, 2.5 task/s, elapsed: 139s, ETA:    58s
[>>>>>>>>>>>>>>>>>>>>         ] 354/500, 2.5 task/s, elapsed: 139s, ETA:    57s
[>>>>>>>>>>>>>>>>>>>>         ] 355/500, 2.5 task/s, elapsed: 139s, ETA:    57s
[>>>>>>>>>>>>>>>>>>>>         ] 356/500, 2.5 task/s, elapsed: 140s, ETA:    57s
[>>>>>>>>>>>>>>>>>>>>         ] 357/500, 2.5 task/s, elapsed: 140s, ETA:    56s
[>>>>>>>>>>>>>>>>>>>>         ] 358/500, 2.5 task/s, elapsed: 141s, ETA:    56s
[>>>>>>>>>>>>>>>>>>>>         ] 359/500, 2.5 task/s, elapsed: 141s, ETA:    55s
[>>>>>>>>>>>>>>>>>>>>         ] 360/500, 2.5 task/s, elapsed: 141s, ETA:    55s
[>>>>>>>>>>>>>>>>>>>>         ] 361/500, 2.5 task/s, elapsed: 142s, ETA:    55s
[>>>>>>>>>>>>>>>>>>>>         ] 362/500, 2.5 task/s, elapsed: 142s, ETA:    54s
[>>>>>>>>>>>>>>>>>>>>>        ] 363/500, 2.5 task/s, elapsed: 143s, ETA:    54s
[>>>>>>>>>>>>>>>>>>>>>        ] 364/500, 2.5 task/s, elapsed: 143s, ETA:    53s
[>>>>>>>>>>>>>>>>>>>>>        ] 365/500, 2.5 task/s, elapsed: 143s, ETA:    53s
[>>>>>>>>>>>>>>>>>>>>>        ] 366/500, 2.5 task/s, elapsed: 144s, ETA:    53s
[>>>>>>>>>>>>>>>>>>>>>        ] 367/500, 2.5 task/s, elapsed: 144s, ETA:    52s
[>>>>>>>>>>>>>>>>>>>>>        ] 368/500, 2.5 task/s, elapsed: 145s, ETA:    52s
[>>>>>>>>>>>>>>>>>>>>>        ] 369/500, 2.5 task/s, elapsed: 145s, ETA:    51s
[>>>>>>>>>>>>>>>>>>>>>        ] 370/500, 2.5 task/s, elapsed: 145s, ETA:    51s
[>>>>>>>>>>>>>>>>>>>>>        ] 371/500, 2.5 task/s, elapsed: 146s, ETA:    51s
[>>>>>>>>>>>>>>>>>>>>>        ] 372/500, 2.5 task/s, elapsed: 146s, ETA:    50s
[>>>>>>>>>>>>>>>>>>>>>        ] 373/500, 2.5 task/s, elapsed: 147s, ETA:    50s
[>>>>>>>>>>>>>>>>>>>>>        ] 374/500, 2.5 task/s, elapsed: 147s, ETA:    49s
[>>>>>>>>>>>>>>>>>>>>>        ] 375/500, 2.5 task/s, elapsed: 147s, ETA:    49s
[>>>>>>>>>>>>>>>>>>>>>        ] 376/500, 2.5 task/s, elapsed: 148s, ETA:    49s
[>>>>>>>>>>>>>>>>>>>>>        ] 377/500, 2.5 task/s, elapsed: 148s, ETA:    48s
[>>>>>>>>>>>>>>>>>>>>>        ] 378/500, 2.5 task/s, elapsed: 148s, ETA:    48s
[>>>>>>>>>>>>>>>>>>>>>        ] 379/500, 2.5 task/s, elapsed: 149s, ETA:    48s
[>>>>>>>>>>>>>>>>>>>>>>       ] 380/500, 2.5 task/s, elapsed: 149s, ETA:    47s
[>>>>>>>>>>>>>>>>>>>>>>       ] 381/500, 2.5 task/s, elapsed: 150s, ETA:    47s
[>>>>>>>>>>>>>>>>>>>>>>       ] 382/500, 2.5 task/s, elapsed: 150s, ETA:    46s
[>>>>>>>>>>>>>>>>>>>>>>       ] 383/500, 2.5 task/s, elapsed: 150s, ETA:    46s
[>>>>>>>>>>>>>>>>>>>>>>       ] 384/500, 2.5 task/s, elapsed: 151s, ETA:    46s
[>>>>>>>>>>>>>>>>>>>>>>       ] 385/500, 2.5 task/s, elapsed: 151s, ETA:    45s
[>>>>>>>>>>>>>>>>>>>>>>       ] 386/500, 2.5 task/s, elapsed: 152s, ETA:    45s
[>>>>>>>>>>>>>>>>>>>>>>       ] 387/500, 2.5 task/s, elapsed: 152s, ETA:    44s
[>>>>>>>>>>>>>>>>>>>>>>       ] 388/500, 2.5 task/s, elapsed: 152s, ETA:    44s
[>>>>>>>>>>>>>>>>>>>>>>       ] 389/500, 2.5 task/s, elapsed: 153s, ETA:    44s
[>>>>>>>>>>>>>>>>>>>>>>       ] 390/500, 2.5 task/s, elapsed: 153s, ETA:    43s
[>>>>>>>>>>>>>>>>>>>>>>       ] 391/500, 2.5 task/s, elapsed: 154s, ETA:    43s
[>>>>>>>>>>>>>>>>>>>>>>       ] 392/500, 2.5 task/s, elapsed: 154s, ETA:    42s
[>>>>>>>>>>>>>>>>>>>>>>       ] 393/500, 2.5 task/s, elapsed: 154s, ETA:    42s
[>>>>>>>>>>>>>>>>>>>>>>       ] 394/500, 2.5 task/s, elapsed: 155s, ETA:    42s
[>>>>>>>>>>>>>>>>>>>>>>       ] 395/500, 2.5 task/s, elapsed: 155s, ETA:    41s
[>>>>>>>>>>>>>>>>>>>>>>       ] 396/500, 2.5 task/s, elapsed: 156s, ETA:    41s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 397/500, 2.5 task/s, elapsed: 156s, ETA:    40s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 398/500, 2.5 task/s, elapsed: 156s, ETA:    40s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 399/500, 2.5 task/s, elapsed: 157s, ETA:    40s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 400/500, 2.5 task/s, elapsed: 157s, ETA:    39s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 401/500, 2.5 task/s, elapsed: 158s, ETA:    39s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 402/500, 2.5 task/s, elapsed: 158s, ETA:    38s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 403/500, 2.5 task/s, elapsed: 158s, ETA:    38s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 404/500, 2.5 task/s, elapsed: 159s, ETA:    38s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 405/500, 2.5 task/s, elapsed: 159s, ETA:    37s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 406/500, 2.5 task/s, elapsed: 159s, ETA:    37s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 407/500, 2.5 task/s, elapsed: 160s, ETA:    37s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 408/500, 2.5 task/s, elapsed: 160s, ETA:    36s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 409/500, 2.5 task/s, elapsed: 161s, ETA:    36s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 410/500, 2.5 task/s, elapsed: 161s, ETA:    35s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 411/500, 2.5 task/s, elapsed: 161s, ETA:    35s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 412/500, 2.5 task/s, elapsed: 162s, ETA:    35s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 413/500, 2.5 task/s, elapsed: 162s, ETA:    34s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 414/500, 2.5 task/s, elapsed: 163s, ETA:    34s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 415/500, 2.5 task/s, elapsed: 163s, ETA:    33s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 416/500, 2.5 task/s, elapsed: 163s, ETA:    33s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 417/500, 2.5 task/s, elapsed: 164s, ETA:    33s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 418/500, 2.5 task/s, elapsed: 164s, ETA:    32s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 419/500, 2.5 task/s, elapsed: 165s, ETA:    32s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 420/500, 2.5 task/s, elapsed: 165s, ETA:    31s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 421/500, 2.5 task/s, elapsed: 165s, ETA:    31s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 422/500, 2.5 task/s, elapsed: 166s, ETA:    31s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 423/500, 2.5 task/s, elapsed: 166s, ETA:    30s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 424/500, 2.5 task/s, elapsed: 166s, ETA:    30s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 425/500, 2.5 task/s, elapsed: 167s, ETA:    29s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 426/500, 2.5 task/s, elapsed: 167s, ETA:    29s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 427/500, 2.5 task/s, elapsed: 168s, ETA:    29s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 428/500, 2.5 task/s, elapsed: 168s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 429/500, 2.5 task/s, elapsed: 168s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 430/500, 2.5 task/s, elapsed: 169s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 431/500, 2.5 task/s, elapsed: 169s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 432/500, 2.5 task/s, elapsed: 170s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 433/500, 2.5 task/s, elapsed: 170s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 434/500, 2.5 task/s, elapsed: 170s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 435/500, 2.5 task/s, elapsed: 171s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 436/500, 2.5 task/s, elapsed: 171s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 437/500, 2.5 task/s, elapsed: 172s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 438/500, 2.5 task/s, elapsed: 172s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 439/500, 2.5 task/s, elapsed: 172s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 440/500, 2.5 task/s, elapsed: 173s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 441/500, 2.5 task/s, elapsed: 173s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 442/500, 2.5 task/s, elapsed: 173s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 443/500, 2.5 task/s, elapsed: 174s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 444/500, 2.5 task/s, elapsed: 174s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 445/500, 2.5 task/s, elapsed: 175s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 446/500, 2.5 task/s, elapsed: 175s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 447/500, 2.5 task/s, elapsed: 175s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 448/500, 2.5 task/s, elapsed: 176s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 449/500, 2.5 task/s, elapsed: 176s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 450/500, 2.5 task/s, elapsed: 177s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 451/500, 2.5 task/s, elapsed: 177s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 452/500, 2.5 task/s, elapsed: 177s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 453/500, 2.5 task/s, elapsed: 178s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 454/500, 2.5 task/s, elapsed: 178s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 455/500, 2.5 task/s, elapsed: 179s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 456/500, 2.5 task/s, elapsed: 179s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 457/500, 2.5 task/s, elapsed: 179s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 458/500, 2.5 task/s, elapsed: 180s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 459/500, 2.5 task/s, elapsed: 180s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 460/500, 2.5 task/s, elapsed: 181s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 461/500, 2.5 task/s, elapsed: 181s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 462/500, 2.5 task/s, elapsed: 181s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 463/500, 2.5 task/s, elapsed: 182s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 464/500, 2.5 task/s, elapsed: 182s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 465/500, 2.5 task/s, elapsed: 183s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 466/500, 2.5 task/s, elapsed: 183s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 467/500, 2.5 task/s, elapsed: 184s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 468/500, 2.5 task/s, elapsed: 184s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 469/500, 2.5 task/s, elapsed: 184s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 470/500, 2.5 task/s, elapsed: 185s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 471/500, 2.5 task/s, elapsed: 185s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 472/500, 2.5 task/s, elapsed: 186s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 473/500, 2.5 task/s, elapsed: 186s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 474/500, 2.5 task/s, elapsed: 186s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 475/500, 2.5 task/s, elapsed: 187s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 476/500, 2.5 task/s, elapsed: 187s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 477/500, 2.5 task/s, elapsed: 188s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 478/500, 2.5 task/s, elapsed: 188s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 479/500, 2.5 task/s, elapsed: 188s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 480/500, 2.5 task/s, elapsed: 189s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 481/500, 2.5 task/s, elapsed: 189s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 482/500, 2.5 task/s, elapsed: 190s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 483/500, 2.5 task/s, elapsed: 190s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 484/500, 2.5 task/s, elapsed: 190s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 485/500, 2.5 task/s, elapsed: 191s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 486/500, 2.5 task/s, elapsed: 191s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 487/500, 2.5 task/s, elapsed: 192s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 488/500, 2.5 task/s, elapsed: 192s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 489/500, 2.5 task/s, elapsed: 192s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 490/500, 2.5 task/s, elapsed: 193s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 491/500, 2.5 task/s, elapsed: 193s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 492/500, 2.5 task/s, elapsed: 194s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 493/500, 2.5 task/s, elapsed: 194s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 494/500, 2.5 task/s, elapsed: 194s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 495/500, 2.5 task/s, elapsed: 195s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 496/500, 2.5 task/s, elapsed: 195s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 497/500, 2.5 task/s, elapsed: 196s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 498/500, 2.5 task/s, elapsed: 196s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 499/500, 2.5 task/s, elapsed: 196s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 500/500, 2.5 task/s, elapsed: 197s, ETA:     0s2022-04-18 20:52:26,721 - mmseg - INFO - per class results:
2022-04-18 20:52:26,723 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|      road     | 37.66 | 100.0 |
|    sidewalk   |  0.0  |  0.0  |
|    building   |  0.0  |  0.0  |
|      wall     |  0.0  |  0.0  |
|     fence     |  0.0  |  0.0  |
|      pole     |  0.0  |  0.0  |
| traffic light |  0.0  |  0.0  |
|  traffic sign |  0.0  |  0.0  |
|   vegetation  |  0.0  |  0.0  |
|    terrain    |  0.0  |  0.0  |
|      sky      |  0.0  |  0.0  |
|     person    |  0.0  |  0.0  |
|     rider     |  0.0  |  0.0  |
|      car      |  0.0  |  0.0  |
|     truck     |  0.0  |  0.0  |
|      bus      |  0.0  |  0.0  |
|     train     |  0.0  |  0.0  |
|   motorcycle  |  0.0  |  0.0  |
|    bicycle    |  0.0  |  0.0  |
+---------------+-------+-------+
2022-04-18 20:52:26,723 - mmseg - INFO - Summary:
2022-04-18 20:52:26,724 - mmseg - INFO - 
+-------+------+------+
|  aAcc | mIoU | mAcc |
+-------+------+------+
| 37.66 | 1.98 | 5.26 |
+-------+------+------+
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:52:26,733 - mmseg - INFO - Exp name: 220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02
2022-04-18 20:52:26,733 - mmseg - INFO - Iter [500/40000]	lr: 4.800e-05, eta: 1 day, 6:06:44, time: 3.351, data_time: 0.018, memory: 9784, aAcc: 0.3766, mIoU: 0.0198, mAcc: 0.0526, IoU.road: 0.3766, IoU.sidewalk: 0.0000, IoU.building: 0.0000, IoU.wall: 0.0000, IoU.fence: 0.0000, IoU.pole: 0.0000, IoU.traffic light: 0.0000, IoU.traffic sign: 0.0000, IoU.vegetation: 0.0000, IoU.terrain: 0.0000, IoU.sky: 0.0000, IoU.person: 0.0000, IoU.rider: 0.0000, IoU.car: 0.0000, IoU.truck: 0.0000, IoU.bus: 0.0000, IoU.train: 0.0000, IoU.motorcycle: 0.0000, IoU.bicycle: 0.0000, Acc.road: 1.0000, Acc.sidewalk: 0.0000, Acc.building: 0.0000, Acc.wall: 0.0000, Acc.fence: 0.0000, Acc.pole: 0.0000, Acc.traffic light: 0.0000, Acc.traffic sign: 0.0000, Acc.vegetation: 0.0000, Acc.terrain: 0.0000, Acc.sky: 0.0000, Acc.person: 0.0000, Acc.rider: 0.0000, Acc.car: 0.0000, Acc.truck: 0.0000, Acc.bus: 0.0000, Acc.train: 0.0000, Acc.motorcycle: 0.0000, Acc.bicycle: 0.0000, decode.loss_seg: nan, decode.acc_seg: 20.8114, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.7681
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:52:44,803 - mmseg - INFO - Iter [17700/40000]	lr: 3.345e-05, eta: 9:33:30, time: 1.550, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1271, decode.acc_seg: 89.4981, src.loss_imnet_feat_dist: 0.1055, mix.decode.loss_seg: 0.1332, mix.decode.acc_seg: 89.5004
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:54:05,204 - mmseg - INFO - Iter [17750/40000]	lr: 3.338e-05, eta: 9:32:17, time: 1.608, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1263, decode.acc_seg: 91.1075, src.loss_imnet_feat_dist: 0.1055, mix.decode.loss_seg: 0.1580, mix.decode.acc_seg: 89.4298
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:55:22,755 - mmseg - INFO - Iter [17800/40000]	lr: 3.330e-05, eta: 9:31:00, time: 1.551, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1350, decode.acc_seg: 90.8328, src.loss_imnet_feat_dist: 0.1061, mix.decode.loss_seg: 0.1513, mix.decode.acc_seg: 89.2917
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:56:41,471 - mmseg - INFO - Iter [17850/40000]	lr: 3.323e-05, eta: 9:29:45, time: 1.574, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1216, decode.acc_seg: 89.7799, src.loss_imnet_feat_dist: 0.1098, mix.decode.loss_seg: 0.1609, mix.decode.acc_seg: 89.1644
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:57:59,697 - mmseg - INFO - Iter [17900/40000]	lr: 3.315e-05, eta: 9:28:29, time: 1.565, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1193, decode.acc_seg: 91.1343, src.loss_imnet_feat_dist: 0.1066, mix.decode.loss_seg: 0.1356, mix.decode.acc_seg: 90.0708
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 20:59:18,461 - mmseg - INFO - Iter [17950/40000]	lr: 3.308e-05, eta: 9:27:14, time: 1.575, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1216, decode.acc_seg: 91.1070, src.loss_imnet_feat_dist: 0.1088, mix.decode.loss_seg: 0.1418, mix.decode.acc_seg: 89.2832
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:00:34,786 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-18 21:00:34,787 - mmseg - INFO - Iter [18000/40000]	lr: 3.300e-05, eta: 9:25:56, time: 1.527, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1204, decode.acc_seg: 90.7658, src.loss_imnet_feat_dist: 0.1068, mix.decode.loss_seg: 0.1342, mix.decode.acc_seg: 90.5380
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:01:50,282 - mmseg - INFO - Iter [18050/40000]	lr: 3.293e-05, eta: 9:24:37, time: 1.510, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1104, decode.acc_seg: 91.2208, src.loss_imnet_feat_dist: 0.1009, mix.decode.loss_seg: 0.1309, mix.decode.acc_seg: 89.8278
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:03:07,706 - mmseg - INFO - Iter [18100/40000]	lr: 3.285e-05, eta: 9:23:20, time: 1.548, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1255, decode.acc_seg: 90.4497, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1335, mix.decode.acc_seg: 89.1168
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:04:25,961 - mmseg - INFO - Iter [18150/40000]	lr: 3.278e-05, eta: 9:22:04, time: 1.565, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1239, decode.acc_seg: 91.1532, src.loss_imnet_feat_dist: 0.1056, mix.decode.loss_seg: 0.1310, mix.decode.acc_seg: 89.7653
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:05:42,741 - mmseg - INFO - Iter [18200/40000]	lr: 3.270e-05, eta: 9:20:46, time: 1.536, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1329, decode.acc_seg: 89.9391, src.loss_imnet_feat_dist: 0.1020, mix.decode.loss_seg: 0.1563, mix.decode.acc_seg: 87.9991
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:06:26,120 - mmseg - INFO - Iter [8250/40000]	lr: 4.763e-05, eta: 1 day, 6:07:36, time: 3.369, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.4950, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.6113
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:09:14,993 - mmseg - INFO - Iter [8300/40000]	lr: 4.755e-05, eta: 1 day, 6:04:38, time: 3.377, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 25.4884, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.1064
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:12:03,439 - mmseg - INFO - Iter [8350/40000]	lr: 4.748e-05, eta: 1 day, 6:01:38, time: 3.369, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.4086, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.3287
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:13:30,170 - mmseg - INFO - Iter [18500/40000]	lr: 3.225e-05, eta: 9:13:08, time: 1.559, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1232, decode.acc_seg: 90.2619, src.loss_imnet_feat_dist: 0.1063, mix.decode.loss_seg: 0.1299, mix.decode.acc_seg: 89.1301
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:14:44,752 - mmseg - INFO - Iter [18550/40000]	lr: 3.218e-05, eta: 9:11:48, time: 1.492, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1097, decode.acc_seg: 91.1980, src.loss_imnet_feat_dist: 0.1046, mix.decode.loss_seg: 0.1286, mix.decode.acc_seg: 89.4123
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:15:57,950 - mmseg - INFO - Iter [18600/40000]	lr: 3.210e-05, eta: 9:10:26, time: 1.464, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1181, decode.acc_seg: 90.4321, src.loss_imnet_feat_dist: 0.1115, mix.decode.loss_seg: 0.1364, mix.decode.acc_seg: 89.1076
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:17:17,787 - mmseg - INFO - Iter [18650/40000]	lr: 3.203e-05, eta: 9:09:12, time: 1.597, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1289, decode.acc_seg: 90.3734, src.loss_imnet_feat_dist: 0.1038, mix.decode.loss_seg: 0.1459, mix.decode.acc_seg: 88.5480
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:18:33,227 - mmseg - INFO - Iter [18700/40000]	lr: 3.195e-05, eta: 9:07:53, time: 1.509, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1145, decode.acc_seg: 91.0343, src.loss_imnet_feat_dist: 0.1050, mix.decode.loss_seg: 0.1270, mix.decode.acc_seg: 90.1251
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:19:51,398 - mmseg - INFO - Iter [18750/40000]	lr: 3.188e-05, eta: 9:06:37, time: 1.563, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1179, decode.acc_seg: 90.8384, src.loss_imnet_feat_dist: 0.1012, mix.decode.loss_seg: 0.1411, mix.decode.acc_seg: 89.8078
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:21:03,324 - mmseg - INFO - Iter [18800/40000]	lr: 3.180e-05, eta: 9:05:14, time: 1.439, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1138, decode.acc_seg: 91.3430, src.loss_imnet_feat_dist: 0.1069, mix.decode.loss_seg: 0.1407, mix.decode.acc_seg: 89.2473
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:22:19,535 - mmseg - INFO - Iter [18850/40000]	lr: 3.173e-05, eta: 9:03:56, time: 1.524, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1084, decode.acc_seg: 91.6066, src.loss_imnet_feat_dist: 0.1094, mix.decode.loss_seg: 0.1278, mix.decode.acc_seg: 89.6146
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:23:35,345 - mmseg - INFO - Iter [18900/40000]	lr: 3.165e-05, eta: 9:02:37, time: 1.516, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1335, decode.acc_seg: 88.9093, src.loss_imnet_feat_dist: 0.1057, mix.decode.loss_seg: 0.1427, mix.decode.acc_seg: 87.7543
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:24:51,514 - mmseg - INFO - Iter [18950/40000]	lr: 3.158e-05, eta: 9:01:19, time: 1.523, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1122, decode.acc_seg: 91.1998, src.loss_imnet_feat_dist: 0.1057, mix.decode.loss_seg: 0.1246, mix.decode.acc_seg: 89.9116
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:26:08,450 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-18 21:26:08,450 - mmseg - INFO - Iter [19000/40000]	lr: 3.150e-05, eta: 9:00:02, time: 1.539, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1102, decode.acc_seg: 90.4748, src.loss_imnet_feat_dist: 0.1048, mix.decode.loss_seg: 0.1417, mix.decode.acc_seg: 88.8539
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:27:27,885 - mmseg - INFO - Iter [19050/40000]	lr: 3.143e-05, eta: 8:58:47, time: 1.589, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1192, decode.acc_seg: 90.5930, src.loss_imnet_feat_dist: 0.1037, mix.decode.loss_seg: 0.1472, mix.decode.acc_seg: 88.8929
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:28:45,322 - mmseg - INFO - Iter [19100/40000]	lr: 3.135e-05, eta: 8:57:30, time: 1.549, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1249, decode.acc_seg: 90.5987, src.loss_imnet_feat_dist: 0.1059, mix.decode.loss_seg: 0.1442, mix.decode.acc_seg: 88.3190
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:30:02,130 - mmseg - INFO - Iter [19150/40000]	lr: 3.128e-05, eta: 8:56:13, time: 1.536, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1275, decode.acc_seg: 89.7080, src.loss_imnet_feat_dist: 0.1080, mix.decode.loss_seg: 0.1399, mix.decode.acc_seg: 87.7597
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:31:14,614 - mmseg - INFO - Iter [19200/40000]	lr: 3.120e-05, eta: 8:54:50, time: 1.450, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1196, decode.acc_seg: 90.9958, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1373, mix.decode.acc_seg: 88.5619
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:32:27,414 - mmseg - INFO - Iter [19250/40000]	lr: 3.113e-05, eta: 8:53:28, time: 1.456, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1177, decode.acc_seg: 90.4582, src.loss_imnet_feat_dist: 0.1059, mix.decode.loss_seg: 0.1405, mix.decode.acc_seg: 89.8041
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:33:44,042 - mmseg - INFO - Iter [19300/40000]	lr: 3.105e-05, eta: 8:52:11, time: 1.533, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1175, decode.acc_seg: 90.4876, src.loss_imnet_feat_dist: 0.1075, mix.decode.loss_seg: 0.1331, mix.decode.acc_seg: 90.1446
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:34:25,731 - mmseg - INFO - Iter [8750/40000]	lr: 4.688e-05, eta: 1 day, 5:37:27, time: 3.389, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.1092, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.6616
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:37:12,387 - mmseg - INFO - Iter [8800/40000]	lr: 4.680e-05, eta: 1 day, 5:34:22, time: 3.333, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.5213, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.7480
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:40:02,164 - mmseg - INFO - Iter [19550/40000]	lr: 3.068e-05, eta: 8:45:37, time: 1.470, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1191, decode.acc_seg: 90.2949, src.loss_imnet_feat_dist: 0.1078, mix.decode.loss_seg: 0.1344, mix.decode.acc_seg: 89.7020
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:41:19,420 - mmseg - INFO - Iter [19600/40000]	lr: 3.060e-05, eta: 8:44:20, time: 1.545, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1184, decode.acc_seg: 90.6958, src.loss_imnet_feat_dist: 0.1039, mix.decode.loss_seg: 0.1314, mix.decode.acc_seg: 89.5818
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:42:38,309 - mmseg - INFO - Iter [19650/40000]	lr: 3.053e-05, eta: 8:43:05, time: 1.578, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1092, decode.acc_seg: 91.3331, src.loss_imnet_feat_dist: 0.1079, mix.decode.loss_seg: 0.1354, mix.decode.acc_seg: 88.9261
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:43:48,561 - mmseg - INFO - Iter [19700/40000]	lr: 3.045e-05, eta: 8:41:41, time: 1.405, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1126, decode.acc_seg: 91.3248, src.loss_imnet_feat_dist: 0.1023, mix.decode.loss_seg: 0.1249, mix.decode.acc_seg: 89.2204
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:45:00,626 - mmseg - INFO - Iter [19750/40000]	lr: 3.038e-05, eta: 8:40:19, time: 1.441, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1177, decode.acc_seg: 90.4869, src.loss_imnet_feat_dist: 0.1057, mix.decode.loss_seg: 0.1317, mix.decode.acc_seg: 89.1401
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:46:14,313 - mmseg - INFO - Iter [19800/40000]	lr: 3.030e-05, eta: 8:38:58, time: 1.474, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1163, decode.acc_seg: 90.4567, src.loss_imnet_feat_dist: 0.1017, mix.decode.loss_seg: 0.1541, mix.decode.acc_seg: 86.6032
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:47:31,077 - mmseg - INFO - Iter [19850/40000]	lr: 3.023e-05, eta: 8:37:41, time: 1.535, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1272, decode.acc_seg: 90.9984, src.loss_imnet_feat_dist: 0.1021, mix.decode.loss_seg: 0.1408, mix.decode.acc_seg: 87.4246
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:48:48,107 - mmseg - INFO - Iter [19900/40000]	lr: 3.015e-05, eta: 8:36:24, time: 1.541, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1114, decode.acc_seg: 90.9855, src.loss_imnet_feat_dist: 0.1051, mix.decode.loss_seg: 0.1226, mix.decode.acc_seg: 89.4179
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:50:07,094 - mmseg - INFO - Iter [19950/40000]	lr: 3.008e-05, eta: 8:35:08, time: 1.580, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1201, decode.acc_seg: 89.9006, src.loss_imnet_feat_dist: 0.1038, mix.decode.loss_seg: 0.1336, mix.decode.acc_seg: 88.1640
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [                                                  ] 0/500, elapsed: 0s, ETA:
[                                 ] 1/500, 1.5 task/s, elapsed: 1s, ETA:   324s
[                                 ] 2/500, 2.5 task/s, elapsed: 1s, ETA:   202s
[                                 ] 3/500, 3.2 task/s, elapsed: 1s, ETA:   157s
[                                 ] 4/500, 3.7 task/s, elapsed: 1s, ETA:   135s
[                                 ] 5/500, 4.1 task/s, elapsed: 1s, ETA:   122s
[                                 ] 6/500, 4.3 task/s, elapsed: 1s, ETA:   115s
[                                 ] 7/500, 4.5 task/s, elapsed: 2s, ETA:   109s
[                                 ] 8/500, 4.7 task/s, elapsed: 2s, ETA:   105s
[                                 ] 9/500, 4.9 task/s, elapsed: 2s, ETA:   101s
[                                ] 10/500, 5.0 task/s, elapsed: 2s, ETA:    98s
[                                ] 11/500, 5.2 task/s, elapsed: 2s, ETA:    95s
[                                ] 12/500, 5.3 task/s, elapsed: 2s, ETA:    93s
[                                ] 13/500, 5.4 task/s, elapsed: 2s, ETA:    91s
[                                ] 14/500, 5.5 task/s, elapsed: 3s, ETA:    89s
[                                ] 15/500, 5.5 task/s, elapsed: 3s, ETA:    88s
[>                               ] 16/500, 5.6 task/s, elapsed: 3s, ETA:    86s
[>                               ] 17/500, 5.7 task/s, elapsed: 3s, ETA:    85s
[>                               ] 18/500, 5.7 task/s, elapsed: 3s, ETA:    84s
[>                               ] 19/500, 5.8 task/s, elapsed: 3s, ETA:    83s
[>                               ] 20/500, 5.8 task/s, elapsed: 3s, ETA:    82s
[>                               ] 21/500, 5.9 task/s, elapsed: 4s, ETA:    82s
[>                               ] 22/500, 5.9 task/s, elapsed: 4s, ETA:    81s
[>                               ] 23/500, 5.9 task/s, elapsed: 4s, ETA:    80s
[>                               ] 24/500, 6.0 task/s, elapsed: 4s, ETA:    80s
[>                               ] 25/500, 6.0 task/s, elapsed: 4s, ETA:    79s
[>                               ] 26/500, 6.0 task/s, elapsed: 4s, ETA:    78s
[>                               ] 27/500, 6.1 task/s, elapsed: 4s, ETA:    78s
[>                               ] 28/500, 6.1 task/s, elapsed: 5s, ETA:    77s
[>                               ] 29/500, 6.1 task/s, elapsed: 5s, ETA:    77s
[>                               ] 30/500, 6.2 task/s, elapsed: 5s, ETA:    76s
[>                               ] 31/500, 6.2 task/s, elapsed: 5s, ETA:    76s
[>>                              ] 32/500, 6.2 task/s, elapsed: 5s, ETA:    76s
[>>                              ] 33/500, 6.2 task/s, elapsed: 5s, ETA:    76s
[>>                              ] 34/500, 6.2 task/s, elapsed: 6s, ETA:    76s
[>>                              ] 35/500, 6.2 task/s, elapsed: 6s, ETA:    75s
[>>                              ] 36/500, 6.2 task/s, elapsed: 6s, ETA:    75s
[>>                              ] 37/500, 6.2 task/s, elapsed: 6s, ETA:    75s
[>>                              ] 38/500, 6.2 task/s, elapsed: 6s, ETA:    74s
[>>                              ] 39/500, 6.2 task/s, elapsed: 6s, ETA:    74s
[>>                              ] 40/500, 6.2 task/s, elapsed: 6s, ETA:    74s
[>>                              ] 41/500, 6.3 task/s, elapsed: 7s, ETA:    73s
[>>                              ] 42/500, 6.2 task/s, elapsed: 7s, ETA:    73s
[>>                              ] 43/500, 6.3 task/s, elapsed: 7s, ETA:    73s
[>>                              ] 44/500, 6.3 task/s, elapsed: 7s, ETA:    73s
[>>                              ] 45/500, 6.3 task/s, elapsed: 7s, ETA:    72s
[>>                              ] 46/500, 6.3 task/s, elapsed: 7s, ETA:    72s
[>>>                             ] 47/500, 6.3 task/s, elapsed: 7s, ETA:    72s
[>>>                             ] 48/500, 6.3 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 49/500, 6.3 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 50/500, 6.3 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 51/500, 6.3 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 52/500, 6.4 task/s, elapsed: 8s, ETA:    70s
[>>>                             ] 53/500, 6.4 task/s, elapsed: 8s, ETA:    70s
[>>>                             ] 54/500, 6.4 task/s, elapsed: 8s, ETA:    70s
[>>>                             ] 55/500, 6.4 task/s, elapsed: 9s, ETA:    70s
[>>>                             ] 56/500, 6.4 task/s, elapsed: 9s, ETA:    69s
[>>>                             ] 57/500, 6.4 task/s, elapsed: 9s, ETA:    69s
[>>>                             ] 58/500, 6.4 task/s, elapsed: 9s, ETA:    69s
[>>>                             ] 59/500, 6.4 task/s, elapsed: 9s, ETA:    69s
[>>>                             ] 60/500, 6.4 task/s, elapsed: 9s, ETA:    69s
[>>>                            ] 61/500, 6.4 task/s, elapsed: 10s, ETA:    69s
[>>>                            ] 62/500, 6.4 task/s, elapsed: 10s, ETA:    68s
[>>>                            ] 63/500, 6.4 task/s, elapsed: 10s, ETA:    68s
[>>>                            ] 64/500, 6.4 task/s, elapsed: 10s, ETA:    68s
[>>>>                           ] 65/500, 6.4 task/s, elapsed: 10s, ETA:    68s
[>>>>                           ] 66/500, 6.4 task/s, elapsed: 10s, ETA:    68s
[>>>>                           ] 67/500, 6.4 task/s, elapsed: 10s, ETA:    68s
[>>>>                           ] 68/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 69/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 70/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 71/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 72/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 73/500, 6.4 task/s, elapsed: 11s, ETA:    66s
[>>>>                           ] 74/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 75/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 76/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 77/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 78/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 79/500, 6.4 task/s, elapsed: 12s, ETA:    65s
[>>>>                           ] 80/500, 6.4 task/s, elapsed: 12s, ETA:    65s
[>>>>>                          ] 81/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 82/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 83/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 84/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 85/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 86/500, 6.4 task/s, elapsed: 13s, ETA:    64s
[>>>>>                          ] 87/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 88/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 89/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 90/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 91/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 92/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 93/500, 6.4 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 94/500, 6.4 task/s, elapsed: 15s, ETA:    63s
[>>>>>                          ] 95/500, 6.4 task/s, elapsed: 15s, ETA:    63s
[>>>>>                          ] 96/500, 6.4 task/s, elapsed: 15s, ETA:    63s
[>>>>>>                         ] 97/500, 6.4 task/s, elapsed: 15s, ETA:    63s
[>>>>>>                         ] 98/500, 6.4 task/s, elapsed: 15s, ETA:    63s
[>>>>>>                         ] 99/500, 6.4 task/s, elapsed: 15s, ETA:    62s
[>>>>>>                        ] 100/500, 6.4 task/s, elapsed: 16s, ETA:    62s
[>>>>>>                        ] 101/500, 6.4 task/s, elapsed: 16s, ETA:    62s
[>>>>>>                        ] 102/500, 6.4 task/s, elapsed: 16s, ETA:    62s
[>>>>>>                        ] 103/500, 6.4 task/s, elapsed: 16s, ETA:    62s
[>>>>>>                        ] 104/500, 6.4 task/s, elapsed: 16s, ETA:    62s
[>>>>>>                        ] 105/500, 6.4 task/s, elapsed: 16s, ETA:    61s
[>>>>>>                        ] 106/500, 6.4 task/s, elapsed: 17s, ETA:    61s
[>>>>>>                        ] 107/500, 6.4 task/s, elapsed: 17s, ETA:    61s
[>>>>>>                        ] 108/500, 6.4 task/s, elapsed: 17s, ETA:    61s
[>>>>>>                        ] 109/500, 6.4 task/s, elapsed: 17s, ETA:    61s
[>>>>>>                        ] 110/500, 6.4 task/s, elapsed: 17s, ETA:    61s
[>>>>>>                        ] 111/500, 6.4 task/s, elapsed: 17s, ETA:    61s
[>>>>>>                        ] 112/500, 6.4 task/s, elapsed: 17s, ETA:    60s
[>>>>>>                        ] 113/500, 6.4 task/s, elapsed: 18s, ETA:    60s
[>>>>>>                        ] 114/500, 6.4 task/s, elapsed: 18s, ETA:    60s
[>>>>>>                        ] 115/500, 6.4 task/s, elapsed: 18s, ETA:    60s
[>>>>>>                        ] 116/500, 6.4 task/s, elapsed: 18s, ETA:    60s
[>>>>>>>                       ] 117/500, 6.4 task/s, elapsed: 18s, ETA:    60s
[>>>>>>>                       ] 118/500, 6.4 task/s, elapsed: 18s, ETA:    59s
[>>>>>>>                       ] 119/500, 6.4 task/s, elapsed: 19s, ETA:    59s
[>>>>>>>                       ] 120/500, 6.4 task/s, elapsed: 19s, ETA:    59s
[>>>>>>>                       ] 121/500, 6.4 task/s, elapsed: 19s, ETA:    59s
[>>>>>>>                       ] 122/500, 6.4 task/s, elapsed: 19s, ETA:    59s
[>>>>>>>                       ] 123/500, 6.4 task/s, elapsed: 19s, ETA:    59s
[>>>>>>>                       ] 124/500, 6.4 task/s, elapsed: 19s, ETA:    59s
[>>>>>>>                       ] 125/500, 6.4 task/s, elapsed: 19s, ETA:    58s
[>>>>>>>                       ] 126/500, 6.4 task/s, elapsed: 20s, ETA:    58s
[>>>>>>>                       ] 127/500, 6.4 task/s, elapsed: 20s, ETA:    58s
[>>>>>>>                       ] 128/500, 6.4 task/s, elapsed: 20s, ETA:    58s
[>>>>>>>                       ] 129/500, 6.4 task/s, elapsed: 20s, ETA:    58s
[>>>>>>>                       ] 130/500, 6.4 task/s, elapsed: 20s, ETA:    58s
[>>>>>>>                       ] 131/500, 6.4 task/s, elapsed: 20s, ETA:    57s
[>>>>>>>                       ] 132/500, 6.4 task/s, elapsed: 21s, ETA:    57s
[>>>>>>>                       ] 133/500, 6.4 task/s, elapsed: 21s, ETA:    57s
[>>>>>>>>                      ] 134/500, 6.4 task/s, elapsed: 21s, ETA:    57s
[>>>>>>>>                      ] 135/500, 6.4 task/s, elapsed: 21s, ETA:    57s
[>>>>>>>>                      ] 136/500, 6.4 task/s, elapsed: 21s, ETA:    57s
[>>>>>>>>                      ] 137/500, 6.4 task/s, elapsed: 21s, ETA:    57s
[>>>>>>>>                      ] 138/500, 6.4 task/s, elapsed: 21s, ETA:    56s
[>>>>>>>>                      ] 139/500, 6.4 task/s, elapsed: 22s, ETA:    56s
[>>>>>>>>                      ] 140/500, 6.4 task/s, elapsed: 22s, ETA:    56s
[>>>>>>>>                      ] 141/500, 6.4 task/s, elapsed: 22s, ETA:    56s
[>>>>>>>>                      ] 142/500, 6.4 task/s, elapsed: 22s, ETA:    56s
[>>>>>>>>                      ] 143/500, 6.4 task/s, elapsed: 22s, ETA:    56s
[>>>>>>>>                      ] 144/500, 6.4 task/s, elapsed: 22s, ETA:    55s
[>>>>>>>>                      ] 145/500, 6.4 task/s, elapsed: 23s, ETA:    55s
[>>>>>>>>                      ] 146/500, 6.4 task/s, elapsed: 23s, ETA:    55s
[>>>>>>>>                      ] 147/500, 6.4 task/s, elapsed: 23s, ETA:    55s
[>>>>>>>>                      ] 148/500, 6.4 task/s, elapsed: 23s, ETA:    55s
[>>>>>>>>                      ] 149/500, 6.4 task/s, elapsed: 23s, ETA:    55s
[>>>>>>>>>                     ] 150/500, 6.4 task/s, elapsed: 23s, ETA:    55s
[>>>>>>>>>                     ] 151/500, 6.4 task/s, elapsed: 24s, ETA:    54s
[>>>>>>>>>                     ] 152/500, 6.4 task/s, elapsed: 24s, ETA:    54s
[>>>>>>>>>                     ] 153/500, 6.4 task/s, elapsed: 24s, ETA:    54s
[>>>>>>>>>                     ] 154/500, 6.4 task/s, elapsed: 24s, ETA:    54s
[>>>>>>>>>                     ] 155/500, 6.4 task/s, elapsed: 24s, ETA:    54s
[>>>>>>>>>                     ] 156/500, 6.4 task/s, elapsed: 24s, ETA:    54s
[>>>>>>>>>                     ] 157/500, 6.4 task/s, elapsed: 24s, ETA:    53s
[>>>>>>>>>                     ] 158/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 159/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 160/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 161/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 162/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 163/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 164/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>                     ] 165/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>                     ] 166/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>>                    ] 167/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>>                    ] 168/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>>                    ] 169/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>>                    ] 170/500, 6.4 task/s, elapsed: 26s, ETA:    51s
[>>>>>>>>>>                    ] 171/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 172/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 173/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 174/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 175/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 176/500, 6.4 task/s, elapsed: 27s, ETA:    50s
[>>>>>>>>>>                    ] 177/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 178/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 179/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 180/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 181/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 182/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 183/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 184/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 185/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 186/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 187/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 188/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 189/500, 6.4 task/s, elapsed: 29s, ETA:    48s
[>>>>>>>>>>>                   ] 190/500, 6.4 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 191/500, 6.4 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 192/500, 6.4 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 193/500, 6.4 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 194/500, 6.4 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 195/500, 6.4 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 196/500, 6.4 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>                   ] 197/500, 6.4 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>                   ] 198/500, 6.4 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>                   ] 199/500, 6.4 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>>                  ] 200/500, 6.4 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>>                  ] 201/500, 6.4 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>>                  ] 202/500, 6.4 task/s, elapsed: 31s, ETA:    46s
[>>>>>>>>>>>>                  ] 203/500, 6.4 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 204/500, 6.4 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 205/500, 6.4 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 206/500, 6.4 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 207/500, 6.4 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 208/500, 6.4 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 209/500, 6.4 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 210/500, 6.4 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 211/500, 6.4 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 212/500, 6.4 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 213/500, 6.4 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 214/500, 6.4 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 215/500, 6.4 task/s, elapsed: 33s, ETA:    44s
[>>>>>>>>>>>>                  ] 216/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 217/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 218/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 219/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 220/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 221/500, 6.4 task/s, elapsed: 34s, ETA:    43s
[>>>>>>>>>>>>>                 ] 222/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 223/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 224/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 225/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 226/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 227/500, 6.4 task/s, elapsed: 35s, ETA:    42s
[>>>>>>>>>>>>>                 ] 228/500, 6.4 task/s, elapsed: 35s, ETA:    42s
[>>>>>>>>>>>>>                 ] 229/500, 6.4 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>                 ] 230/500, 6.4 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>                 ] 231/500, 6.4 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>                 ] 232/500, 6.4 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>                 ] 233/500, 6.4 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>>                ] 234/500, 6.4 task/s, elapsed: 36s, ETA:    41s
[>>>>>>>>>>>>>>                ] 235/500, 6.4 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 236/500, 6.4 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 237/500, 6.4 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 238/500, 6.4 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 239/500, 6.4 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 240/500, 6.4 task/s, elapsed: 37s, ETA:    40s
[>>>>>>>>>>>>>>                ] 241/500, 6.4 task/s, elapsed: 37s, ETA:    40s
[>>>>>>>>>>>>>>                ] 242/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 243/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 244/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 245/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 246/500, 6.4 task/s, elapsed: 38s, ETA:    39s
[>>>>>>>>>>>>>>                ] 247/500, 6.4 task/s, elapsed: 38s, ETA:    39s
[>>>>>>>>>>>>>>                ] 248/500, 6.4 task/s, elapsed: 38s, ETA:    39s
[>>>>>>>>>>>>>>                ] 249/500, 6.4 task/s, elapsed: 39s, ETA:    39s
[>>>>>>>>>>>>>>>               ] 250/500, 6.4 task/s, elapsed: 39s, ETA:    39s
[>>>>>>>>>>>>>>>               ] 251/500, 6.4 task/s, elapsed: 39s, ETA:    39s
[>>>>>>>>>>>>>>>               ] 252/500, 6.5 task/s, elapsed: 39s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 253/500, 6.5 task/s, elapsed: 39s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 254/500, 6.5 task/s, elapsed: 39s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 255/500, 6.5 task/s, elapsed: 40s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 256/500, 6.5 task/s, elapsed: 40s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 257/500, 6.5 task/s, elapsed: 40s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 258/500, 6.5 task/s, elapsed: 40s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 259/500, 6.5 task/s, elapsed: 40s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 260/500, 6.5 task/s, elapsed: 40s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 261/500, 6.5 task/s, elapsed: 40s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 262/500, 6.5 task/s, elapsed: 41s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 263/500, 6.5 task/s, elapsed: 41s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 264/500, 6.5 task/s, elapsed: 41s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 265/500, 6.5 task/s, elapsed: 41s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 266/500, 6.5 task/s, elapsed: 41s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 267/500, 6.5 task/s, elapsed: 41s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 268/500, 6.5 task/s, elapsed: 41s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 269/500, 6.5 task/s, elapsed: 42s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 270/500, 6.5 task/s, elapsed: 42s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 271/500, 6.5 task/s, elapsed: 42s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 272/500, 6.5 task/s, elapsed: 42s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 273/500, 6.5 task/s, elapsed: 42s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 274/500, 6.5 task/s, elapsed: 42s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 275/500, 6.5 task/s, elapsed: 42s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 276/500, 6.5 task/s, elapsed: 43s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 277/500, 6.5 task/s, elapsed: 43s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 278/500, 6.5 task/s, elapsed: 43s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 279/500, 6.5 task/s, elapsed: 43s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 280/500, 6.5 task/s, elapsed: 43s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 281/500, 6.5 task/s, elapsed: 43s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 282/500, 6.5 task/s, elapsed: 43s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 283/500, 6.5 task/s, elapsed: 44s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 284/500, 6.5 task/s, elapsed: 44s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 285/500, 6.5 task/s, elapsed: 44s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 286/500, 6.5 task/s, elapsed: 44s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 287/500, 6.5 task/s, elapsed: 44s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 288/500, 6.5 task/s, elapsed: 44s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 289/500, 6.5 task/s, elapsed: 44s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 290/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 291/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 292/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 293/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 294/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 295/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 296/500, 6.5 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 297/500, 6.5 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 298/500, 6.5 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 299/500, 6.5 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>>            ] 300/500, 6.5 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>>            ] 301/500, 6.5 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>>            ] 302/500, 6.5 task/s, elapsed: 46s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 303/500, 6.5 task/s, elapsed: 47s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 304/500, 6.5 task/s, elapsed: 47s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 305/500, 6.5 task/s, elapsed: 47s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 306/500, 6.5 task/s, elapsed: 47s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 307/500, 6.5 task/s, elapsed: 47s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 308/500, 6.5 task/s, elapsed: 47s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 309/500, 6.5 task/s, elapsed: 48s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 310/500, 6.5 task/s, elapsed: 48s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 311/500, 6.5 task/s, elapsed: 48s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 312/500, 6.5 task/s, elapsed: 48s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 313/500, 6.5 task/s, elapsed: 48s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 314/500, 6.5 task/s, elapsed: 48s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 315/500, 6.5 task/s, elapsed: 48s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 316/500, 6.5 task/s, elapsed: 49s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 317/500, 6.5 task/s, elapsed: 49s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 318/500, 6.5 task/s, elapsed: 49s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 319/500, 6.5 task/s, elapsed: 49s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 320/500, 6.5 task/s, elapsed: 49s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 321/500, 6.5 task/s, elapsed: 49s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 322/500, 6.5 task/s, elapsed: 50s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 323/500, 6.5 task/s, elapsed: 50s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 324/500, 6.5 task/s, elapsed: 50s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 325/500, 6.5 task/s, elapsed: 50s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 326/500, 6.5 task/s, elapsed: 50s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 327/500, 6.5 task/s, elapsed: 50s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 328/500, 6.5 task/s, elapsed: 50s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 329/500, 6.5 task/s, elapsed: 51s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 330/500, 6.5 task/s, elapsed: 51s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 331/500, 6.5 task/s, elapsed: 51s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 332/500, 6.5 task/s, elapsed: 51s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 333/500, 6.5 task/s, elapsed: 51s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>          ] 334/500, 6.5 task/s, elapsed: 51s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>          ] 335/500, 6.5 task/s, elapsed: 51s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 336/500, 6.5 task/s, elapsed: 52s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 337/500, 6.5 task/s, elapsed: 52s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 338/500, 6.5 task/s, elapsed: 52s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 339/500, 6.5 task/s, elapsed: 52s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 340/500, 6.5 task/s, elapsed: 52s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 341/500, 6.5 task/s, elapsed: 52s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 342/500, 6.5 task/s, elapsed: 53s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 343/500, 6.5 task/s, elapsed: 53s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 344/500, 6.5 task/s, elapsed: 53s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 345/500, 6.5 task/s, elapsed: 53s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 346/500, 6.5 task/s, elapsed: 53s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 347/500, 6.5 task/s, elapsed: 53s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 348/500, 6.5 task/s, elapsed: 53s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 349/500, 6.5 task/s, elapsed: 53s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>         ] 350/500, 6.5 task/s, elapsed: 54s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>         ] 351/500, 6.5 task/s, elapsed: 54s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>         ] 352/500, 6.5 task/s, elapsed: 54s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>         ] 353/500, 6.5 task/s, elapsed: 54s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>         ] 354/500, 6.5 task/s, elapsed: 54s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 355/500, 6.5 task/s, elapsed: 54s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 356/500, 6.5 task/s, elapsed: 54s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 357/500, 6.5 task/s, elapsed: 55s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 358/500, 6.5 task/s, elapsed: 55s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 359/500, 6.5 task/s, elapsed: 55s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 360/500, 6.5 task/s, elapsed: 55s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 361/500, 6.5 task/s, elapsed: 55s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 362/500, 6.5 task/s, elapsed: 55s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 363/500, 6.5 task/s, elapsed: 56s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 364/500, 6.5 task/s, elapsed: 56s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 365/500, 6.5 task/s, elapsed: 56s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 366/500, 6.5 task/s, elapsed: 56s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 367/500, 6.5 task/s, elapsed: 56s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 368/500, 6.5 task/s, elapsed: 56s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 369/500, 6.5 task/s, elapsed: 56s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 370/500, 6.5 task/s, elapsed: 57s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 371/500, 6.5 task/s, elapsed: 57s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 372/500, 6.5 task/s, elapsed: 57s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 373/500, 6.5 task/s, elapsed: 57s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 374/500, 6.5 task/s, elapsed: 57s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 375/500, 6.6 task/s, elapsed: 57s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 376/500, 6.6 task/s, elapsed: 57s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 377/500, 6.6 task/s, elapsed: 58s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 378/500, 6.6 task/s, elapsed: 58s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 379/500, 6.6 task/s, elapsed: 58s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 380/500, 6.6 task/s, elapsed: 58s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 381/500, 6.6 task/s, elapsed: 58s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 382/500, 6.6 task/s, elapsed: 58s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 383/500, 6.6 task/s, elapsed: 58s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 384/500, 6.6 task/s, elapsed: 59s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 385/500, 6.6 task/s, elapsed: 59s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 386/500, 6.6 task/s, elapsed: 59s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 387/500, 6.6 task/s, elapsed: 59s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 388/500, 6.6 task/s, elapsed: 59s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 389/500, 6.6 task/s, elapsed: 59s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 390/500, 6.6 task/s, elapsed: 59s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 391/500, 6.6 task/s, elapsed: 60s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 392/500, 6.6 task/s, elapsed: 60s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 393/500, 6.6 task/s, elapsed: 60s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 394/500, 6.6 task/s, elapsed: 60s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 395/500, 6.6 task/s, elapsed: 60s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 396/500, 6.6 task/s, elapsed: 60s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 397/500, 6.6 task/s, elapsed: 61s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 398/500, 6.6 task/s, elapsed: 61s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 399/500, 6.6 task/s, elapsed: 61s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 400/500, 6.6 task/s, elapsed: 61s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 401/500, 6.6 task/s, elapsed: 61s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 402/500, 6.6 task/s, elapsed: 61s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 403/500, 6.6 task/s, elapsed: 61s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 404/500, 6.6 task/s, elapsed: 62s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 405/500, 6.6 task/s, elapsed: 62s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 406/500, 6.6 task/s, elapsed: 62s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 407/500, 6.6 task/s, elapsed: 62s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 408/500, 6.6 task/s, elapsed: 62s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 409/500, 6.6 task/s, elapsed: 62s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 410/500, 6.6 task/s, elapsed: 62s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 411/500, 6.6 task/s, elapsed: 63s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 412/500, 6.6 task/s, elapsed: 63s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 413/500, 6.6 task/s, elapsed: 63s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 414/500, 6.6 task/s, elapsed: 63s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 415/500, 6.6 task/s, elapsed: 63s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 416/500, 6.6 task/s, elapsed: 63s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 417/500, 6.6 task/s, elapsed: 63s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 418/500, 6.6 task/s, elapsed: 64s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 419/500, 6.6 task/s, elapsed: 64s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 420/500, 6.6 task/s, elapsed: 64s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 421/500, 6.6 task/s, elapsed: 64s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 422/500, 6.6 task/s, elapsed: 64s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 423/500, 6.6 task/s, elapsed: 64s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 424/500, 6.6 task/s, elapsed: 64s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 425/500, 6.6 task/s, elapsed: 65s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 426/500, 6.6 task/s, elapsed: 65s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 427/500, 6.6 task/s, elapsed: 65s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 428/500, 6.6 task/s, elapsed: 65s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 429/500, 6.6 task/s, elapsed: 65s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 430/500, 6.6 task/s, elapsed: 65s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 431/500, 6.6 task/s, elapsed: 66s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 432/500, 6.6 task/s, elapsed: 66s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 433/500, 6.6 task/s, elapsed: 66s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 434/500, 6.6 task/s, elapsed: 66s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 435/500, 6.6 task/s, elapsed: 66s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 436/500, 6.6 task/s, elapsed: 66s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 437/500, 6.6 task/s, elapsed: 66s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 438/500, 6.6 task/s, elapsed: 67s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 439/500, 6.6 task/s, elapsed: 67s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 440/500, 6.6 task/s, elapsed: 67s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 441/500, 6.6 task/s, elapsed: 67s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 442/500, 6.6 task/s, elapsed: 67s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 443/500, 6.6 task/s, elapsed: 67s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 444/500, 6.6 task/s, elapsed: 67s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 445/500, 6.6 task/s, elapsed: 68s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 446/500, 6.6 task/s, elapsed: 68s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 447/500, 6.6 task/s, elapsed: 68s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 448/500, 6.6 task/s, elapsed: 68s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 449/500, 6.6 task/s, elapsed: 68s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 450/500, 6.6 task/s, elapsed: 68s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 451/500, 6.6 task/s, elapsed: 69s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 452/500, 6.6 task/s, elapsed: 69s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 453/500, 6.6 task/s, elapsed: 69s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 454/500, 6.6 task/s, elapsed: 69s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 455/500, 6.6 task/s, elapsed: 69s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 456/500, 6.6 task/s, elapsed: 69s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 457/500, 6.6 task/s, elapsed: 69s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 458/500, 6.6 task/s, elapsed: 70s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 459/500, 6.6 task/s, elapsed: 70s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 460/500, 6.6 task/s, elapsed: 70s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 461/500, 6.6 task/s, elapsed: 70s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 462/500, 6.6 task/s, elapsed: 70s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 463/500, 6.6 task/s, elapsed: 70s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 464/500, 6.6 task/s, elapsed: 71s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 465/500, 6.6 task/s, elapsed: 71s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 466/500, 6.6 task/s, elapsed: 71s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 467/500, 6.6 task/s, elapsed: 71s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 468/500, 6.6 task/s, elapsed: 71s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 469/500, 6.6 task/s, elapsed: 71s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 470/500, 6.6 task/s, elapsed: 71s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 471/500, 6.6 task/s, elapsed: 72s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 472/500, 6.6 task/s, elapsed: 72s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 473/500, 6.6 task/s, elapsed: 72s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 474/500, 6.6 task/s, elapsed: 72s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 475/500, 6.6 task/s, elapsed: 72s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 476/500, 6.6 task/s, elapsed: 72s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 477/500, 6.6 task/s, elapsed: 73s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 478/500, 6.6 task/s, elapsed: 73s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 479/500, 6.6 task/s, elapsed: 73s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 480/500, 6.6 task/s, elapsed: 73s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 481/500, 6.6 task/s, elapsed: 73s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 482/500, 6.6 task/s, elapsed: 73s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 483/500, 6.6 task/s, elapsed: 73s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 484/500, 6.6 task/s, elapsed: 74s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 485/500, 6.6 task/s, elapsed: 74s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 486/500, 6.6 task/s, elapsed: 74s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 487/500, 6.6 task/s, elapsed: 74s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 488/500, 6.6 task/s, elapsed: 74s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 489/500, 6.6 task/s, elapsed: 74s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 490/500, 6.6 task/s, elapsed: 75s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 491/500, 6.6 task/s, elapsed: 75s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 492/500, 6.6 task/s, elapsed: 75s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 493/500, 6.6 task/s, elapsed: 75s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 494/500, 6.6 task/s, elapsed: 75s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 495/500, 6.6 task/s, elapsed: 75s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 496/500, 6.6 task/s, elapsed: 76s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 497/500, 6.6 task/s, elapsed: 76s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 498/500, 6.6 task/s, elapsed: 76s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 499/500, 6.6 task/s, elapsed: 76s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 500/500, 6.6 task/s, elapsed: 76s, ETA:     0s2022-04-18 21:53:25,092 - mmseg - INFO - per class results:
2022-04-18 21:53:25,093 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|      road     | 87.53 | 91.54 |
|    sidewalk   | 43.04 | 70.71 |
|    building   | 87.07 | 95.86 |
|      wall     | 26.67 | 30.52 |
|     fence     |  6.5  |  7.22 |
|      pole     | 47.92 | 57.22 |
| traffic light | 53.03 | 61.17 |
|  traffic sign | 53.14 | 61.11 |
|   vegetation  | 85.78 | 93.22 |
|    terrain    |  0.0  |  0.0  |
|      sky      | 86.11 | 98.94 |
|     person    | 73.89 | 85.33 |
|     rider     | 46.37 | 64.82 |
|      car      | 87.71 | 95.74 |
|     truck     |  0.0  |  0.0  |
|      bus      | 60.43 | 85.66 |
|     train     |  0.0  |  0.0  |
|   motorcycle  | 49.17 | 56.57 |
|    bicycle    | 60.82 | 71.35 |
+---------------+-------+-------+
2022-04-18 21:53:25,093 - mmseg - INFO - Summary:
2022-04-18 21:53:25,093 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 88.79 | 50.27 | 59.31 |
+-------+-------+-------+
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:53:25,103 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-18 21:53:25,103 - mmseg - INFO - Iter [500/40000]	lr: 3.000e-05, eta: 8:33:52, time: 1.554, data_time: 0.013, memory: 9636, aAcc: 0.8879, mIoU: 0.5027, mAcc: 0.5931, IoU.road: 0.8753, IoU.sidewalk: 0.4304, IoU.building: 0.8707, IoU.wall: 0.2667, IoU.fence: 0.0650, IoU.pole: 0.4792, IoU.traffic light: 0.5303, IoU.traffic sign: 0.5314, IoU.vegetation: 0.8578, IoU.terrain: 0.0000, IoU.sky: 0.8611, IoU.person: 0.7389, IoU.rider: 0.4637, IoU.car: 0.8771, IoU.truck: 0.0000, IoU.bus: 0.6043, IoU.train: 0.0000, IoU.motorcycle: 0.4917, IoU.bicycle: 0.6082, Acc.road: 0.9154, Acc.sidewalk: 0.7071, Acc.building: 0.9586, Acc.wall: 0.3052, Acc.fence: 0.0722, Acc.pole: 0.5722, Acc.traffic light: 0.6117, Acc.traffic sign: 0.6111, Acc.vegetation: 0.9322, Acc.terrain: 0.0000, Acc.sky: 0.9894, Acc.person: 0.8533, Acc.rider: 0.6482, Acc.car: 0.9574, Acc.truck: 0.0000, Acc.bus: 0.8566, Acc.train: 0.0000, Acc.motorcycle: 0.5657, Acc.bicycle: 0.7135, decode.loss_seg: 0.1085, decode.acc_seg: 90.3856, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1353, mix.decode.acc_seg: 88.6233
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:54:43,264 - mmseg - INFO - Iter [20050/40000]	lr: 2.993e-05, eta: 8:34:36, time: 3.969, data_time: 2.421, memory: 9636, decode.loss_seg: 0.1165, decode.acc_seg: 90.2785, src.loss_imnet_feat_dist: 0.0998, mix.decode.loss_seg: 0.1380, mix.decode.acc_seg: 89.1596
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:56:01,290 - mmseg - INFO - Iter [20100/40000]	lr: 2.985e-05, eta: 8:33:19, time: 1.560, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1214, decode.acc_seg: 90.2459, src.loss_imnet_feat_dist: 0.1039, mix.decode.loss_seg: 0.1387, mix.decode.acc_seg: 88.7052
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:57:19,127 - mmseg - INFO - Iter [20150/40000]	lr: 2.978e-05, eta: 8:32:02, time: 1.557, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1075, decode.acc_seg: 91.7153, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1348, mix.decode.acc_seg: 89.2958
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:58:33,617 - mmseg - INFO - Iter [20200/40000]	lr: 2.970e-05, eta: 8:30:42, time: 1.490, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1191, decode.acc_seg: 90.6677, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1317, mix.decode.acc_seg: 87.8834
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 21:59:53,027 - mmseg - INFO - Iter [20250/40000]	lr: 2.963e-05, eta: 8:29:26, time: 1.588, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1108, decode.acc_seg: 91.8751, src.loss_imnet_feat_dist: 0.1058, mix.decode.loss_seg: 0.1355, mix.decode.acc_seg: 89.9293
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:01:08,386 - mmseg - INFO - Iter [20300/40000]	lr: 2.955e-05, eta: 8:28:07, time: 1.507, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1201, decode.acc_seg: 90.9582, src.loss_imnet_feat_dist: 0.1050, mix.decode.loss_seg: 0.1392, mix.decode.acc_seg: 88.2855
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:02:22,666 - mmseg - INFO - Iter [20350/40000]	lr: 2.948e-05, eta: 8:26:47, time: 1.486, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1190, decode.acc_seg: 89.4127, src.loss_imnet_feat_dist: 0.1023, mix.decode.loss_seg: 0.1345, mix.decode.acc_seg: 88.3270
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:03:38,116 - mmseg - INFO - Iter [20400/40000]	lr: 2.940e-05, eta: 8:25:27, time: 1.509, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1142, decode.acc_seg: 90.3888, src.loss_imnet_feat_dist: 0.1025, mix.decode.loss_seg: 0.1302, mix.decode.acc_seg: 88.4114
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:04:57,046 - mmseg - INFO - Iter [20450/40000]	lr: 2.933e-05, eta: 8:24:12, time: 1.579, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1162, decode.acc_seg: 91.7051, src.loss_imnet_feat_dist: 0.1048, mix.decode.loss_seg: 0.1310, mix.decode.acc_seg: 89.9915
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:06:13,836 - mmseg - INFO - Iter [20500/40000]	lr: 2.925e-05, eta: 8:22:54, time: 1.536, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1124, decode.acc_seg: 91.5995, src.loss_imnet_feat_dist: 0.1074, mix.decode.loss_seg: 0.1292, mix.decode.acc_seg: 89.3240
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:07:33,525 - mmseg - INFO - Iter [20550/40000]	lr: 2.918e-05, eta: 8:21:38, time: 1.594, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1185, decode.acc_seg: 90.5223, src.loss_imnet_feat_dist: 0.1055, mix.decode.loss_seg: 0.1460, mix.decode.acc_seg: 87.1297
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:08:48,544 - mmseg - INFO - Iter [20600/40000]	lr: 2.910e-05, eta: 8:20:19, time: 1.500, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1090, decode.acc_seg: 92.0059, src.loss_imnet_feat_dist: 0.1024, mix.decode.loss_seg: 0.1388, mix.decode.acc_seg: 88.5752
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:10:03,399 - mmseg - INFO - Iter [20650/40000]	lr: 2.903e-05, eta: 8:18:59, time: 1.497, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1164, decode.acc_seg: 90.8881, src.loss_imnet_feat_dist: 0.1094, mix.decode.loss_seg: 0.1298, mix.decode.acc_seg: 88.5219
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:11:20,675 - mmseg - INFO - Iter [20700/40000]	lr: 2.895e-05, eta: 8:17:42, time: 1.546, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1233, decode.acc_seg: 91.0021, src.loss_imnet_feat_dist: 0.1104, mix.decode.loss_seg: 0.1558, mix.decode.acc_seg: 87.5673
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:12:40,069 - mmseg - INFO - Iter [20750/40000]	lr: 2.888e-05, eta: 8:16:26, time: 1.588, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1127, decode.acc_seg: 90.6655, src.loss_imnet_feat_dist: 0.1077, mix.decode.loss_seg: 0.1225, mix.decode.acc_seg: 89.2413
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:13:42,660 - mmseg - INFO - Iter [9450/40000]	lr: 4.583e-05, eta: 1 day, 4:55:55, time: 3.365, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.0964, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.1529
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:16:30,341 - mmseg - INFO - Iter [9500/40000]	lr: 4.575e-05, eta: 1 day, 4:52:55, time: 3.354, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 25.3030, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 63.3963
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:19:18,118 - mmseg - INFO - Iter [9550/40000]	lr: 4.568e-05, eta: 1 day, 4:49:56, time: 3.356, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.6672, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.1479
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:22:04,781 - mmseg - INFO - Iter [9600/40000]	lr: 4.560e-05, eta: 1 day, 4:46:54, time: 3.333, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 24.4022, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.6916
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:24:55,316 - mmseg - INFO - Iter [9650/40000]	lr: 4.553e-05, eta: 1 day, 4:44:04, time: 3.411, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 17.6076, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.9957
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:25:16,428 - mmseg - INFO - Iter [21250/40000]	lr: 2.813e-05, eta: 8:03:17, time: 1.452, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1106, decode.acc_seg: 91.5863, src.loss_imnet_feat_dist: 0.1083, mix.decode.loss_seg: 0.1435, mix.decode.acc_seg: 88.4429
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:26:27,094 - mmseg - INFO - Iter [21300/40000]	lr: 2.805e-05, eta: 8:01:54, time: 1.413, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1107, decode.acc_seg: 91.2637, src.loss_imnet_feat_dist: 0.1059, mix.decode.loss_seg: 0.1348, mix.decode.acc_seg: 88.7905
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:27:40,376 - mmseg - INFO - Iter [21350/40000]	lr: 2.798e-05, eta: 8:00:33, time: 1.466, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1177, decode.acc_seg: 91.2672, src.loss_imnet_feat_dist: 0.1058, mix.decode.loss_seg: 0.1322, mix.decode.acc_seg: 89.2762
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:28:57,590 - mmseg - INFO - Iter [21400/40000]	lr: 2.790e-05, eta: 7:59:16, time: 1.544, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1165, decode.acc_seg: 90.9014, src.loss_imnet_feat_dist: 0.1071, mix.decode.loss_seg: 0.1281, mix.decode.acc_seg: 89.2904
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:30:13,317 - mmseg - INFO - Iter [21450/40000]	lr: 2.783e-05, eta: 7:57:57, time: 1.515, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1102, decode.acc_seg: 90.2715, src.loss_imnet_feat_dist: 0.0970, mix.decode.loss_seg: 0.1206, mix.decode.acc_seg: 89.1020
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:31:27,114 - mmseg - INFO - Iter [21500/40000]	lr: 2.775e-05, eta: 7:56:37, time: 1.476, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1183, decode.acc_seg: 90.3243, src.loss_imnet_feat_dist: 0.1008, mix.decode.loss_seg: 0.1314, mix.decode.acc_seg: 88.1828
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:32:44,736 - mmseg - INFO - Iter [21550/40000]	lr: 2.768e-05, eta: 7:55:20, time: 1.552, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1274, decode.acc_seg: 90.5367, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1302, mix.decode.acc_seg: 89.3275
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:34:01,761 - mmseg - INFO - Iter [21600/40000]	lr: 2.760e-05, eta: 7:54:02, time: 1.541, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1125, decode.acc_seg: 90.3533, src.loss_imnet_feat_dist: 0.1038, mix.decode.loss_seg: 0.1310, mix.decode.acc_seg: 87.8860
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:35:18,642 - mmseg - INFO - Iter [21650/40000]	lr: 2.753e-05, eta: 7:52:45, time: 1.538, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1079, decode.acc_seg: 91.1243, src.loss_imnet_feat_dist: 0.1054, mix.decode.loss_seg: 0.1355, mix.decode.acc_seg: 88.6501
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:36:35,049 - mmseg - INFO - Iter [21700/40000]	lr: 2.745e-05, eta: 7:51:27, time: 1.528, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1110, decode.acc_seg: 90.8067, src.loss_imnet_feat_dist: 0.1012, mix.decode.loss_seg: 0.1283, mix.decode.acc_seg: 89.6039
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:37:51,550 - mmseg - INFO - Iter [21750/40000]	lr: 2.738e-05, eta: 7:50:09, time: 1.530, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1115, decode.acc_seg: 90.7636, src.loss_imnet_feat_dist: 0.0997, mix.decode.loss_seg: 0.1202, mix.decode.acc_seg: 88.5132
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:38:53,953 - mmseg - INFO - Iter [9900/40000]	lr: 4.515e-05, eta: 1 day, 4:29:11, time: 3.380, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.1872, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.5474
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:41:41,307 - mmseg - INFO - Iter [9950/40000]	lr: 4.508e-05, eta: 1 day, 4:26:11, time: 3.347, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.3426, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.3574
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:44:27,747 - mmseg - INFO - Exp name: 220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02
2022-04-18 22:44:27,747 - mmseg - INFO - Iter [10000/40000]	lr: 4.500e-05, eta: 1 day, 4:23:09, time: 3.329, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.9622, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.6728
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:47:16,920 - mmseg - INFO - Iter [10050/40000]	lr: 4.493e-05, eta: 1 day, 4:20:16, time: 3.383, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.6370, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.0488
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:50:03,549 - mmseg - INFO - Iter [10100/40000]	lr: 4.485e-05, eta: 1 day, 4:17:14, time: 3.333, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.3439, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.8588
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:50:41,868 - mmseg - INFO - Iter [22250/40000]	lr: 2.663e-05, eta: 7:37:14, time: 1.583, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1186, decode.acc_seg: 90.0469, src.loss_imnet_feat_dist: 0.1068, mix.decode.loss_seg: 0.1345, mix.decode.acc_seg: 88.9004
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:51:55,702 - mmseg - INFO - Iter [22300/40000]	lr: 2.655e-05, eta: 7:35:54, time: 1.477, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1001, decode.acc_seg: 91.0261, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1300, mix.decode.acc_seg: 89.5042
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:53:11,090 - mmseg - INFO - Iter [22350/40000]	lr: 2.648e-05, eta: 7:34:35, time: 1.508, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1212, decode.acc_seg: 90.4053, src.loss_imnet_feat_dist: 0.1005, mix.decode.loss_seg: 0.1402, mix.decode.acc_seg: 88.1502
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:54:22,049 - mmseg - INFO - Iter [22400/40000]	lr: 2.640e-05, eta: 7:33:13, time: 1.419, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1204, decode.acc_seg: 91.1183, src.loss_imnet_feat_dist: 0.1039, mix.decode.loss_seg: 0.1286, mix.decode.acc_seg: 88.5354
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:55:39,687 - mmseg - INFO - Iter [22450/40000]	lr: 2.633e-05, eta: 7:31:56, time: 1.553, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1062, decode.acc_seg: 90.4209, src.loss_imnet_feat_dist: 0.1011, mix.decode.loss_seg: 0.1277, mix.decode.acc_seg: 87.7052
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:56:58,117 - mmseg - INFO - Iter [22500/40000]	lr: 2.625e-05, eta: 7:30:40, time: 1.569, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1036, decode.acc_seg: 91.2041, src.loss_imnet_feat_dist: 0.1027, mix.decode.loss_seg: 0.1169, mix.decode.acc_seg: 89.5960
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:58:17,821 - mmseg - INFO - Iter [22550/40000]	lr: 2.618e-05, eta: 7:29:24, time: 1.594, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1174, decode.acc_seg: 90.5484, src.loss_imnet_feat_dist: 0.1051, mix.decode.loss_seg: 0.1450, mix.decode.acc_seg: 87.9023
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 22:59:35,834 - mmseg - INFO - Iter [22600/40000]	lr: 2.610e-05, eta: 7:28:07, time: 1.560, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1154, decode.acc_seg: 90.6774, src.loss_imnet_feat_dist: 0.1045, mix.decode.loss_seg: 0.1308, mix.decode.acc_seg: 88.3808
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:00:54,572 - mmseg - INFO - Iter [22650/40000]	lr: 2.603e-05, eta: 7:26:51, time: 1.575, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1287, decode.acc_seg: 90.8640, src.loss_imnet_feat_dist: 0.1047, mix.decode.loss_seg: 0.1788, mix.decode.acc_seg: 87.9480
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:02:09,011 - mmseg - INFO - Iter [22700/40000]	lr: 2.595e-05, eta: 7:25:32, time: 1.489, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1195, decode.acc_seg: 90.1565, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1425, mix.decode.acc_seg: 86.9983
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:03:20,629 - mmseg - INFO - Iter [22750/40000]	lr: 2.588e-05, eta: 7:24:10, time: 1.432, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1149, decode.acc_seg: 90.3631, src.loss_imnet_feat_dist: 0.1023, mix.decode.loss_seg: 0.1329, mix.decode.acc_seg: 88.1647
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:04:33,428 - mmseg - INFO - Iter [22800/40000]	lr: 2.580e-05, eta: 7:22:50, time: 1.456, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1094, decode.acc_seg: 90.0663, src.loss_imnet_feat_dist: 0.1018, mix.decode.loss_seg: 0.1221, mix.decode.acc_seg: 89.0767
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:05:45,344 - mmseg - INFO - Iter [22850/40000]	lr: 2.573e-05, eta: 7:21:29, time: 1.438, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1202, decode.acc_seg: 89.7292, src.loss_imnet_feat_dist: 0.1029, mix.decode.loss_seg: 0.1325, mix.decode.acc_seg: 88.3148
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:07:01,165 - mmseg - INFO - Iter [22900/40000]	lr: 2.565e-05, eta: 7:20:10, time: 1.516, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1136, decode.acc_seg: 90.9382, src.loss_imnet_feat_dist: 0.1072, mix.decode.loss_seg: 0.1261, mix.decode.acc_seg: 89.7562
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:08:12,854 - mmseg - INFO - Iter [22950/40000]	lr: 2.558e-05, eta: 7:18:49, time: 1.434, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1372, decode.acc_seg: 89.8794, src.loss_imnet_feat_dist: 0.1032, mix.decode.loss_seg: 0.1395, mix.decode.acc_seg: 87.7709
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:09:24,497 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-18 23:09:24,497 - mmseg - INFO - Iter [23000/40000]	lr: 2.550e-05, eta: 7:17:28, time: 1.433, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1203, decode.acc_seg: 90.1176, src.loss_imnet_feat_dist: 0.1069, mix.decode.loss_seg: 0.1332, mix.decode.acc_seg: 88.3552
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:09:42,133 - mmseg - INFO - Iter [10450/40000]	lr: 4.433e-05, eta: 1 day, 3:56:44, time: 3.398, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.9970, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.9735
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:12:28,611 - mmseg - INFO - Iter [10500/40000]	lr: 4.425e-05, eta: 1 day, 3:53:43, time: 3.330, data_time: 0.017, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.4494, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.5549
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:15:15,751 - mmseg - INFO - Iter [10550/40000]	lr: 4.418e-05, eta: 1 day, 3:50:45, time: 3.343, data_time: 0.017, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.6134, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.0993
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:18:03,567 - mmseg - INFO - Iter [10600/40000]	lr: 4.410e-05, eta: 1 day, 3:47:48, time: 3.356, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.7467, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.7498
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:20:51,827 - mmseg - INFO - Iter [10650/40000]	lr: 4.403e-05, eta: 1 day, 3:44:52, time: 3.365, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.4574, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.9919
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:21:09,190 - mmseg - INFO - Iter [23450/40000]	lr: 2.483e-05, eta: 7:06:00, time: 1.574, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1097, decode.acc_seg: 91.4033, src.loss_imnet_feat_dist: 0.1003, mix.decode.loss_seg: 0.1303, mix.decode.acc_seg: 89.5086
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:22:27,576 - mmseg - INFO - Iter [23500/40000]	lr: 2.475e-05, eta: 7:04:43, time: 1.568, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1176, decode.acc_seg: 91.0781, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1407, mix.decode.acc_seg: 88.4129
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:23:41,646 - mmseg - INFO - Iter [23550/40000]	lr: 2.468e-05, eta: 7:03:24, time: 1.481, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1111, decode.acc_seg: 91.5829, src.loss_imnet_feat_dist: 0.0964, mix.decode.loss_seg: 0.1404, mix.decode.acc_seg: 89.1421
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:24:51,747 - mmseg - INFO - Iter [23600/40000]	lr: 2.460e-05, eta: 7:02:02, time: 1.402, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1327, decode.acc_seg: 90.0974, src.loss_imnet_feat_dist: 0.1023, mix.decode.loss_seg: 0.1333, mix.decode.acc_seg: 88.4522
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:26:04,729 - mmseg - INFO - Iter [23650/40000]	lr: 2.453e-05, eta: 7:00:42, time: 1.460, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1051, decode.acc_seg: 91.4311, src.loss_imnet_feat_dist: 0.1064, mix.decode.loss_seg: 0.1145, mix.decode.acc_seg: 89.9997
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:27:16,421 - mmseg - INFO - Iter [23700/40000]	lr: 2.445e-05, eta: 6:59:21, time: 1.434, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1223, decode.acc_seg: 90.4630, src.loss_imnet_feat_dist: 0.1020, mix.decode.loss_seg: 0.1185, mix.decode.acc_seg: 89.1048
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:28:28,852 - mmseg - INFO - Iter [23750/40000]	lr: 2.438e-05, eta: 6:58:00, time: 1.449, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1085, decode.acc_seg: 90.7472, src.loss_imnet_feat_dist: 0.1001, mix.decode.loss_seg: 0.1268, mix.decode.acc_seg: 89.8828
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:29:39,940 - mmseg - INFO - Iter [23800/40000]	lr: 2.430e-05, eta: 6:56:39, time: 1.422, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1289, decode.acc_seg: 90.7975, src.loss_imnet_feat_dist: 0.1058, mix.decode.loss_seg: 0.1350, mix.decode.acc_seg: 88.9162
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:30:52,617 - mmseg - INFO - Iter [23850/40000]	lr: 2.423e-05, eta: 6:55:19, time: 1.454, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1220, decode.acc_seg: 90.2176, src.loss_imnet_feat_dist: 0.1035, mix.decode.loss_seg: 0.1320, mix.decode.acc_seg: 88.4990
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:32:03,243 - mmseg - INFO - Iter [23900/40000]	lr: 2.415e-05, eta: 6:53:57, time: 1.413, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1106, decode.acc_seg: 90.2296, src.loss_imnet_feat_dist: 0.1016, mix.decode.loss_seg: 0.1499, mix.decode.acc_seg: 86.9866
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:33:15,214 - mmseg - INFO - Iter [23950/40000]	lr: 2.408e-05, eta: 6:52:37, time: 1.439, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1078, decode.acc_seg: 91.4516, src.loss_imnet_feat_dist: 0.1014, mix.decode.loss_seg: 0.1133, mix.decode.acc_seg: 90.2558
[                                                  ] 0/500, elapsed: 0s, ETA:
[                                 ] 1/500, 1.4 task/s, elapsed: 1s, ETA:   363s
[                                 ] 2/500, 2.3 task/s, elapsed: 1s, ETA:   218s
[                                 ] 3/500, 2.9 task/s, elapsed: 1s, ETA:   171s
[                                 ] 4/500, 3.4 task/s, elapsed: 1s, ETA:   146s
[                                 ] 5/500, 3.8 task/s, elapsed: 1s, ETA:   131s
[                                 ] 6/500, 4.1 task/s, elapsed: 1s, ETA:   121s
[                                 ] 7/500, 4.3 task/s, elapsed: 2s, ETA:   114s
[                                 ] 8/500, 4.5 task/s, elapsed: 2s, ETA:   108s
[                                 ] 9/500, 4.7 task/s, elapsed: 2s, ETA:   104s
[                                ] 10/500, 4.9 task/s, elapsed: 2s, ETA:   101s
[                                ] 11/500, 5.0 task/s, elapsed: 2s, ETA:    98s
[                                ] 12/500, 5.1 task/s, elapsed: 2s, ETA:    95s
[                                ] 13/500, 5.2 task/s, elapsed: 2s, ETA:    93s
[                                ] 14/500, 5.3 task/s, elapsed: 3s, ETA:    91s
[                                ] 15/500, 5.4 task/s, elapsed: 3s, ETA:    90s
[>                               ] 16/500, 5.4 task/s, elapsed: 3s, ETA:    89s
[>                               ] 17/500, 5.5 task/s, elapsed: 3s, ETA:    88s
[>                               ] 18/500, 5.5 task/s, elapsed: 3s, ETA:    87s
[>                               ] 19/500, 5.6 task/s, elapsed: 3s, ETA:    86s
[>                               ] 20/500, 5.6 task/s, elapsed: 4s, ETA:    85s
[>                               ] 21/500, 5.7 task/s, elapsed: 4s, ETA:    85s
[>                               ] 22/500, 5.7 task/s, elapsed: 4s, ETA:    84s
[>                               ] 23/500, 5.7 task/s, elapsed: 4s, ETA:    83s
[>                               ] 24/500, 5.7 task/s, elapsed: 4s, ETA:    83s
[>                               ] 25/500, 5.8 task/s, elapsed: 4s, ETA:    82s
[>                               ] 26/500, 5.8 task/s, elapsed: 4s, ETA:    81s
[>                               ] 27/500, 5.9 task/s, elapsed: 5s, ETA:    81s
[>                               ] 28/500, 5.9 task/s, elapsed: 5s, ETA:    80s
[>                               ] 29/500, 5.9 task/s, elapsed: 5s, ETA:    80s
[>                               ] 30/500, 5.9 task/s, elapsed: 5s, ETA:    79s
[>                               ] 31/500, 6.0 task/s, elapsed: 5s, ETA:    79s
[>>                              ] 32/500, 6.0 task/s, elapsed: 5s, ETA:    78s
[>>                              ] 33/500, 6.0 task/s, elapsed: 5s, ETA:    78s
[>>                              ] 34/500, 6.0 task/s, elapsed: 6s, ETA:    77s
[>>                              ] 35/500, 6.1 task/s, elapsed: 6s, ETA:    77s
[>>                              ] 36/500, 6.1 task/s, elapsed: 6s, ETA:    76s
[>>                              ] 37/500, 6.1 task/s, elapsed: 6s, ETA:    76s
[>>                              ] 38/500, 6.1 task/s, elapsed: 6s, ETA:    75s
[>>                              ] 39/500, 6.1 task/s, elapsed: 6s, ETA:    75s
[>>                              ] 40/500, 6.2 task/s, elapsed: 6s, ETA:    75s
[>>                              ] 41/500, 6.2 task/s, elapsed: 7s, ETA:    74s
[>>                              ] 42/500, 6.2 task/s, elapsed: 7s, ETA:    74s
[>>                              ] 43/500, 6.2 task/s, elapsed: 7s, ETA:    74s
[>>                              ] 44/500, 6.2 task/s, elapsed: 7s, ETA:    73s
[>>                              ] 45/500, 6.2 task/s, elapsed: 7s, ETA:    73s
[>>                              ] 46/500, 6.2 task/s, elapsed: 7s, ETA:    73s
[>>>                             ] 47/500, 6.3 task/s, elapsed: 8s, ETA:    72s
[>>>                             ] 48/500, 6.3 task/s, elapsed: 8s, ETA:    72s
[>>>                             ] 49/500, 6.3 task/s, elapsed: 8s, ETA:    72s
[>>>                             ] 50/500, 6.3 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 51/500, 6.3 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 52/500, 6.3 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 53/500, 6.3 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 54/500, 6.3 task/s, elapsed: 9s, ETA:    71s
[>>>                             ] 55/500, 6.3 task/s, elapsed: 9s, ETA:    70s
[>>>                             ] 56/500, 6.3 task/s, elapsed: 9s, ETA:    70s
[>>>                             ] 57/500, 6.3 task/s, elapsed: 9s, ETA:    70s
[>>>                             ] 58/500, 6.3 task/s, elapsed: 9s, ETA:    70s
[>>>                             ] 59/500, 6.3 task/s, elapsed: 9s, ETA:    70s
[>>>                             ] 60/500, 6.3 task/s, elapsed: 9s, ETA:    69s
[>>>                            ] 61/500, 6.3 task/s, elapsed: 10s, ETA:    69s
[>>>                            ] 62/500, 6.3 task/s, elapsed: 10s, ETA:    69s
[>>>                            ] 63/500, 6.3 task/s, elapsed: 10s, ETA:    69s
[>>>                            ] 64/500, 6.4 task/s, elapsed: 10s, ETA:    69s
[>>>>                           ] 65/500, 6.4 task/s, elapsed: 10s, ETA:    68s
[>>>>                           ] 66/500, 6.3 task/s, elapsed: 10s, ETA:    69s
[>>>>                           ] 67/500, 6.3 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 68/500, 6.3 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 69/500, 6.4 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 70/500, 6.4 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 71/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 72/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 73/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 74/500, 6.4 task/s, elapsed: 12s, ETA:    67s
[>>>>                           ] 75/500, 6.4 task/s, elapsed: 12s, ETA:    67s
[>>>>                           ] 76/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 77/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 78/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 79/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 80/500, 6.4 task/s, elapsed: 13s, ETA:    66s
[>>>>>                          ] 81/500, 6.4 task/s, elapsed: 13s, ETA:    66s
[>>>>>                          ] 82/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 83/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 84/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 85/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 86/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 87/500, 6.4 task/s, elapsed: 14s, ETA:    65s
[>>>>>                          ] 88/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 89/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 90/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 91/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 92/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 93/500, 6.4 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 94/500, 6.4 task/s, elapsed: 15s, ETA:    63s
[>>>>>                          ] 95/500, 6.4 task/s, elapsed: 15s, ETA:    63s
[>>>>>                          ] 96/500, 6.4 task/s, elapsed: 15s, ETA:    63s
[>>>>>>                         ] 97/500, 6.4 task/s, elapsed: 15s, ETA:    63s/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:34:48,893 - mmseg - INFO - Iter [10900/40000]	lr: 4.365e-05, eta: 1 day, 3:30:05, time: 3.319, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.5525, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.9010
 task/s, elapsed: 16s, ETA:    62s
[>>>>>>                        ] 106/500, 6.4 task/s, elapsed: 17s, ETA:    62s
[>>>>>>                        ] 107/500, 6.4 task/s, elapsed: 17s, ETA:    61s
[>>>>>>                        ] 108/500, 6.4 task/s, elapsed: 17s, ETA:    61s
[>>>>>>                        ] 109/500, 6.4 task/s, elapsed: 17s, ETA:    61s
[>>>>>>                        ] 110/500, 6.4 task/s, elapsed: 17s, ETA:    61s
[>>>>>>                        ] 111/500, 6.4 task/s, elapsed: 17s, ETA:    61s
[>>>>>>                        ] 112/500, 6.4 task/s, elapsed: 18s, ETA:    61s
[>>>>>>                        ] 113/500, 6.4 task/s, elapsed: 18s, ETA:    60s
[>>>>>>                        ] 114/500, 6.4 task/s, elapsed: 18s, ETA:    60s
[>>>>>>                        ] 115/500, 6.4 task/s, elapsed: 18s, ETA:    60s
[>>>>>>                        ] 116/500, 6.4 task/s, elapsed: 18s, ETA:    60s
[>>>>>>>                       ] 117/500, 6.4 task/s, elapsed: 18s, ETA:    60s
[>>>>>>>                       ] 118/500, 6.4 task/s, elapsed: 18s, ETA:    60s
[>>>>>>>                       ] 119/500, 6.4 task/s, elapsed: 19s, ETA:    59s
[>>>>>>>                       ] 120/500, 6.4 task/s, elapsed: 19s, ETA:    59s
[>>>>>>>                       ] 121/500, 6.4 task/s, elapsed: 19s, ETA:    59s
[>>>>>>>                       ] 122/500, 6.4 task/s, elapsed: 19s, ETA:    59s
[>>>>>>>                       ] 123/500, 6.4 task/s, elapsed: 19s, ETA:    59s
[>>>>>>>                       ] 124/500, 6.4 task/s, elapsed: 19s, ETA:    58s
[>>>>>>>                       ] 125/500, 6.4 task/s, elapsed: 19s, ETA:    58s
[>>>>>>>                       ] 126/500, 6.4 task/s, elapsed: 20s, ETA:    58s
[>>>>>>>                       ] 127/500, 6.4 task/s, elapsed: 20s, ETA:    58s
[>>>>>>>                       ] 128/500, 6.4 task/s, elapsed: 20s, ETA:    58s
[>>>>>>>                       ] 129/500, 6.5 task/s, elapsed: 20s, ETA:    57s
[>>>>>>>                       ] 130/500, 6.5 task/s, elapsed: 20s, ETA:    57s
[>>>>>>>                       ] 131/500, 6.5 task/s, elapsed: 20s, ETA:    57s
[>>>>>>>                       ] 132/500, 6.5 task/s, elapsed: 20s, ETA:    57s
[>>>>>>>                       ] 133/500, 6.5 task/s, elapsed: 21s, ETA:    57s
[>>>>>>>>                      ] 134/500, 6.5 task/s, elapsed: 21s, ETA:    57s
[>>>>>>>>                      ] 135/500, 6.5 task/s, elapsed: 21s, ETA:    56s
[>>>>>>>>                      ] 136/500, 6.5 task/s, elapsed: 21s, ETA:    56s
[>>>>>>>>                      ] 137/500, 6.5 task/s, elapsed: 21s, ETA:    56s
[>>>>>>>>                      ] 138/500, 6.5 task/s, elapsed: 21s, ETA:    56s
[>>>>>>>>                      ] 139/500, 6.5 task/s, elapsed: 21s, ETA:    56s
[>>>>>>>>                      ] 140/500, 6.5 task/s, elapsed: 22s, ETA:    56s
[>>>>>>>>                      ] 141/500, 6.5 task/s, elapsed: 22s, ETA:    55s
[>>>>>>>>                      ] 142/500, 6.5 task/s, elapsed: 22s, ETA:    55s
[>>>>>>>>                      ] 143/500, 6.5 task/s, elapsed: 22s, ETA:    55s
[>>>>>>>>                      ] 144/500, 6.5 task/s, elapsed: 22s, ETA:    55s
[>>>>>>>>                      ] 145/500, 6.5 task/s, elapsed: 22s, ETA:    55s
[>>>>>>>>                      ] 146/500, 6.5 task/s, elapsed: 22s, ETA:    54s
[>>>>>>>>                      ] 147/500, 6.5 task/s, elapsed: 23s, ETA:    54s
[>>>>>>>>                      ] 148/500, 6.5 task/s, elapsed: 23s, ETA:    54s
[>>>>>>>>                      ] 149/500, 6.5 task/s, elapsed: 23s, ETA:    54s
[>>>>>>>>>                     ] 150/500, 6.5 task/s, elapsed: 23s, ETA:    54s
[>>>>>>>>>                     ] 151/500, 6.5 task/s, elapsed: 23s, ETA:    54s
[>>>>>>>>>                     ] 152/500, 6.5 task/s, elapsed: 23s, ETA:    53s
[>>>>>>>>>                     ] 153/500, 6.5 task/s, elapsed: 23s, ETA:    53s
[>>>>>>>>>                     ] 154/500, 6.5 task/s, elapsed: 24s, ETA:    53s
[>>>>>>>>>                     ] 155/500, 6.5 task/s, elapsed: 24s, ETA:    53s
[>>>>>>>>>                     ] 156/500, 6.5 task/s, elapsed: 24s, ETA:    53s
[>>>>>>>>>                     ] 157/500, 6.5 task/s, elapsed: 24s, ETA:    53s
[>>>>>>>>>                     ] 158/500, 6.5 task/s, elapsed: 24s, ETA:    52s
[>>>>>>>>>                     ] 159/500, 6.5 task/s, elapsed: 24s, ETA:    52s
[>>>>>>>>>                     ] 160/500, 6.5 task/s, elapsed: 24s, ETA:    52s
[>>>>>>>>>                     ] 161/500, 6.5 task/s, elapsed: 25s, ETA:    52s
[>>>>>>>>>                     ] 162/500, 6.5 task/s, elapsed: 25s, ETA:    52s
[>>>>>>>>>                     ] 163/500, 6.5 task/s, elapsed: 25s, ETA:    51s
[>>>>>>>>>                     ] 164/500, 6.5 task/s, elapsed: 25s, ETA:    51s
[>>>>>>>>>                     ] 165/500, 6.5 task/s, elapsed: 25s, ETA:    51s
[>>>>>>>>>                     ] 166/500, 6.6 task/s, elapsed: 25s, ETA:    51s
[>>>>>>>>>>                    ] 167/500, 6.6 task/s, elapsed: 25s, ETA:    51s
[>>>>>>>>>>                    ] 168/500, 6.6 task/s, elapsed: 26s, ETA:    51s
[>>>>>>>>>>                    ] 169/500, 6.6 task/s, elapsed: 26s, ETA:    50s
[>>>>>>>>>>                    ] 170/500, 6.6 task/s, elapsed: 26s, ETA:    50s
[>>>>>>>>>>                    ] 171/500, 6.6 task/s, elapsed: 26s, ETA:    50s
[>>>>>>>>>>                    ] 172/500, 6.6 task/s, elapsed: 26s, ETA:    50s
[>>>>>>>>>>                    ] 173/500, 6.6 task/s, elapsed: 26s, ETA:    50s
[>>>>>>>>>>                    ] 174/500, 6.6 task/s, elapsed: 26s, ETA:    50s
[>>>>>>>>>>                    ] 175/500, 6.6 task/s, elapsed: 27s, ETA:    49s
[>>>>>>>>>>                    ] 176/500, 6.6 task/s, elapsed: 27s, ETA:    49s
[>>>>>>>>>>                    ] 177/500, 6.6 task/s, elapsed: 27s, ETA:    49s
[>>>>>>>>>>                    ] 178/500, 6.6 task/s, elapsed: 27s, ETA:    49s
[>>>>>>>>>>                    ] 179/500, 6.6 task/s, elapsed: 27s, ETA:    49s
[>>>>>>>>>>                    ] 180/500, 6.6 task/s, elapsed: 27s, ETA:    49s
[>>>>>>>>>>                    ] 181/500, 6.6 task/s, elapsed: 28s, ETA:    49s
[>>>>>>>>>>                    ] 182/500, 6.6 task/s, elapsed: 28s, ETA:    48s
[>>>>>>>>>>                    ] 183/500, 6.6 task/s, elapsed: 28s, ETA:    48s
[>>>>>>>>>>>                   ] 184/500, 6.6 task/s, elapsed: 28s, ETA:    48s
[>>>>>>>>>>>                   ] 185/500, 6.6 task/s, elapsed: 28s, ETA:    48s
[>>>>>>>>>>>                   ] 186/500, 6.6 task/s, elapsed: 28s, ETA:    48s
[>>>>>>>>>>>                   ] 187/500, 6.6 task/s, elapsed: 28s, ETA:    48s
[>>>>>>>>>>>                   ] 188/500, 6.6 task/s, elapsed: 29s, ETA:    47s
[>>>>>>>>>>>                   ] 189/500, 6.6 task/s, elapsed: 29s, ETA:    47s
[>>>>>>>>>>>                   ] 190/500, 6.6 task/s, elapsed: 29s, ETA:    47s
[>>>>>>>>>>>                   ] 191/500, 6.6 task/s, elapsed: 29s, ETA:    47s
[>>>>>>>>>>>                   ] 192/500, 6.6 task/s, elapsed: 29s, ETA:    47s
[>>>>>>>>>>>                   ] 193/500, 6.6 task/s, elapsed: 29s, ETA:    47s
[>>>>>>>>>>>                   ] 194/500, 6.6 task/s, elapsed: 29s, ETA:    47s
[>>>>>>>>>>>                   ] 195/500, 6.6 task/s, elapsed: 30s, ETA:    46s
[>>>>>>>>>>>                   ] 196/500, 6.6 task/s, elapsed: 30s, ETA:    46s
[>>>>>>>>>>>                   ] 197/500, 6.6 task/s, elapsed: 30s, ETA:    46s
[>>>>>>>>>>>                   ] 198/500, 6.6 task/s, elapsed: 30s, ETA:    46s
[>>>>>>>>>>>                   ] 199/500, 6.6 task/s, elapsed: 30s, ETA:    46s
[>>>>>>>>>>>>                  ] 200/500, 6.6 task/s, elapsed: 30s, ETA:    46s
[>>>>>>>>>>>>                  ] 201/500, 6.6 task/s, elapsed: 31s, ETA:    45s
[>>>>>>>>>>>>                  ] 202/500, 6.6 task/s, elapsed: 31s, ETA:    45s
[>>>>>>>>>>>>                  ] 203/500, 6.6 task/s, elapsed: 31s, ETA:    45s
[>>>>>>>>>>>>                  ] 204/500, 6.6 task/s, elapsed: 31s, ETA:    45s
[>>>>>>>>>>>>                  ] 205/500, 6.6 task/s, elapsed: 31s, ETA:    45s
[>>>>>>>>>>>>                  ] 206/500, 6.6 task/s, elapsed: 31s, ETA:    45s
[>>>>>>>>>>>>                  ] 207/500, 6.6 task/s, elapsed: 31s, ETA:    45s
[>>>>>>>>>>>>                  ] 208/500, 6.6 task/s, elapsed: 32s, ETA:    44s
[>>>>>>>>>>>>                  ] 209/500, 6.6 task/s, elapsed: 32s, ETA:    44s
[>>>>>>>>>>>>                  ] 210/500, 6.6 task/s, elapsed: 32s, ETA:    44s
[>>>>>>>>>>>>                  ] 211/500, 6.6 task/s, elapsed: 32s, ETA:    44s
[>>>>>>>>>>>>                  ] 212/500, 6.6 task/s, elapsed: 32s, ETA:    44s
[>>>>>>>>>>>>                  ] 213/500, 6.6 task/s, elapsed: 32s, ETA:    44s
[>>>>>>>>>>>>                  ] 214/500, 6.6 task/s, elapsed: 33s, ETA:    44s
[>>>>>>>>>>>>                  ] 215/500, 6.6 task/s, elapsed: 33s, ETA:    43s
[>>>>>>>>>>>>                  ] 216/500, 6.6 task/s, elapsed: 33s, ETA:    43s
[>>>>>>>>>>>>>                 ] 217/500, 6.6 task/s, elapsed: 33s, ETA:    43s
[>>>>>>>>>>>>>                 ] 218/500, 6.6 task/s, elapsed: 33s, ETA:    43s
[>>>>>>>>>>>>>                 ] 219/500, 6.6 task/s, elapsed: 33s, ETA:    43s
[>>>>>>>>>>>>>                 ] 220/500, 6.6 task/s, elapsed: 33s, ETA:    43s
[>>>>>>>>>>>>>                 ] 221/500, 6.6 task/s, elapsed: 34s, ETA:    42s
[>>>>>>>>>>>>>                 ] 222/500, 6.6 task/s, elapsed: 34s, ETA:    42s
[>>>>>>>>>>>>>                 ] 223/500, 6.6 task/s, elapsed: 34s, ETA:    42s
[>>>>>>>>>>>>>                 ] 224/500, 6.6 task/s, elapsed: 34s, ETA:    42s
[>>>>>>>>>>>>>                 ] 225/500, 6.6 task/s, elapsed: 34s, ETA:    42s
[>>>>>>>>>>>>>                 ] 226/500, 6.6 task/s, elapsed: 34s, ETA:    42s
[>>>>>>>>>>>>>                 ] 227/500, 6.6 task/s, elapsed: 34s, ETA:    41s
[>>>>>>>>>>>>>                 ] 228/500, 6.6 task/s, elapsed: 35s, ETA:    41s
[>>>>>>>>>>>>>                 ] 229/500, 6.6 task/s, elapsed: 35s, ETA:    41s
[>>>>>>>>>>>>>                 ] 230/500, 6.6 task/s, elapsed: 35s, ETA:    41s
[>>>>>>>>>>>>>                 ] 231/500, 6.6 task/s, elapsed: 35s, ETA:    41s
[>>>>>>>>>>>>>                 ] 232/500, 6.6 task/s, elapsed: 35s, ETA:    41s
[>>>>>>>>>>>>>                 ] 233/500, 6.6 task/s, elapsed: 35s, ETA:    41s
[>>>>>>>>>>>>>>                ] 234/500, 6.6 task/s, elapsed: 36s, ETA:    40s
[>>>>>>>>>>>>>>                ] 235/500, 6.6 task/s, elapsed: 36s, ETA:    40s
[>>>>>>>>>>>>>>                ] 236/500, 6.6 task/s, elapsed: 36s, ETA:    40s
[>>>>>>>>>>>>>>                ] 237/500, 6.6 task/s, elapsed: 36s, ETA:    40s
[>>>>>>>>>>>>>>                ] 238/500, 6.6 task/s, elapsed: 36s, ETA:    40s
[>>>>>>>>>>>>>>                ] 239/500, 6.6 task/s, elapsed: 36s, ETA:    40s
[>>>>>>>>>>>>>>                ] 240/500, 6.6 task/s, elapsed: 37s, ETA:    40s
[>>>>>>>>>>>>>>                ] 241/500, 6.6 task/s, elapsed: 37s, ETA:    39s
[>>>>>>>>>>>>>>                ] 242/500, 6.6 task/s, elapsed: 37s, ETA:    39s
[>>>>>>>>>>>>>>                ] 243/500, 6.6 task/s, elapsed: 37s, ETA:    39s
[>>>>>>>>>>>>>>                ] 244/500, 6.6 task/s, elapsed: 37s, ETA:    39s
[>>>>>>>>>>>>>>                ] 245/500, 6.6 task/s, elapsed: 37s, ETA:    39s
[>>>>>>>>>>>>>>                ] 246/500, 6.6 task/s, elapsed: 37s, ETA:    39s
[>>>>>>>>>>>>>>                ] 247/500, 6.6 task/s, elapsed: 38s, ETA:    39s
[>>>>>>>>>>>>>>                ] 248/500, 6.6 task/s, elapsed: 38s, ETA:    38s
[>>>>>>>>>>>>>>                ] 249/500, 6.6 task/s, elapsed: 38s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 250/500, 6.6 task/s, elapsed: 38s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 251/500, 6.6 task/s, elapsed: 38s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 252/500, 6.6 task/s, elapsed: 38s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 253/500, 6.6 task/s, elapsed: 39s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 254/500, 6.6 task/s, elapsed: 39s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 255/500, 6.6 task/s, elapsed: 39s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 256/500, 6.6 task/s, elapsed: 39s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 257/500, 6.6 task/s, elapsed: 39s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 258/500, 6.6 task/s, elapsed: 39s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 259/500, 6.6 task/s, elapsed: 39s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 260/500, 6.6 task/s, elapsed: 40s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 261/500, 6.6 task/s, elapsed: 40s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 262/500, 6.6 task/s, elapsed: 40s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 263/500, 6.6 task/s, elapsed: 40s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 264/500, 6.6 task/s, elapsed: 40s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 265/500, 6.6 task/s, elapsed: 40s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 266/500, 6.6 task/s, elapsed: 40s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 267/500, 6.6 task/s, elapsed: 41s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 268/500, 6.6 task/s, elapsed: 41s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 269/500, 6.6 task/s, elapsed: 41s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 270/500, 6.6 task/s, elapsed: 41s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 271/500, 6.6 task/s, elapsed: 41s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 272/500, 6.6 task/s, elapsed: 41s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 273/500, 6.6 task/s, elapsed: 41s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 274/500, 6.6 task/s, elapsed: 42s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 275/500, 6.6 task/s, elapsed: 42s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 276/500, 6.6 task/s, elapsed: 42s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 277/500, 6.6 task/s, elapsed: 42s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 278/500, 6.6 task/s, elapsed: 42s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 279/500, 6.6 task/s, elapsed: 42s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 280/500, 6.6 task/s, elapsed: 42s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 281/500, 6.6 task/s, elapsed: 43s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 282/500, 6.6 task/s, elapsed: 43s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 283/500, 6.6 task/s, elapsed: 43s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 284/500, 6.6 task/s, elapsed: 43s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 285/500, 6.6 task/s, elapsed: 43s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 286/500, 6.6 task/s, elapsed: 43s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 287/500, 6.6 task/s, elapsed: 43s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 288/500, 6.6 task/s, elapsed: 44s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 289/500, 6.6 task/s, elapsed: 44s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 290/500, 6.6 task/s, elapsed: 44s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 291/500, 6.6 task/s, elapsed: 44s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 292/500, 6.6 task/s, elapsed: 44s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 293/500, 6.6 task/s, elapsed: 44s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 294/500, 6.6 task/s, elapsed: 44s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 295/500, 6.6 task/s, elapsed: 45s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 296/500, 6.6 task/s, elapsed: 45s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 297/500, 6.6 task/s, elapsed: 45s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 298/500, 6.6 task/s, elapsed: 45s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 299/500, 6.6 task/s, elapsed: 45s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 300/500, 6.6 task/s, elapsed: 45s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 301/500, 6.6 task/s, elapsed: 45s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 302/500, 6.6 task/s, elapsed: 46s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 303/500, 6.6 task/s, elapsed: 46s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 304/500, 6.6 task/s, elapsed: 46s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 305/500, 6.6 task/s, elapsed: 46s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 306/500, 6.6 task/s, elapsed: 46s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 307/500, 6.6 task/s, elapsed: 46s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 308/500, 6.6 task/s, elapsed: 46s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 309/500, 6.6 task/s, elapsed: 47s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 310/500, 6.6 task/s, elapsed: 47s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 311/500, 6.6 task/s, elapsed: 47s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 312/500, 6.6 task/s, elapsed: 47s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 313/500, 6.6 task/s, elapsed: 47s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 314/500, 6.6 task/s, elapsed: 47s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 315/500, 6.6 task/s, elapsed: 47s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 316/500, 6.6 task/s, elapsed: 48s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 317/500, 6.6 task/s, elapsed: 48s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 318/500, 6.6 task/s, elapsed: 48s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 319/500, 6.6 task/s, elapsed: 48s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 320/500, 6.6 task/s, elapsed: 48s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 321/500, 6.6 task/s, elapsed: 48s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 322/500, 6.6 task/s, elapsed: 48s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 323/500, 6.6 task/s, elapsed: 49s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 324/500, 6.6 task/s, elapsed: 49s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 325/500, 6.6 task/s, elapsed: 49s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 326/500, 6.6 task/s, elapsed: 49s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 327/500, 6.6 task/s, elapsed: 49s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 328/500, 6.6 task/s, elapsed: 49s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 329/500, 6.6 task/s, elapsed: 49s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 330/500, 6.6 task/s, elapsed: 50s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 331/500, 6.6 task/s, elapsed: 50s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 332/500, 6.7 task/s, elapsed: 50s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 333/500, 6.7 task/s, elapsed: 50s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 334/500, 6.7 task/s, elapsed: 50s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 335/500, 6.7 task/s, elapsed: 50s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 336/500, 6.7 task/s, elapsed: 50s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 337/500, 6.7 task/s, elapsed: 51s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 338/500, 6.7 task/s, elapsed: 51s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 339/500, 6.7 task/s, elapsed: 51s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 340/500, 6.7 task/s, elapsed: 51s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 341/500, 6.7 task/s, elapsed: 51s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 342/500, 6.7 task/s, elapsed: 51s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 343/500, 6.7 task/s, elapsed: 51s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 344/500, 6.7 task/s, elapsed: 52s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 345/500, 6.7 task/s, elapsed: 52s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 346/500, 6.7 task/s, elapsed: 52s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 347/500, 6.7 task/s, elapsed: 52s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 348/500, 6.7 task/s, elapsed: 52s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 349/500, 6.7 task/s, elapsed: 52s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>         ] 350/500, 6.7 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 351/500, 6.7 task/s, elapsed: 53s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 352/500, 6.7 task/s, elapsed: 53s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 353/500, 6.7 task/s, elapsed: 53s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 354/500, 6.7 task/s, elapsed: 53s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 355/500, 6.7 task/s, elapsed: 53s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 356/500, 6.7 task/s, elapsed: 53s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 357/500, 6.7 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 358/500, 6.7 task/s, elapsed: 54s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 359/500, 6.7 task/s, elapsed: 54s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 360/500, 6.7 task/s, elapsed: 54s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 361/500, 6.7 task/s, elapsed: 54s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 362/500, 6.7 task/s, elapsed: 54s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 363/500, 6.7 task/s, elapsed: 54s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 364/500, 6.7 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 365/500, 6.7 task/s, elapsed: 55s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 366/500, 6.7 task/s, elapsed: 55s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 367/500, 6.7 task/s, elapsed: 55s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 368/500, 6.7 task/s, elapsed: 55s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 369/500, 6.7 task/s, elapsed: 55s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 370/500, 6.7 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 371/500, 6.7 task/s, elapsed: 56s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 372/500, 6.7 task/s, elapsed: 56s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 373/500, 6.7 task/s, elapsed: 56s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 374/500, 6.7 task/s, elapsed: 56s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 375/500, 6.7 task/s, elapsed: 56s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 376/500, 6.7 task/s, elapsed: 56s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 377/500, 6.7 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 378/500, 6.7 task/s, elapsed: 57s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 379/500, 6.7 task/s, elapsed: 57s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 380/500, 6.7 task/s, elapsed: 57s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 381/500, 6.7 task/s, elapsed: 57s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 382/500, 6.7 task/s, elapsed: 57s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 383/500, 6.7 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 384/500, 6.7 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 385/500, 6.7 task/s, elapsed: 58s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 386/500, 6.7 task/s, elapsed: 58s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 387/500, 6.7 task/s, elapsed: 58s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 388/500, 6.7 task/s, elapsed: 58s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 389/500, 6.7 task/s, elapsed: 58s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 390/500, 6.7 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 391/500, 6.7 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 392/500, 6.7 task/s, elapsed: 59s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 393/500, 6.7 task/s, elapsed: 59s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 394/500, 6.7 task/s, elapsed: 59s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 395/500, 6.7 task/s, elapsed: 59s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 396/500, 6.7 task/s, elapsed: 59s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 397/500, 6.7 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 398/500, 6.7 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 399/500, 6.7 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 400/500, 6.7 task/s, elapsed: 60s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 401/500, 6.7 task/s, elapsed: 60s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 402/500, 6.7 task/s, elapsed: 60s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 403/500, 6.7 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 404/500, 6.7 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 405/500, 6.7 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 406/500, 6.7 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 407/500, 6.7 task/s, elapsed: 61s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 408/500, 6.7 task/s, elapsed: 61s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 409/500, 6.7 task/s, elapsed: 61s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 410/500, 6.7 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 411/500, 6.7 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 412/500, 6.7 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 413/500, 6.7 task/s, elapsed: 62s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 414/500, 6.7 task/s, elapsed: 62s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 415/500, 6.7 task/s, elapsed: 62s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 416/500, 6.7 task/s, elapsed: 62s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 417/500, 6.7 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 418/500, 6.7 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 419/500, 6.7 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 420/500, 6.7 task/s, elapsed: 63s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 421/500, 6.7 task/s, elapsed: 63s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 422/500, 6.7 task/s, elapsed: 63s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 423/500, 6.7 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 424/500, 6.7 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 425/500, 6.7 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 426/500, 6.7 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 427/500, 6.7 task/s, elapsed: 64s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 428/500, 6.7 task/s, elapsed: 64s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 429/500, 6.7 task/s, elapsed: 64s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 430/500, 6.7 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 431/500, 6.7 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 432/500, 6.7 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 433/500, 6.7 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 434/500, 6.7 task/s, elapsed: 65s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 435/500, 6.7 task/s, elapsed: 65s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 436/500, 6.7 task/s, elapsed: 65s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 437/500, 6.7 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 438/500, 6.7 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 439/500, 6.7 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 440/500, 6.7 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 441/500, 6.7 task/s, elapsed: 66s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 442/500, 6.7 task/s, elapsed: 66s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 443/500, 6.7 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 444/500, 6.7 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 445/500, 6.7 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 446/500, 6.7 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 447/500, 6.7 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 448/500, 6.7 task/s, elapsed: 67s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 449/500, 6.7 task/s, elapsed: 67s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 450/500, 6.7 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 451/500, 6.7 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 452/500, 6.7 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 453/500, 6.7 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 454/500, 6.7 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 455/500, 6.7 task/s, elapsed: 68s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 456/500, 6.7 task/s, elapsed: 68s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 457/500, 6.7 task/s, elapsed: 68s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 458/500, 6.7 task/s, elapsed: 68s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 459/500, 6.7 task/s, elapsed: 68s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 460/500, 6.7 task/s, elapsed: 68s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 461/500, 6.7 task/s, elapsed: 68s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 462/500, 6.7 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 463/500, 6.7 task/s, elapsed: 69s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 464/500, 6.7 task/s, elapsed: 69s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 465/500, 6.7 task/s, elapsed: 69s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 466/500, 6.7 task/s, elapsed: 69s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 467/500, 6.7 task/s, elapsed: 69s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 468/500, 6.7 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 469/500, 6.7 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 470/500, 6.7 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 471/500, 6.7 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 472/500, 6.7 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 473/500, 6.7 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 474/500, 6.7 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 475/500, 6.7 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 476/500, 6.7 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 477/500, 6.7 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 478/500, 6.7 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 479/500, 6.7 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 480/500, 6.7 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 481/500, 6.7 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 482/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 483/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 484/500, 6.7 task/s, elapsed: 72s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 485/500, 6.7 task/s, elapsed: 72s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 486/500, 6.7 task/s, elapsed: 72s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 487/500, 6.7 task/s, elapsed: 72s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 488/500, 6.7 task/s, elapsed: 72s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 489/500, 6.7 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 490/500, 6.7 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 491/500, 6.7 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 492/500, 6.7 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 493/500, 6.7 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 494/500, 6.7 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 495/500, 6.7 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 496/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 497/500, 6.7 task/s, elapsed: 74s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 498/500, 6.7 task/s, elapsed: 74s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 499/500, 6.7 task/s, elapsed: 74s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 500/500, 6.7 task/s, elapsed: 74s, ETA:     0s2022-04-18 23:36:30,733 - mmseg - INFO - per class results:
2022-04-18 23:36:30,734 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|      road     | 87.81 | 90.12 |
|    sidewalk   | 47.95 | 81.91 |
|    building   | 87.69 | 95.13 |
|      wall     | 39.16 |  46.7 |
|     fence     |  7.8  |  8.72 |
|      pole     |  48.9 | 56.95 |
| traffic light | 54.46 | 66.81 |
|  traffic sign | 54.68 | 67.48 |
|   vegetation  | 84.39 |  93.5 |
|    terrain    |  0.0  |  0.0  |
|      sky      | 84.25 | 99.17 |
|     person    | 73.05 | 86.09 |
|     rider     | 47.15 | 72.85 |
|      car      | 87.15 | 95.52 |
|     truck     |  0.0  |  0.0  |
|      bus      | 57.42 | 76.92 |
|     train     |  0.0  |  0.0  |
|   motorcycle  | 55.05 | 65.89 |
|    bicycle    | 62.26 | 72.61 |
+---------------+-------+-------+
2022-04-18 23:36:30,734 - mmseg - INFO - Summary:
2022-04-18 23:36:30,735 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 88.93 | 51.53 | 61.92 |
+-------+-------+-------+
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:36:30,747 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-18 23:36:30,747 - mmseg - INFO - Iter [500/40000]	lr: 2.400e-05, eta: 6:51:19, time: 1.539, data_time: 0.013, memory: 9636, aAcc: 0.8893, mIoU: 0.5153, mAcc: 0.6192, IoU.road: 0.8781, IoU.sidewalk: 0.4795, IoU.building: 0.8769, IoU.wall: 0.3916, IoU.fence: 0.0780, IoU.pole: 0.4890, IoU.traffic light: 0.5446, IoU.traffic sign: 0.5468, IoU.vegetation: 0.8439, IoU.terrain: 0.0000, IoU.sky: 0.8425, IoU.person: 0.7305, IoU.rider: 0.4715, IoU.car: 0.8715, IoU.truck: 0.0000, IoU.bus: 0.5742, IoU.train: 0.0000, IoU.motorcycle: 0.5505, IoU.bicycle: 0.6226, Acc.road: 0.9012, Acc.sidewalk: 0.8191, Acc.building: 0.9513, Acc.wall: 0.4670, Acc.fence: 0.0872, Acc.pole: 0.5695, Acc.traffic light: 0.6681, Acc.traffic sign: 0.6748, Acc.vegetation: 0.9350, Acc.terrain: 0.0000, Acc.sky: 0.9917, Acc.person: 0.8609, Acc.rider: 0.7285, Acc.car: 0.9552, Acc.truck: 0.0000, Acc.bus: 0.7692, Acc.train: 0.0000, Acc.motorcycle: 0.6589, Acc.bicycle: 0.7261, decode.loss_seg: 0.1194, decode.acc_seg: 91.1563, src.loss_imnet_feat_dist: 0.1082, mix.decode.loss_seg: 0.1315, mix.decode.acc_seg: 88.9630
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:37:48,245 - mmseg - INFO - Iter [24050/40000]	lr: 2.393e-05, eta: 6:51:21, time: 3.921, data_time: 2.386, memory: 9636, decode.loss_seg: 0.1050, decode.acc_seg: 90.8718, src.loss_imnet_feat_dist: 0.1021, mix.decode.loss_seg: 0.1326, mix.decode.acc_seg: 88.6531
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:39:05,654 - mmseg - INFO - Iter [24100/40000]	lr: 2.385e-05, eta: 6:50:04, time: 1.548, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1085, decode.acc_seg: 90.7628, src.loss_imnet_feat_dist: 0.0999, mix.decode.loss_seg: 0.1353, mix.decode.acc_seg: 88.3195
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:40:20,800 - mmseg - INFO - Iter [24150/40000]	lr: 2.378e-05, eta: 6:48:45, time: 1.503, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1115, decode.acc_seg: 91.1900, src.loss_imnet_feat_dist: 0.1027, mix.decode.loss_seg: 0.1450, mix.decode.acc_seg: 88.4865
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:40:25,902 - mmseg - INFO - Exp name: 220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02
2022-04-18 23:40:25,902 - mmseg - INFO - Iter [11000/40000]	lr: 4.350e-05, eta: 1 day, 3:24:16, time: 3.391, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 24.1299, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.1465
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:43:13,987 - mmseg - INFO - Iter [11050/40000]	lr: 4.343e-05, eta: 1 day, 3:21:21, time: 3.362, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 18.8218, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.6636
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:46:01,134 - mmseg - INFO - Iter [11100/40000]	lr: 4.335e-05, eta: 1 day, 3:18:23, time: 3.343, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.4634, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.4941
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:46:35,090 - mmseg - INFO - Iter [24400/40000]	lr: 2.340e-05, eta: 6:42:10, time: 1.537, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1100, decode.acc_seg: 91.1239, src.loss_imnet_feat_dist: 0.1031, mix.decode.loss_seg: 0.1234, mix.decode.acc_seg: 88.9292
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:47:46,903 - mmseg - INFO - Iter [24450/40000]	lr: 2.333e-05, eta: 6:40:49, time: 1.436, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1030, decode.acc_seg: 91.0150, src.loss_imnet_feat_dist: 0.1070, mix.decode.loss_seg: 0.1348, mix.decode.acc_seg: 89.3312
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:48:58,089 - mmseg - INFO - Iter [24500/40000]	lr: 2.325e-05, eta: 6:39:28, time: 1.424, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1154, decode.acc_seg: 89.9395, src.loss_imnet_feat_dist: 0.1004, mix.decode.loss_seg: 0.1284, mix.decode.acc_seg: 88.2476
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:50:13,080 - mmseg - INFO - Iter [24550/40000]	lr: 2.318e-05, eta: 6:38:09, time: 1.500, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1186, decode.acc_seg: 90.6953, src.loss_imnet_feat_dist: 0.0960, mix.decode.loss_seg: 0.1181, mix.decode.acc_seg: 89.5322
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:51:29,129 - mmseg - INFO - Iter [24600/40000]	lr: 2.310e-05, eta: 6:36:51, time: 1.521, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1059, decode.acc_seg: 92.0040, src.loss_imnet_feat_dist: 0.0995, mix.decode.loss_seg: 0.1168, mix.decode.acc_seg: 89.8662
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:52:48,219 - mmseg - INFO - Iter [24650/40000]	lr: 2.303e-05, eta: 6:35:35, time: 1.582, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1025, decode.acc_seg: 91.3394, src.loss_imnet_feat_dist: 0.1048, mix.decode.loss_seg: 0.1287, mix.decode.acc_seg: 89.0162
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:54:04,940 - mmseg - INFO - Iter [24700/40000]	lr: 2.295e-05, eta: 6:34:17, time: 1.534, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1081, decode.acc_seg: 91.9166, src.loss_imnet_feat_dist: 0.1005, mix.decode.loss_seg: 0.1207, mix.decode.acc_seg: 90.0027
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:55:23,197 - mmseg - INFO - Iter [24750/40000]	lr: 2.288e-05, eta: 6:33:01, time: 1.565, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1038, decode.acc_seg: 90.5933, src.loss_imnet_feat_dist: 0.0964, mix.decode.loss_seg: 0.1227, mix.decode.acc_seg: 89.3484
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:56:39,096 - mmseg - INFO - Iter [24800/40000]	lr: 2.280e-05, eta: 6:31:42, time: 1.518, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1023, decode.acc_seg: 90.7997, src.loss_imnet_feat_dist: 0.0999, mix.decode.loss_seg: 0.1204, mix.decode.acc_seg: 88.7118
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:57:58,319 - mmseg - INFO - Iter [24850/40000]	lr: 2.273e-05, eta: 6:30:26, time: 1.584, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1205, decode.acc_seg: 91.0234, src.loss_imnet_feat_dist: 0.1083, mix.decode.loss_seg: 0.1302, mix.decode.acc_seg: 89.5242
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-18 23:59:16,179 - mmseg - INFO - Iter [24900/40000]	lr: 2.265e-05, eta: 6:29:09, time: 1.557, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1101, decode.acc_seg: 90.9124, src.loss_imnet_feat_dist: 0.0988, mix.decode.loss_seg: 0.1274, mix.decode.acc_seg: 88.8052
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:00:32,999 - mmseg - INFO - Iter [24950/40000]	lr: 2.258e-05, eta: 6:27:52, time: 1.536, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1153, decode.acc_seg: 91.6718, src.loss_imnet_feat_dist: 0.1030, mix.decode.loss_seg: 0.1404, mix.decode.acc_seg: 89.2211
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:01:45,350 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-19 00:01:45,350 - mmseg - INFO - Iter [25000/40000]	lr: 2.250e-05, eta: 6:26:31, time: 1.447, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1066, decode.acc_seg: 91.6281, src.loss_imnet_feat_dist: 0.1019, mix.decode.loss_seg: 0.1214, mix.decode.acc_seg: 89.8444
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:03:04,625 - mmseg - INFO - Iter [25050/40000]	lr: 2.243e-05, eta: 6:25:15, time: 1.586, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1121, decode.acc_seg: 91.0952, src.loss_imnet_feat_dist: 0.1008, mix.decode.loss_seg: 0.1183, mix.decode.acc_seg: 89.2624
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:04:21,338 - mmseg - INFO - Iter [25100/40000]	lr: 2.235e-05, eta: 6:23:58, time: 1.534, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1067, decode.acc_seg: 90.7349, src.loss_imnet_feat_dist: 0.0987, mix.decode.loss_seg: 0.1235, mix.decode.acc_seg: 89.2344
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:05:39,558 - mmseg - INFO - Iter [25150/40000]	lr: 2.228e-05, eta: 6:22:41, time: 1.564, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1201, decode.acc_seg: 90.7720, src.loss_imnet_feat_dist: 0.1005, mix.decode.loss_seg: 0.1341, mix.decode.acc_seg: 88.7223
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:06:52,964 - mmseg - INFO - Iter [25200/40000]	lr: 2.220e-05, eta: 6:21:21, time: 1.468, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1067, decode.acc_seg: 90.7906, src.loss_imnet_feat_dist: 0.0992, mix.decode.loss_seg: 0.1246, mix.decode.acc_seg: 89.4370
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:08:09,361 - mmseg - INFO - Iter [25250/40000]	lr: 2.213e-05, eta: 6:20:03, time: 1.528, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1073, decode.acc_seg: 89.8807, src.loss_imnet_feat_dist: 0.1012, mix.decode.loss_seg: 0.1300, mix.decode.acc_seg: 87.7306
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:09:19,885 - mmseg - INFO - Iter [25300/40000]	lr: 2.205e-05, eta: 6:18:42, time: 1.410, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1013, decode.acc_seg: 92.0253, src.loss_imnet_feat_dist: 0.0974, mix.decode.loss_seg: 0.1256, mix.decode.acc_seg: 90.1926
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:10:32,400 - mmseg - INFO - Iter [25350/40000]	lr: 2.198e-05, eta: 6:17:22, time: 1.450, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1046, decode.acc_seg: 90.4897, src.loss_imnet_feat_dist: 0.1018, mix.decode.loss_seg: 0.1346, mix.decode.acc_seg: 88.3341
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:11:18,974 - mmseg - INFO - Iter [11550/40000]	lr: 4.268e-05, eta: 1 day, 2:52:21, time: 3.452, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.3555, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 63.0338
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:14:05,908 - mmseg - INFO - Iter [11600/40000]	lr: 4.260e-05, eta: 1 day, 2:49:23, time: 3.339, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.9830, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.9336
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:16:56,308 - mmseg - INFO - Iter [11650/40000]	lr: 4.253e-05, eta: 1 day, 2:46:34, time: 3.408, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.4266, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, , mix.decode.acc_seg: 88.9132
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:18:03,247 - mmseg - INFO - Iter [25650/40000]	lr: 2.153e-05, eta: 6:09:31, time: 1.522, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1167, decode.acc_seg: 90.4334, src.loss_imnet_feat_dist: 0.1040, mix.decode.loss_seg: 0.1274, mix.decode.acc_seg: 88.6293
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:19:20,829 - mmseg - INFO - Iter [25700/40000]	lr: 2.145e-05, eta: 6:08:14, time: 1.552, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1155, decode.acc_seg: 90.6174, src.loss_imnet_feat_dist: 0.1004, mix.decode.loss_seg: 0.1303, mix.decode.acc_seg: 90.0434
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:20:40,159 - mmseg - INFO - Iter [25750/40000]	lr: 2.138e-05, eta: 6:06:58, time: 1.587, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1093, decode.acc_seg: 91.3806, src.loss_imnet_feat_dist: 0.0995, mix.decode.loss_seg: 0.1190, mix.decode.acc_seg: 90.3663
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:21:57,521 - mmseg - INFO - Iter [25800/40000]	lr: 2.130e-05, eta: 6:05:41, time: 1.547, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1101, decode.acc_seg: 91.3166, src.loss_imnet_feat_dist: 0.1027, mix.decode.loss_seg: 0.1360, mix.decode.acc_seg: 89.0809
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:23:16,703 - mmseg - INFO - Iter [25850/40000]	lr: 2.123e-05, eta: 6:04:25, time: 1.584, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1094, decode.acc_seg: 91.7921, src.loss_imnet_feat_dist: 0.1052, mix.decode.loss_seg: 0.1272, mix.decode.acc_seg: 89.4410
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:24:31,697 - mmseg - INFO - Iter [25900/40000]	lr: 2.115e-05, eta: 6:03:06, time: 1.500, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1060, decode.acc_seg: 91.7638, src.loss_imnet_feat_dist: 0.0989, mix.decode.loss_seg: 0.1203, mix.decode.acc_seg: 89.5506
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:25:45,298 - mmseg - INFO - Iter [25950/40000]	lr: 2.108e-05, eta: 6:01:47, time: 1.472, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1099, decode.acc_seg: 91.1628, src.loss_imnet_feat_dist: 0.1047, mix.decode.loss_seg: 0.1229, mix.decode.acc_seg: 88.9428
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:26:59,551 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-19 00:26:59,551 - mmseg - INFO - Iter [26000/40000]	lr: 2.100e-05, eta: 6:00:28, time: 1.485, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1101, decode.acc_seg: 90.9768, src.loss_imnet_feat_dist: 0.1011, mix.decode.loss_seg: 0.1294, mix.decode.acc_seg: 89.1807
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:28:16,833 - mmseg - INFO - Iter [26050/40000]	lr: 2.093e-05, eta: 5:59:11, time: 1.546, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1075, decode.acc_seg: 90.1086, src.loss_imnet_feat_dist: 0.1046, mix.decode.loss_seg: 0.1262, mix.decode.acc_seg: 89.2055
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:29:31,702 - mmseg - INFO - Iter [26100/40000]	lr: 2.085e-05, eta: 5:57:52, time: 1.497, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1088, decode.acc_seg: 90.3977, src.loss_imnet_feat_dist: 0.0986, mix.decode.loss_seg: 0.1215, mix.decode.acc_seg: 89.4867
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:30:51,519 - mmseg - INFO - Iter [26150/40000]	lr: 2.078e-05, eta: 5:56:36, time: 1.596, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1078, decode.acc_seg: 90.7714, src.loss_imnet_feat_dist: 0.1002, mix.decode.loss_seg: 0.1360, mix.decode.acc_seg: 88.9745
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:32:10,506 - mmseg - INFO - Iter [26200/40000]	lr: 2.070e-05, eta: 5:55:20, time: 1.580, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1188, decode.acc_seg: 90.6793, src.loss_imnet_feat_dist: 0.0999, mix.decode.loss_seg: 0.1680, mix.decode.acc_seg: 88.1947
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:33:29,450 - mmseg - INFO - Iter [26250/40000]	lr: 2.063e-05, eta: 5:54:04, time: 1.579, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1098, decode.acc_seg: 91.3480, src.loss_imnet_feat_dist: 0.0980, mix.decode.loss_seg: 0.1327, mix.decode.acc_seg: 88.9928
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:34:45,981 - mmseg - INFO - Iter [26300/40000]	lr: 2.055e-05, eta: 5:52:46, time: 1.531, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1109, decode.acc_seg: 91.5136, src.loss_imnet_feat_dist: 0.1027, mix.decode.loss_seg: 0.1261, mix.decode.acc_seg: 89.5143
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:36:05,273 - mmseg - INFO - Iter [26350/40000]	lr: 2.048e-05, eta: 5:51:30, time: 1.586, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1116, decode.acc_seg: 91.0579, src.loss_imnet_feat_dist: 0.0992, mix.decode.loss_seg: 0.1207, mix.decode.acc_seg: 89.9484
[                                                  ] 0/500, elapsed: 0s, ETA:
[                                 ] 1/500, 1.4 task/s, elapsed: 1s, ETA:   369s
[                                 ] 2/500, 1.8 task/s, elapsed: 1s, ETA:   280s
[                                 ] 3/500, 2.0 task/s, elapsed: 2s, ETA:   251s
[                                 ] 4/500, 2.1 task/s, elapsed: 2s, ETA:   235s
[                                 ] 5/500, 2.2 task/s, elapsed: 2s, ETA:   226s
[                                 ] 6/500, 2.2 task/s, elapsed: 3s, ETA:   220s
[                                 ] 7/500, 2.3 task/s, elapsed: 3s, ETA:   215s
[                                 ] 8/500, 2.3 task/s, elapsed: 3s, ETA:   212s
[                                 ] 9/500, 2.3 task/s, elapsed: 4s, ETA:   209s
[                                ] 10/500, 2.4 task/s, elapsed: 4s, ETA:   207s
[                                ] 11/500, 2.4 task/s, elapsed: 5s, ETA:   205s
[                                ] 12/500, 2.4 task/s, elapsed: 5s, ETA:   203s
[                                ] 13/500, 2.4 task/s, elapsed: 5s, ETA:   202s
[                                ] 14/500, 2.4 task/s, elapsed: 6s, ETA:   200s
[                                ] 15/500, 2.4 task/s, elapsed: 6s, ETA:   199s
[>                               ] 16/500, 2.4 task/s, elapsed: 7s, ETA:   198s
[>                               ] 17/500, 2.5 task/s, elapsed: 7s, ETA:   197s
[>                               ] 18/500, 2.5 task/s, elapsed: 7s, ETA:   196s
[>                               ] 19/500, 2.5 task/s, elapsed: 8s, ETA:   195s
[>                               ] 20/500, 2.5 task/s, elapsed: 8s, ETA:   194s
[>                               ] 21/500, 2.5 task/s, elapsed: 8s, ETA:   193s
[>                               ] 22/500, 2.5 task/s, elapsed: 9s, ETA:   193s
[>                               ] 23/500, 2.5 task/s, elapsed: 9s, ETA:   192s
[>                              ] 24/500, 2.5 task/s, elapsed: 10s, ETA:   191s
[>                              ] 25/500, 2.5 task/s, elapsed: 10s, ETA:   190s
[>                              ] 26/500, 2.5 task/s, elapsed: 10s, ETA:   190s
[>                              ] 27/500, 2.5 task/s, elapsed: 11s, ETA:   189s
[>                              ] 28/500, 2.5 task/s, elapsed: 11s, ETA:   189s
[>                              ] 29/500, 2.5 task/s, elapsed: 12s, ETA:   188s
[>                              ] 30/500, 2.5 task/s, elapsed: 12s, ETA:   187s
[>                              ] 31/500, 2.5 task/s, elapsed: 12s, ETA:   187s
[>                              ] 32/500, 2.5 task/s, elapsed: 13s, ETA:   186s
[>>                             ] 33/500, 2.5 task/s, elapsed: 13s, ETA:   186s
[>>                             ] 34/500, 2.5 task/s, elapsed: 14s, ETA:   185s
[>>                             ] 35/500, 2.5 task/s, elapsed: 14s, ETA:   185s
[>>                             ] 36/500, 2.5 task/s, elapsed: 14s, ETA:   184s
[>>                             ] 37/500, 2.5 task/s, elapsed: 15s, ETA:   184s
[>>                             ] 38/500, 2.5 task/s, elapsed: 15s, ETA:   183s
[>>                             ] 39/500, 2.5 task/s, elapsed: 15s, ETA:   183s
[>>                             ] 40/500, 2.5 task/s, elapsed: 16s, ETA:   182s
[>>                             ] 41/500, 2.5 task/s, elapsed: 16s, ETA:   182s
[>>                             ] 42/500, 2.5 task/s, elapsed: 17s, ETA:   181s
[>>                             ] 43/500, 2.5 task/s, elapsed: 17s, ETA:   181s
[>>                             ] 44/500, 2.5 task/s, elapsed: 17s, ETA:   180s
[>>                             ] 45/500, 2.5 task/s, elapsed: 18s, ETA:   180s
[>>                             ] 46/500, 2.5 task/s, elapsed: 18s, ETA:   179s
[>>                             ] 47/500, 2.5 task/s, elapsed: 19s, ETA:   179s
[>>                             ] 48/500, 2.5 task/s, elapsed: 19s, ETA:   178s
[>>>                            ] 49/500, 2.5 task/s, elapsed: 19s, ETA:   178s
[>>>                            ] 50/500, 2.5 task/s, elapsed: 20s, ETA:   177s
[>>>                            ] 51/500, 2.5 task/s, elapsed: 20s, ETA:   177s
[>>>                            ] 52/500, 2.5 task/s, elapsed: 20s, ETA:   176s
[>>>                            ] 53/500, 2.5 task/s, elapsed: 21s, ETA:   176s
[>>>                            ] 54/500, 2.5 task/s, elapsed: 21s, ETA:   175s
[>>>                            ] 55/500, 2.5 task/s, elapsed: 22s, ETA:   175s
[>>>                            ] 56/500, 2.5 task/s, elapsed: 22s, ETA:   175s
[>>>                            ] 57/500, 2.5 task/s, elapsed: 22s, ETA:   174s
[>>>                            ] 58/500, 2.5 task/s, elapsed: 23s, ETA:   174s
[>>>                            ] 59/500, 2.5 task/s, elapsed: 23s, ETA:   173s
[>>>                            ] 60/500, 2.5 task/s, elapsed: 24s, ETA:   173s
[>>>                            ] 61/500, 2.5 task/s, elapsed: 24s, ETA:   172s
[>>>                            ] 62/500, 2.5 task/s, elapsed: 24s, ETA:   172s
[>>>                            ] 63/500, 2.5 task/s, elapsed: 25s, ETA:   172s
[>>>                            ] 64/500, 2.5 task/s, elapsed: 25s, ETA:   171s
[>>>>                           ] 65/500, 2.5 task/s, elapsed: 26s, ETA:   171s
[>>>>                           ] 66/500, 2.5 task/s, elapsed: 26s, ETA:   170s
[>>>>                           ] 67/500, 2.5 task/s, elapsed: 26s, ETA:   170s
[>>>>                           ] 68/500, 2.6 task/s, elapsed: 27s, ETA:   169s
[>>>>                           ] 69/500, 2.6 task/s, elapsed: 27s, ETA:   169s
[>>>>                           ] 70/500, 2.6 task/s, elapsed: 27s, ETA:   169s
[>>>>                           ] 71/500, 2.6 task/s, elapsed: 28s, ETA:   168s
[>>>>                           ] 72/500, 2.6 task/s, elapsed: 28s, ETA:   168s
[>>>>                           ] 73/500, 2.6 task/s, elapsed: 29s, ETA:   167s
[>>>>                           ] 74/500, 2.6 task/s, elapsed: 29s, ETA:   167s
[>>>>                           ] 75/500, 2.6 task/s, elapsed: 29s, ETA:   166s
[>>>>                           ] 76/500, 2.6 task/s, elapsed: 30s, ETA:   166s
[>>>>                           ] 77/500, 2.6 task/s, elapsed: 30s, ETA:   166s
[>>>>                           ] 78/500, 2.6 task/s, elapsed: 31s, ETA:   165s
[>>>>                           ] 79/500, 2.6 task/s, elapsed: 31s, ETA:   165s
[>>>>                           ] 80/500, 2.6 task/s, elapsed: 31s, ETA:   164s
[>>>>>                          ] 81/500, 2.6 task/s, elapsed: 32s, ETA:   164s
[>>>>>                          ] 82/500, 2.6 task/s, elapsed: 32s, ETA:   164s
[>>>>>                          ] 83/500, 2.6 task/s, elapsed: 32s, ETA:   163s
[>>>>>                          ] 84/500, 2.6 task/s, elapsed: 33s, ETA:   163s
[>>>>>                          ] 85/500, 2.6 task/s, elapsed: 33s, ETA:   162s
[>>>>>                          ] 86/500, 2.6 task/s, elapsed: 34s, ETA:   162s
[>>>>>                          ] 87/500, 2.6 task/s, elapsed: 34s, ETA:   161s
[>>>>>                          ] 88/500, 2.6 task/s, elapsed: 34s, ETA:   161s
[>>>>>                          ] 89/500, 2.6 task/s, elapsed: 35s, ETA:   161s
[>>>>>                          ] 90/500, 2.6 task/s, elapsed: 35s, ETA:   160s
[>>>>>                          ] 91/500, 2.6 task/s, elapsed: 36s, ETA:   160s
[>>>>>                          ] 92/500, 2.6 task/s, elapsed: 36s, ETA:   159s
[>>>>>                          ] 93/500, 2.6 task/s, elapsed: 36s, ETA:   159s
[>>>>>                          ] 94/500, 2.6 task/s, elapsed: 37s, ETA:   159s
[>>>>>                          ] 95/500, 2.6 task/s, elapsed: 37s, ETA:   158s
[>>>>>                          ] 96/500, 2.6 task/s, elapsed: 38s, ETA:   158s
[>>>>>>                         ] 97/500, 2.6 task/s, elapsed: 38s, ETA:   157s
[>>>>>>                         ] 98/500, 2.6 task/s, elapsed: 38s, ETA:   157s
[>>>>>>                         ] 99/500, 2.6 task/s, elapsed: 39s, ETA:   157s
[>>>>>>                        ] 100/500, 2.6 task/s, elapsed: 39s, ETA:   156s
[>>>>>>                        ] 101/500, 2.6 task/s, elapsed: 39s, ETA:   156s
[>>>>>>                        ] 102/500, 2.6 task/s, elapsed: 40s, ETA:   155s
[>>>>>>                        ] 103/500, 2.6 task/s, elapsed: 40s, ETA:   155s
[>>>>>>                        ] 104/500, 2.6 task/s, elapsed: 41s, ETA:   155s
[>>>>>>                        ] 105/500, 2.6 task/s, elapsed: 41s, ETA:   154s
[>>>>>>                        ] 106/500, 2.6 task/s, elapsed: 41s, ETA:   154s
[>>>>>>                        ] 107/500, 2.6 task/s, elapsed: 42s, ETA:   153s
[>>>>>>                        ] 108/500, 2.6 task/s, elapsed: 42s, ETA:   153s
[>>>>>>                        ] 109/500, 2.6 task/s, elapsed: 43s, ETA:   153s
[>>>>>>                        ] 110/500, 2.6 task/s, elapsed: 43s, ETA:   152s
[>>>>>>                        ] 111/500, 2.6 task/s, elapsed: 43s, ETA:   152s
[>>>>>>                        ] 112/500, 2.6 task/s, elapsed: 44s, ETA:   151s
[>>>>>>                        ] 113/500, 2.6 task/s, elapsed: 44s, ETA:   151s
[>>>>>>                        ] 114/500, 2.6 task/s, elapsed: 44s, ETA:   151s
[>>>>>>                        ] 115/500, 2.6 task/s, elapsed: 45s, ETA:   150s
[>>>>>>                        ] 116/500, 2.6 task/s, elapsed: 45s, ETA:   150s
[>>>>>>>                       ] 117/500, 2.6 task/s, elapsed: 46s, ETA:   149s
[>>>>>>>                       ] 118/500, 2.6 task/s, elapsed: 46s, ETA:   149s
[>>>>>>>                       ] 119/500, 2.6 task/s, elapsed: 46s, ETA:   149s
[>>>>>>>                       ] 120/500, 2.6 task/s, elapsed: 47s, ETA:   148s
[>>>>>>>                       ] 121/500, 2.6 task/s, elapsed: 47s, ETA:   148s
[>>>>>>>                       ] 122/500, 2.6 task/s, elapsed: 48s, ETA:   147s
[>>>>>>>                       ] 123/500, 2.6 task/s, elapsed: 48s, ETA:   147s
[>>>>>>>                       ] 124/500, 2.6 task/s, elapsed: 48s, ETA:   147s
[>>>>>>>                       ] 125/500, 2.6 task/s, elapsed: 49s, ETA:   146s
[>>>>>>>                       ] 126/500, 2.6 task/s, elapsed: 49s, ETA:   146s
[>>>>>>>                       ] 127/500, 2.6 task/s, elapsed: 50s, ETA:   145s
[>>>>>>>                       ] 128/500, 2.6 task/s, elapsed: 50s, ETA:   145s
[>>>>>>>                       ] 129/500, 2.6 task/s, elapsed: 50s, ETA:   145s
[>>>>>>>                       ] 130/500, 2.6 task/s, elapsed: 51s, ETA:   144s
[>>>>>>>                       ] 131/500, 2.6 task/s, elapsed: 51s, ETA:   144s
[>>>>>>>                       ] 132/500, 2.6 task/s, elapsed: 51s, ETA:   143s
[>>>>>>>                       ] 133/500, 2.6 task/s, elapsed: 52s, ETA:   143s
[>>>>>>>>                      ] 134/500, 2.6 task/s, elapsed: 52s, ETA:   143s
[>>>>>>>>                      ] 135/500, 2.6 task/s, elapsed: 53s, ETA:   142s
[>>>>>>>>                      ] 136/500, 2.6 task/s, elapsed: 53s, ETA:   142s
[>>>>>>>>                      ] 137/500, 2.6 task/s, elapsed: 53s, ETA:   141s
[>>>>>>>>                      ] 138/500, 2.6 task/s, elapsed: 54s, ETA:   141s
[>>>>>>>>                      ] 139/500, 2.6 task/s, elapsed: 54s, ETA:   141s
[>>>>>>>>                      ] 140/500, 2.6 task/s, elapsed: 55s, ETA:   140s
[>>>>>>>>                      ] 141/500, 2.6 task/s, elapsed: 55s, ETA:   140s
[>>>>>>>>                      ] 142/500, 2.6 task/s, elapsed: 55s, ETA:   140s
[>>>>>>>>                      ] 143/500, 2.6 task/s, elapsed: 56s, ETA:   139s
[>>>>>>>>                      ] 144/500, 2.6 task/s, elapsed: 56s, ETA:   139s
[>>>>>>>>                      ] 145/500, 2.6 task/s, elapsed: 56s, ETA:   138s
[>>>>>>>>                      ] 146/500, 2.6 task/s, elapsed: 57s, ETA:   138s
[>>>>>>>>                      ] 147/500, 2.6 task/s, elapsed: 57s, ETA:   138s
[>>>>>>>>                      ] 148/500, 2.6 task/s, elapsed: 58s, ETA:   137s
[>>>>>>>>                      ] 149/500, 2.6 task/s, elapsed: 58s, ETA:   137s
[>>>>>>>>>                     ] 150/500, 2.6 task/s, elapsed: 58s, ETA:   136s
[>>>>>>>>>                     ] 151/500, 2.6 task/s, elapsed: 59s, ETA:   136s
[>>>>>>>>>                     ] 152/500, 2.6 task/s, elapsed: 59s, ETA:   136s
[>>>>>>>>>                     ] 153/500, 2.6 task/s, elapsed: 60s, ETA:   135s
[>>>>>>>>>                     ] 154/500, 2.6 task/s, elapsed: 60s, ETA:   135s
[>>>>>>>>>                     ] 155/500, 2.6 task/s, elapsed: 60s, ETA:   135s
[>>>>>>>>>                     ] 156/500, 2.6 task/s, elapsed: 61s, ETA:   134s
[>>>>>>>>>                     ] 157/500, 2.6 task/s, elapsed: 61s, ETA:   134s
[>>>>>>>>>                     ] 158/500, 2.6 task/s, elapsed: 62s, ETA:   133s
[>>>>>>>>>                     ] 159/500, 2.6 task/s, elapsed: 62s, ETA:   133s
[>>>>>>>>>                     ] 160/500, 2.6 task/s, elapsed: 62s, ETA:   133s
[>>>>>>>>>                     ] 161/500, 2.6 task/s, elapsed: 63s, ETA:   132s
[>>>>>>>>>                     ] 162/500, 2.6 task/s, elapsed: 63s, ETA:   132s
[>>>>>>>>>                     ] 163/500, 2.6 task/s, elapsed: 64s, ETA:   131s
[>>>>>>>>>                     ] 164/500, 2.6 task/s, elapsed: 64s, ETA:   131s
[>>>>>>>>>                     ] 165/500, 2.6 task/s, elapsed: 64s, ETA:   131s
[>>>>>>>>>                     ] 166/500, 2.6 task/s, elapsed: 65s, ETA:   130s
[>>>>>>>>>>                    ] 167/500, 2.6 task/s, elapsed: 65s, ETA:   130s
[>>>>>>>>>>                    ] 168/500, 2.6 task/s, elapsed: 66s, ETA:   130s
[>>>>>>>>>>                    ] 169/500, 2.6 task/s, elapsed: 66s, ETA:   129s
[>>>>>>>>>>                    ] 170/500, 2.6 task/s, elapsed: 66s, ETA:   129s
[>>>>>>>>>>                    ] 171/500, 2.6 task/s, elapsed: 67s, ETA:   128s
[>>>>>>>>>>                    ] 172/500, 2.6 task/s, elapsed: 67s, ETA:   128s
[>>>>>>>>>>                    ] 173/500, 2.6 task/s, elapsed: 68s, ETA:   128s
[>>>>>>>>>>                    ] 174/500, 2.6 task/s, elapsed: 68s, ETA:   127s
[>>>>>>>>>>                    ] 175/500, 2.6 task/s, elapsed: 68s, ETA:   127s
[>>>>>>>>>>                    ] 176/500, 2.6 task/s, elapsed: 69s, ETA:   126s
[>>>>>>>>>>                    ] 177/500, 2.6 task/s, elapsed: 69s, ETA:   126s
[>>>>>>>>>>                    ] 178/500, 2.6 task/s, elapsed: 69s, ETA:   126s
[>>>>>>>>>>                    ] 179/500, 2.6 task/s, elapsed: 70s, ETA:   125s
[>>>>>>>>>>                    ] 180/500, 2.6 task/s, elapsed: 70s, ETA:   125s
[>>>>>>>>>>                    ] 181/500, 2.6 task/s, elapsed: 71s, ETA:   125s
[>>>>>>>>>>                    ] 182/500, 2.6 task/s, elapsed: 71s, ETA:   124s
[>>>>>>>>>>                    ] 183/500, 2.6 task/s, elapsed: 71s, ETA:   124s
[>>>>>>>>>>>                   ] 184/500, 2.6 task/s, elapsed: 72s, ETA:   123s
[>>>>>>>>>>>                   ] 185/500, 2.6 task/s, elapsed: 72s, ETA:   123s
[>>>>>>>>>>>                   ] 186/500, 2.6 task/s, elapsed: 73s, ETA:   123s
[>>>>>>>>>>>                   ] 187/500, 2.6 task/s, elapsed: 73s, ETA:   122s
[>>>>>>>>>>>                   ] 188/500, 2.6 task/s, elapsed: 73s, ETA:   122s
[>>>>>>>>>>>                   ] 189/500, 2.6 task/s, elapsed: 74s, ETA:   121s
[>>>>>>>>>>>                   ] 190/500, 2.6 task/s, elapsed: 74s, ETA:   121s
[>>>>>>>>>>>                   ] 191/500, 2.6 task/s, elapsed: 75s, ETA:   121s
[>>>>>>>>>>>                   ] 192/500, 2.6 task/s, elapsed: 75s, ETA:   120s
[>>>>>>>>>>>                   ] 193/500, 2.6 task/s, elapsed: 75s, ETA:   120s
[>>>>>>>>>>>                   ] 194/500, 2.6 task/s, elapsed: 76s, ETA:   120s
[>>>>>>>>>>>                   ] 195/500, 2.6 task/s, elapsed: 76s, ETA:   119s
[>>>>>>>>>>>                   ] 196/500, 2.6 task/s, elapsed: 77s, ETA:   119s
[>>>>>>>>>>>                   ] 197/500, 2.6 task/s, elapsed: 77s, ETA:   118s
[>>>>>>>>>>>                   ] 198/500, 2.6 task/s, elapsed: 77s, ETA:   118s
[>>>>>>>>>>>                   ] 199/500, 2.6 task/s, elapsed: 78s, ETA:   118s
[>>>>>>>>>>>>                  ] 200/500, 2.6 task/s, elapsed: 78s, ETA:   117s
[>>>>>>>>>>>>                  ] 201/500, 2.6 task/s, elapsed: 79s, ETA:   117s
[>>>>>>>>>>>>                  ] 202/500, 2.6 task/s, elapsed: 79s, ETA:   117s
[>>>>>>>>>>>>                  ] 203/500, 2.6 task/s, elapsed: 79s, ETA:   116s
[>>>>>>>>>>>>                  ] 204/500, 2.6 task/s, elapsed: 80s, ETA:   116s
[>>>>>>>>>>>>                  ] 205/500, 2.6 task/s, elapsed: 80s, ETA:   115s
[>>>>>>>>>>>>                  ] 206/500, 2.6 task/s, elapsed: 81s, ETA:   115s
[>>>>>>>>>>>>                  ] 207/500, 2.6 task/s, elapsed: 81s, ETA:   115s
[>>>>>>>>>>>>                  ] 208/500, 2.6 task/s, elapsed: 81s, ETA:   114s
[>>>>>>>>>>>>                  ] 209/500, 2.6 task/s, elapsed: 82s, ETA:   114s
[>>>>>>>>>>>>                  ] 210/500, 2.6 task/s, elapsed: 82s, ETA:   114s
[>>>>>>>>>>>>                  ] 211/500, 2.6 task/s, elapsed: 83s, ETA:   113s
[>>>>>>>>>>>>                  ] 212/500, 2.6 task/s, elapsed: 83s, ETA:   113s
[>>>>>>>>>>>>                  ] 213/500, 2.6 task/s, elapsed: 83s, ETA:   112s
[>>>>>>>>>>>>                  ] 214/500, 2.6 task/s, elapsed: 84s, ETA:   112s
[>>>>>>>>>>>>                  ] 215/500, 2.6 task/s, elapsed: 84s, ETA:   112s
[>>>>>>>>>>>>                  ] 216/500, 2.6 task/s, elapsed: 85s, ETA:   111s
[>>>>>>>>>>>>>                 ] 217/500, 2.5 task/s, elapsed: 85s, ETA:   111s
[>>>>>>>>>>>>>                 ] 218/500, 2.5 task/s, elapsed: 85s, ETA:   111s
[>>>>>>>>>>>>>                 ] 219/500, 2.5 task/s, elapsed: 86s, ETA:   110s
[>>>>>>>>>>>>>                 ] 220/500, 2.5 task/s, elapsed: 86s, ETA:   110s
[>>>>>>>>>>>>>                 ] 221/500, 2.5 task/s, elapsed: 87s, ETA:   109s
[>>>>>>>>>>>>>                 ] 222/500, 2.5 task/s, elapsed: 87s, ETA:   109s
[>>>>>>>>>>>>>                 ] 223/500, 2.5 task/s, elapsed: 88s, ETA:   109s
[>>>>>>>>>>>>>                 ] 224/500, 2.5 task/s, elapsed: 88s, ETA:   108s
[>>>>>>>>>>>>>                 ] 225/500, 2.5 task/s, elapsed: 88s, ETA:   108s
[>>>>>>>>>>>>>                 ] 226/500, 2.5 task/s, elapsed: 89s, ETA:   108s
[>>>>>>>>>>>>>                 ] 227/500, 2.5 task/s, elapsed: 89s, ETA:   107s
[>>>>>>>>>>>>>                 ] 228/500, 2.5 task/s, elapsed: 90s, ETA:   107s
[>>>>>>>>>>>>>                 ] 229/500, 2.5 task/s, elapsed: 90s, ETA:   106s
[>>>>>>>>>>>>>                 ] 230/500, 2.5 task/s, elapsed: 90s, ETA:   106s
[>>>>>>>>>>>>>                 ] 231/500, 2.5 task/s, elapsed: 91s, ETA:   106s
[>>>>>>>>>>>>>                 ] 232/500, 2.5 task/s, elapsed: 91s, ETA:   105s
[>>>>>>>>>>>>>                 ] 233/500, 2.5 task/s, elapsed: 92s, ETA:   105s
[>>>>>>>>>>>>>>                ] 234/500, 2.5 task/s, elapsed: 92s, ETA:   104s
[>>>>>>>>>>>>>>                ] 235/500, 2.5 task/s, elapsed: 92s, ETA:   104s
[>>>>>>>>>>>>>>                ] 236/500, 2.5 task/s, elapsed: 93s, ETA:   104s
[>>>>>>>>>>>>>>                ] 237/500, 2.5 task/s, elapsed: 93s, ETA:   103s
[>>>>>>>>>>>>>>                ] 238/500, 2.5 task/s, elapsed: 94s, ETA:   103s
[>>>>>>>>>>>>>>                ] 239/500, 2.5 task/s, elapsed: 94s, ETA:   103s
[>>>>>>>>>>>>>>                ] 240/500, 2.5 task/s, elapsed: 94s, ETA:   102s
[>>>>>>>>>>>>>>                ] 241/500, 2.5 task/s, elapsed: 95s, ETA:   102s
[>>>>>>>>>>>>>>                ] 242/500, 2.5 task/s, elapsed: 95s, ETA:   101s
[>>>>>>>>>>>>>>                ] 243/500, 2.5 task/s, elapsed: 96s, ETA:   101s
[>>>>>>>>>>>>>>                ] 244/500, 2.5 task/s, elapsed: 96s, ETA:   101s
[>>>>>>>>>>>>>>                ] 245/500, 2.5 task/s, elapsed: 96s, ETA:   100s
[>>>>>>>>>>>>>>                ] 246/500, 2.5 task/s, elapsed: 97s, ETA:   100s
[>>>>>>>>>>>>>>                ] 247/500, 2.5 task/s, elapsed: 97s, ETA:    99s
[>>>>>>>>>>>>>>                ] 248/500, 2.5 task/s, elapsed: 98s, ETA:    99s
[>>>>>>>>>>>>>>                ] 249/500, 2.5 task/s, elapsed: 98s, ETA:    99s
[>>>>>>>>>>>>>>>               ] 250/500, 2.5 task/s, elapsed: 98s, ETA:    98s
[>>>>>>>>>>>>>>>               ] 251/500, 2.5 task/s, elapsed: 99s, ETA:    98s
[>>>>>>>>>>>>>>>               ] 252/500, 2.5 task/s, elapsed: 99s, ETA:    98s
[>>>>>>>>>>>>>>               ] 253/500, 2.5 task/s, elapsed: 100s, ETA:    97s
[>>>>>>>>>>>>>>               ] 254/500, 2.5 task/s, elapsed: 100s, ETA:    97s
[>>>>>>>>>>>>>>               ] 255/500, 2.5 task/s, elapsed: 100s, ETA:    96s
[>>>>>>>>>>>>>>               ] 256/500, 2.5 task/s, elapsed: 101s, ETA:    96s
[>>>>>>>>>>>>>>               ] 257/500, 2.5 task/s, elapsed: 101s, ETA:    96s
[>>>>>>>>>>>>>>               ] 258/500, 2.5 task/s, elapsed: 102s, ETA:    95s
[>>>>>>>>>>>>>>>              ] 259/500, 2.5 task/s, elapsed: 102s, ETA:    95s
[>>>>>>>>>>>>>>>              ] 260/500, 2.5 task/s, elapsed: 102s, ETA:    94s
[>>>>>>>>>>>>>>>              ] 261/500, 2.5 task/s, elapsed: 103s, ETA:    94s
[>>>>>>>>>>>>>>>              ] 262/500, 2.5 task/s, elapsed: 103s, ETA:    94s
[>>>>>>>>>>>>>>>              ] 263/500, 2.5 task/s, elapsed: 104s, ETA:    93s
[>>>>>>>>>>>>>>>              ] 264/500, 2.5 task/s, elapsed: 104s, ETA:    93s
[>>>>>>>>>>>>>>>              ] 265/500, 2.5 task/s, elapsed: 104s, ETA:    92s
[>>>>>>>>>>>>>>>              ] 266/500, 2.5 task/s, elapsed: 105s, ETA:    92s
[>>>>>>>>>>>>>>>              ] 267/500, 2.5 task/s, elapsed: 105s, ETA:    92s
[>>>>>>>>>>>>>>>              ] 268/500, 2.5 task/s, elapsed: 105s, ETA:    91s
[>>>>>>>>>>>>>>>              ] 269/500, 2.5 task/s, elapsed: 106s, ETA:    91s
[>>>>>>>>>>>>>>>              ] 270/500, 2.5 task/s, elapsed: 106s, ETA:    91s
[>>>>>>>>>>>>>>>              ] 271/500, 2.5 task/s, elapsed: 107s, ETA:    90s
[>>>>>>>>>>>>>>>              ] 272/500, 2.5 task/s, elapsed: 107s, ETA:    90s
[>>>>>>>>>>>>>>>              ] 273/500, 2.5 task/s, elapsed: 107s, ETA:    89s
[>>>>>>>>>>>>>>>              ] 274/500, 2.5 task/s, elapsed: 108s, ETA:    89s
[>>>>>>>>>>>>>>>              ] 275/500, 2.5 task/s, elapsed: 108s, ETA:    89s
[>>>>>>>>>>>>>>>>             ] 276/500, 2.5 task/s, elapsed: 109s, ETA:    88s
[>>>>>>>>>>>>>>>>             ] 277/500, 2.5 task/s, elapsed: 109s, ETA:    88s
[>>>>>>>>>>>>>>>>             ] 278/500, 2.5 task/s, elapsed: 109s, ETA:    87s
[>>>>>>>>>>>>>>>>             ] 279/500, 2.5 task/s, elapsed: 110s, ETA:    87s
[>>>>>>>>>>>>>>>>             ] 280/500, 2.5 task/s, elapsed: 110s, ETA:    87s
[>>>>>>>>>>>>>>>>             ] 281/500, 2.5 task/s, elapsed: 111s, ETA:    86s
[>>>>>>>>>>>>>>>>             ] 282/500, 2.5 task/s, elapsed: 111s, ETA:    86s
[>>>>>>>>>>>>>>>>             ] 283/500, 2.5 task/s, elapsed: 111s, ETA:    85s
[>>>>>>>>>>>>>>>>             ] 284/500, 2.5 task/s, elapsed: 112s, ETA:    85s
[>>>>>>>>>>>>>>>>             ] 285/500, 2.5 task/s, elapsed: 112s, ETA:    85s
[>>>>>>>>>>>>>>>>             ] 286/500, 2.5 task/s, elapsed: 113s, ETA:    84s
[>>>>>>>>>>>>>>>>             ] 287/500, 2.5 task/s, elapsed: 113s, ETA:    84s
[>>>>>>>>>>>>>>>>             ] 288/500, 2.5 task/s, elapsed: 113s, ETA:    83s
[>>>>>>>>>>>>>>>>             ] 289/500, 2.5 task/s, elapsed: 114s, ETA:    83s
[>>>>>>>>>>>>>>>>             ] 290/500, 2.5 task/s, elapsed: 114s, ETA:    83s
[>>>>>>>>>>>>>>>>             ] 291/500, 2.5 task/s, elapsed: 114s, ETA:    82s
[>>>>>>>>>>>>>>>>             ] 292/500, 2.5 task/s, elapsed: 115s, ETA:    82s
[>>>>>>>>>>>>>>>>             ] 293/500, 2.5 task/s, elapsed: 115s, ETA:    81s
[>>>>>>>>>>>>>>>>>            ] 294/500, 2.5 task/s, elapsed: 116s, ETA:    81s
[>>>>>>>>>>>>>>>>>            ] 295/500, 2.5 task/s, elapsed: 116s, ETA:    81s
[>>>>>>>>>>>>>>>>>            ] 296/500, 2.5 task/s, elapsed: 116s, ETA:    80s
[>>>>>>>>>>>>>>>>>            ] 297/500, 2.5 task/s, elapsed: 117s, ETA:    80s
[>>>>>>>>>>>>>>>>>            ] 298/500, 2.5 task/s, elapsed: 117s, ETA:    79s
[>>>>>>>>>>>>>>>>>            ] 299/500, 2.5 task/s, elapsed: 118s, ETA:    79s
[>>>>>>>>>>>>>>>>>            ] 300/500, 2.5 task/s, elapsed: 118s, ETA:    79s
[>>>>>>>>>>>>>>>>>            ] 301/500, 2.5 task/s, elapsed: 118s, ETA:    78s
[>>>>>>>>>>>>>>>>>            ] 302/500, 2.5 task/s, elapsed: 119s, ETA:    78s
[>>>>>>>>>>>>>>>>>            ] 303/500, 2.5 task/s, elapsed: 119s, ETA:    77s
[>>>>>>>>>>>>>>>>>            ] 304/500, 2.5 task/s, elapsed: 119s, ETA:    77s
[>>>>>>>>>>>>>>>>>            ] 305/500, 2.5 task/s, elapsed: 120s, ETA:    77s
[>>>>>>>>>>>>>>>>>            ] 306/500, 2.5 task/s, elapsed: 120s, ETA:    76s
[>>>>>>>>>>>>>>>>>            ] 307/500, 2.5 task/s, elapsed: 121s, ETA:    76s
[>>>>>>>>>>>>>>>>>            ] 308/500, 2.5 task/s, elapsed: 121s, ETA:    75s
[>>>>>>>>>>>>>>>>>            ] 309/500, 2.5 task/s, elapsed: 121s, ETA:    75s
[>>>>>>>>>>>>>>>>>            ] 310/500, 2.5 task/s, elapsed: 122s, ETA:    75s
[>>>>>>>>>>>>>>>>>>           ] 311/500, 2.5 task/s, elapsed: 122s, ETA:    74s
[>>>>>>>>>>>>>>>>>>           ] 312/500, 2.5 task/s, elapsed: 123s, ETA:    74s
[>>>>>>>>>>>>>>>>>>           ] 313/500, 2.5 task/s, elapsed: 123s, ETA:    73s
[>>>>>>>>>>>>>>>>>>           ] 314/500, 2.5 task/s, elapsed: 123s, ETA:    73s
[>>>>>>>>>>>>>>>>>>           ] 315/500, 2.5 task/s, elapsed: 124s, ETA:    73s
[>>>>>>>>>>>>>>>>>>           ] 316/500, 2.5 task/s, elapsed: 124s, ETA:    72s
[>>>>>>>>>>>>>>>>>>           ] 317/500, 2.5 task/s, elapsed: 125s, ETA:    72s
[>>>>>>>>>>>>>>>>>>           ] 318/500, 2.5 task/s, elapsed: 125s, ETA:    71s
[>>>>>>>>>>>>>>>>>>           ] 319/500, 2.5 task/s, elapsed: 125s, ETA:    71s
[>>>>>>>>>>>>>>>>>>           ] 320/500, 2.5 task/s, elapsed: 126s, ETA:    71s
[>>>>>>>>>>>>>>>>>>           ] 321/500, 2.5 task/s, elapsed: 126s, ETA:    70s
[>>>>>>>>>>>>>>>>>>           ] 322/500, 2.5 task/s, elapsed: 126s, ETA:    70s
[>>>>>>>>>>>>>>>>>>           ] 323/500, 2.5 task/s, elapsed: 127s, ETA:    70s
[>>>>>>>>>>>>>>>>>>           ] 324/500, 2.5 task/s, elapsed: 127s, ETA:    69s
[>>>>>>>>>>>>>>>>>>           ] 325/500, 2.5 task/s, elapsed: 128s, ETA:    69s
[>>>>>>>>>>>>>>>>>>           ] 326/500, 2.5 task/s, elapsed: 128s, ETA:    68s
[>>>>>>>>>>>>>>>>>>           ] 327/500, 2.5 task/s, elapsed: 128s, ETA:    68s
[>>>>>>>>>>>>>>>>>>>          ] 328/500, 2.5 task/s, elapsed: 129s, ETA:    68s
[>>>>>>>>>>>>>>>>>>>          ] 329/500, 2.5 task/s, elapsed: 129s, ETA:    67s
[>>>>>>>>>>>>>>>>>>>          ] 330/500, 2.5 task/s, elapsed: 130s, ETA:    67s
[>>>>>>>>>>>>>>>>>>>          ] 331/500, 2.5 task/s, elapsed: 130s, ETA:    66s
[>>>>>>>>>>>>>>>>>>>          ] 332/500, 2.5 task/s, elapsed: 130s, ETA:    66s
[>>>>>>>>>>>>>>>>>>>          ] 333/500, 2.5 task/s, elapsed: 131s, ETA:    66s
[>>>>>>>>>>>>>>>>>>>          ] 334/500, 2.5 task/s, elapsed: 131s, ETA:    65s
[>>>>>>>>>>>>>>>>>>>          ] 335/500, 2.5 task/s, elapsed: 131s, ETA:    65s
[>>>>>>>>>>>>>>>>>>>          ] 336/500, 2.5 task/s, elapsed: 132s, ETA:    64s
[>>>>>>>>>>>>>>>>>>>          ] 337/500, 2.5 task/s, elapsed: 132s, ETA:    64s
[>>>>>>>>>>>>>>>>>>>          ] 338/500, 2.5 task/s, elapsed: 133s, ETA:    64s
[>>>>>>>>>>>>>>>>>>>          ] 339/500, 2.5 task/s, elapsed: 133s, ETA:    63s
[>>>>>>>>>>>>>>>>>>>          ] 340/500, 2.5 task/s, elapsed: 133s, ETA:    63s
[>>>>>>>>>>>>>>>>>>>          ] 341/500, 2.5 task/s, elapsed: 134s, ETA:    62s
[>>>>>>>>>>>>>>>>>>>          ] 342/500, 2.5 task/s, elapsed: 134s, ETA:    62s
[>>>>>>>>>>>>>>>>>>>          ] 343/500, 2.5 task/s, elapsed: 135s, ETA:    62s
[>>>>>>>>>>>>>>>>>>>          ] 344/500, 2.5 task/s, elapsed: 135s, ETA:    61s
[>>>>>>>>>>>>>>>>>>>>         ] 345/500, 2.5 task/s, elapsed: 135s, ETA:    61s
[>>>>>>>>>>>>>>>>>>>>         ] 346/500, 2.5 task/s, elapsed: 136s, ETA:    60s
[>>>>>>>>>>>>>>>>>>>>         ] 347/500, 2.5 task/s, elapsed: 136s, ETA:    60s
[>>>>>>>>>>>>>>>>>>>>         ] 348/500, 2.5 task/s, elapsed: 137s, ETA:    60s
[>>>>>>>>>>>>>>>>>>>>         ] 349/500, 2.5 task/s, elapsed: 137s, ETA:    59s
[>>>>>>>>>>>>>>>>>>>>         ] 350/500, 2.5 task/s, elapsed: 137s, ETA:    59s
[>>>>>>>>>>>>>>>>>>>>         ] 351/500, 2.5 task/s, elapsed: 138s, ETA:    58s
[>>>>>>>>>>>>>>>>>>>>         ] 352/500, 2.5 task/s, elapsed: 138s, ETA:    58s
[>>>>>>>>>>>>>>>>>>>>         ] 353/500, 2.5 task/s, elapsed: 139s, ETA:    58s
[>>>>>>>>>>>>>>>>>>>>         ] 354/500, 2.5 task/s, elapsed: 139s, ETA:    57s
[>>>>>>>>>>>>>>>>>>>>         ] 355/500, 2.5 task/s, elapsed: 139s, ETA:    57s
[>>>>>>>>>>>>>>>>>>>>         ] 356/500, 2.5 task/s, elapsed: 140s, ETA:    57s
[>>>>>>>>>>>>>>>>>>>>         ] 357/500, 2.5 task/s, elapsed: 140s, ETA:    56s
[>>>>>>>>>>>>>>>>>>>>         ] 358/500, 2.5 task/s, elapsed: 141s, ETA:    56s
[>>>>>>>>>>>>>>>>>>>>         ] 359/500, 2.5 task/s, elapsed: 141s, ETA:    55s
[>>>>>>>>>>>>>>>>>>>>         ] 360/500, 2.5 task/s, elapsed: 141s, ETA:    55s
[>>>>>>>>>>>>>>>>>>>>         ] 361/500, 2.5 task/s, elapsed: 142s, ETA:    55s
[>>>>>>>>>>>>>>>>>>>>         ] 362/500, 2.5 task/s, elapsed: 142s, ETA:    54s
[>>>>>>>>>>>>>>>>>>>>>        ] 363/500, 2.5 task/s, elapsed: 142s, ETA:    54s
[>>>>>>>>>>>>>>>>>>>>>        ] 364/500, 2.5 task/s, elapsed: 143s, ETA:    53s
[>>>>>>>>>>>>>>>>>>>>>        ] 365/500, 2.5 task/s, elapsed: 143s, ETA:    53s
[>>>>>>>>>>>>>>>>>>>>>        ] 366/500, 2.5 task/s, elapsed: 144s, ETA:    53s
[>>>>>>>>>>>>>>>>>>>>>        ] 367/500, 2.5 task/s, elapsed: 144s, ETA:    52s
[>>>>>>>>>>>>>>>>>>>>>        ] 368/500, 2.5 task/s, elapsed: 144s, ETA:    52s
[>>>>>>>>>>>>>>>>>>>>>        ] 369/500, 2.5 task/s, elapsed: 145s, ETA:    51s
[>>>>>>>>>>>>>>>>>>>>>        ] 370/500, 2.5 task/s, elapsed: 145s, ETA:    51s
[>>>>>>>>>>>>>>>>>>>>>        ] 371/500, 2.5 task/s, elapsed: 146s, ETA:    51s
[>>>>>>>>>>>>>>>>>>>>>        ] 372/500, 2.5 task/s, elapsed: 146s, ETA:    50s
[>>>>>>>>>>>>>>>>>>>>>        ] 373/500, 2.5 task/s, elapsed: 146s, ETA:    50s
[>>>>>>>>>>>>>>>>>>>>>        ] 374/500, 2.5 task/s, elapsed: 147s, ETA:    49s
[>>>>>>>>>>>>>>>>>>>>>        ] 375/500, 2.5 task/s, elapsed: 147s, ETA:    49s
[>>>>>>>>>>>>>>>>>>>>>        ] 376/500, 2.5 task/s, elapsed: 148s, ETA:    49s
[>>>>>>>>>>>>>>>>>>>>>        ] 377/500, 2.5 task/s, elapsed: 148s, ETA:    48s
[>>>>>>>>>>>>>>>>>>>>>        ] 378/500, 2.5 task/s, elapsed: 148s, ETA:    48s
[>>>>>>>>>>>>>>>>>>>>>        ] 379/500, 2.5 task/s, elapsed: 149s, ETA:    47s
[>>>>>>>>>>>>>>>>>>>>>>       ] 380/500, 2.5 task/s, elapsed: 149s, ETA:    47s
[>>>>>>>>>>>>>>>>>>>>>>       ] 381/500, 2.5 task/s, elapsed: 149s, ETA:    47s
[>>>>>>>>>>>>>>>>>>>>>>       ] 382/500, 2.5 task/s, elapsed: 150s, ETA:    46s
[>>>>>>>>>>>>>>>>>>>>>>       ] 383/500, 2.5 task/s, elapsed: 150s, ETA:    46s
[>>>>>>>>>>>>>>>>>>>>>>       ] 384/500, 2.5 task/s, elapsed: 151s, ETA:    46s
[>>>>>>>>>>>>>>>>>>>>>>       ] 385/500, 2.5 task/s, elapsed: 151s, ETA:    45s
[>>>>>>>>>>>>>>>>>>>>>>       ] 386/500, 2.5 task/s, elapsed: 151s, ETA:    45s
[>>>>>>>>>>>>>>>>>>>>>>       ] 387/500, 2.5 task/s, elapsed: 152s, ETA:    44s
[>>>>>>>>>>>>>>>>>>>>>>       ] 388/500, 2.5 task/s, elapsed: 152s, ETA:    44s
[>>>>>>>>>>>>>>>>>>>>>>       ] 389/500, 2.5 task/s, elapsed: 153s, ETA:    44s
[>>>>>>>>>>>>>>>>>>>>>>       ] 390/500, 2.5 task/s, elapsed: 153s, ETA:    43s
[>>>>>>>>>>>>>>>>>>>>>>       ] 391/500, 2.5 task/s, elapsed: 153s, ETA:    43s
[>>>>>>>>>>>>>>>>>>>>>>       ] 392/500, 2.6 task/s, elapsed: 154s, ETA:    42s
[>>>>>>>>>>>>>>>>>>>>>>       ] 393/500, 2.6 task/s, elapsed: 154s, ETA:    42s
[>>>>>>>>>>>>>>>>>>>>>>       ] 394/500, 2.6 task/s, elapsed: 154s, ETA:    42s
[>>>>>>>>>>>>>>>>>>>>>>       ] 395/500, 2.6 task/s, elapsed: 155s, ETA:    41s
[>>>>>>>>>>>>>>>>>>>>>>       ] 396/500, 2.6 task/s, elapsed: 155s, ETA:    41s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 397/500, 2.6 task/s, elapsed: 156s, ETA:    40s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 398/500, 2.6 task/s, elapsed: 156s, ETA:    40s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 399/500, 2.6 task/s, elapsed: 156s, ETA:    40s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 400/500, 2.6 task/s, elapsed: 157s, ETA:    39s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 401/500, 2.6 task/s, elapsed: 157s, ETA:    39s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 402/500, 2.6 task/s, elapsed: 158s, ETA:    38s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 403/500, 2.6 task/s, elapsed: 158s, ETA:    38s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 404/500, 2.6 task/s, elapsed: 158s, ETA:    38s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 405/500, 2.6 task/s, elapsed: 159s, ETA:    37s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 406/500, 2.6 task/s, elapsed: 159s, ETA:    37s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 407/500, 2.6 task/s, elapsed: 160s, ETA:    36s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 408/500, 2.6 task/s, elapsed: 160s, ETA:    36s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 409/500, 2.6 task/s, elapsed: 160s, ETA:    36s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 410/500, 2.6 task/s, elapsed: 161s, ETA:    35s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 411/500, 2.6 task/s, elapsed: 161s, ETA:    35s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 412/500, 2.6 task/s, elapsed: 161s, ETA:    34s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 413/500, 2.6 task/s, elapsed: 162s, ETA:    34s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 414/500, 2.6 task/s, elapsed: 162s, ETA:    34s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 415/500, 2.6 task/s, elapsed: 163s, ETA:    33s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 416/500, 2.6 task/s, elapsed: 163s, ETA:    33s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 417/500, 2.6 task/s, elapsed: 163s, ETA:    33s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 418/500, 2.6 task/s, elapsed: 164s, ETA:    32s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 419/500, 2.6 task/s, elapsed: 164s, ETA:    32s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 420/500, 2.6 task/s, elapsed: 165s, ETA:    31s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 421/500, 2.6 task/s, elapsed: 165s, ETA:    31s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 422/500, 2.6 task/s, elapsed: 165s, ETA:    31s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 423/500, 2.6 task/s, elapsed: 166s, ETA:    30s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 424/500, 2.6 task/s, elapsed: 166s, ETA:    30s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 425/500, 2.6 task/s, elapsed: 167s, ETA:    29s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 426/500, 2.6 task/s, elapsed: 167s, ETA:    29s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 427/500, 2.6 task/s, elapsed: 167s, ETA:    29s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 428/500, 2.5 task/s, elapsed: 168s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 429/500, 2.5 task/s, elapsed: 168s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 430/500, 2.5 task/s, elapsed: 169s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 431/500, 2.5 task/s, elapsed: 169s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 432/500, 2.5 task/s, elapsed: 169s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 433/500, 2.5 task/s, elapsed: 170s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 434/500, 2.5 task/s, elapsed: 170s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 435/500, 2.5 task/s, elapsed: 171s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 436/500, 2.5 task/s, elapsed: 171s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 437/500, 2.5 task/s, elapsed: 171s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 438/500, 2.5 task/s, elapsed: 172s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 439/500, 2.5 task/s, elapsed: 172s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 440/500, 2.5 task/s, elapsed: 173s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 441/500, 2.5 task/s, elapsed: 173s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 442/500, 2.5 task/s, elapsed: 173s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 443/500, 2.5 task/s, elapsed: 174s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 444/500, 2.5 task/s, elapsed: 174s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 445/500, 2.5 task/s, elapsed: 175s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 446/500, 2.5 task/s, elapsed: 175s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 447/500, 2.5 task/s, elapsed: 175s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 448/500, 2.5 task/s, elapsed: 176s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 449/500, 2.5 task/s, elapsed: 176s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 450/500, 2.5 task/s, elapsed: 177s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 451/500, 2.5 task/s, elapsed: 177s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 452/500, 2.5 task/s, elapsed: 177s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 453/500, 2.5 task/s, elapsed: 178s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 454/500, 2.5 task/s, elapsed: 178s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 455/500, 2.5 task/s, elapsed: 179s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 456/500, 2.5 task/s, elapsed: 179s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 457/500, 2.5 task/s, elapsed: 179s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 458/500, 2.5 task/s, elapsed: 180s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 459/500, 2.5 task/s, elapsed: 180s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 460/500, 2.5 task/s, elapsed: 181s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 461/500, 2.5 task/s, elapsed: 181s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 462/500, 2.5 task/s, elapsed: 181s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 463/500, 2.5 task/s, elapsed: 182s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 464/500, 2.5 task/s, elapsed: 182s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 465/500, 2.5 task/s, elapsed: 183s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 466/500, 2.5 task/s, elapsed: 183s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 467/500, 2.5 task/s, elapsed: 183s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 468/500, 2.5 task/s, elapsed: 184s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 469/500, 2.5 task/s, elapsed: 184s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 470/500, 2.5 task/s, elapsed: 185s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 471/500, 2.5 task/s, elapsed: 185s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 472/500, 2.5 task/s, elapsed: 185s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 473/500, 2.5 task/s, elapsed: 186s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 474/500, 2.5 task/s, elapsed: 186s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 475/500, 2.5 task/s, elapsed: 187s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 476/500, 2.5 task/s, elapsed: 187s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 477/500, 2.5 task/s, elapsed: 187s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 478/500, 2.5 task/s, elapsed: 188s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 479/500, 2.5 task/s, elapsed: 188s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 480/500, 2.5 task/s, elapsed: 189s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 481/500, 2.5 task/s, elapsed: 189s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 482/500, 2.5 task/s, elapsed: 190s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 483/500, 2.5 task/s, elapsed: 190s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 484/500, 2.5 task/s, elapsed: 190s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 485/500, 2.5 task/s, elapsed: 191s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 486/500, 2.5 task/s, elapsed: 191s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 487/500, 2.5 task/s, elapsed: 192s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 488/500, 2.5 task/s, elapsed: 192s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 489/500, 2.5 task/s, elapsed: 192s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 490/500, 2.5 task/s, elapsed: 193s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 491/500, 2.5 task/s, elapsed: 193s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 492/500, 2.5 task/s, elapsed: 194s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 493/500, 2.5 task/s, elapsed: 194s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 494/500, 2.5 task/s, elapsed: 194s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 495/500, 2.5 task/s, elapsed: 195s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 496/500, 2.5 task/s, elapsed: 195s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 497/500, 2.5 task/s, elapsed: 196s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 498/500, 2.5 task/s, elapsed: 196s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 499/500, 2.5 task/s, elapsed: 196s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 500/500, 2.5 task/s, elapsed: 197s, ETA:     0s/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:39:55,267 - mmseg - INFO - Iter [26500/40000]	lr: 2.025e-05, eta: 5:47:37, time: 1.489, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1052, decode.acc_seg: 91.1814, src.loss_imnet_feat_dist: 0.0993, mix.decode.loss_seg: 0.1296, mix.decode.acc_seg: 89.8555
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:41:10,304 - mmseg - INFO - Iter [26550/40000]	lr: 2.018e-05, eta: 5:46:19, time: 1.501, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1093, decode.acc_seg: 91.0392, src.loss_imnet_feat_dist: 0.1009, mix.decode.loss_seg: 0.1255, mix.decode.acc_seg: 89.3322
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:42:21,337 - mmseg - INFO - Iter [26600/40000]	lr: 2.010e-05, eta: 5:44:58, time: 1.421, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1043, decode.acc_seg: 90.5237, src.loss_imnet_feat_dist: 0.1028, mix.decode.loss_seg: 0.1185, mix.decode.acc_seg: 89.1283
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:43:34,069 - mmseg - INFO - Iter [26650/40000]	lr: 2.003e-05, eta: 5:43:39, time: 1.455, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1068, decode.acc_seg: 90.8389, src.loss_imnet_feat_dist: 0.0957, mix.decode.loss_seg: 0.1118, mix.decode.acc_seg: 89.1425
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:44:44,245 - mmseg - INFO - Iter [26700/40000]	lr: 1.995e-05, eta: 5:42:18, time: 1.404, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1087, decode.acc_seg: 90.4547, src.loss_imnet_feat_dist: 0.1025, mix.decode.loss_seg: 0.1214, mix.decode.acc_seg: 88.3736
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:45:57,093 - mmseg - INFO - Iter [26750/40000]	lr: 1.988e-05, eta: 5:40:59, time: 1.457, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1102, decode.acc_seg: 90.7100, src.loss_imnet_feat_dist: 0.0949, mix.decode.loss_seg: 0.1269, mix.decode.acc_seg: 89.1664
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:47:12,109 - mmseg - INFO - Iter [26800/40000]	lr: 1.980e-05, eta: 5:39:41, time: 1.500, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1081, decode.acc_seg: 91.6474, src.loss_imnet_feat_dist: 0.1018, mix.decode.loss_seg: 0.1305, mix.decode.acc_seg: 90.1461
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:48:30,253 - mmseg - INFO - Iter [26850/40000]	lr: 1.973e-05, eta: 5:38:24, time: 1.563, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1103, decode.acc_seg: 90.3265, src.loss_imnet_feat_dist: 0.1020, mix.decode.loss_seg: 0.1229, mix.decode.acc_seg: 89.6511
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:49:48,019 - mmseg - INFO - Iter [26900/40000]	lr: 1.965e-05, eta: 5:37:07, time: 1.555, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1060, decode.acc_seg: 91.3108, src.loss_imnet_feat_dist: 0.0986, mix.decode.loss_seg: 0.1237, mix.decode.acc_seg: 89.0001
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:51:02,053 - mmseg - INFO - Iter [26950/40000]	lr: 1.958e-05, eta: 5:35:48, time: 1.481, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1158, decode.acc_seg: 91.2034, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1294, mix.decode.acc_seg: 88.9738
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:52:15,335 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-19 00:52:15,335 - mmseg - INFO - Iter [27000/40000]	lr: 1.950e-05, eta: 5:34:29, time: 1.466, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1029, decode.acc_seg: 90.6103, src.loss_imnet_feat_dist: 0.0983, mix.decode.loss_seg: 0.1135, mix.decode.acc_seg: 89.8983
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:53:35,010 - mmseg - INFO - Iter [27050/40000]	lr: 1.943e-05, eta: 5:33:13, time: 1.593, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1106, decode.acc_seg: 90.4341, src.loss_imnet_feat_dist: 0.0986, mix.decode.loss_seg: 0.1192, mix.decode.acc_seg: 88.9673
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:54:35,740 - mmseg - INFO - Iter [12250/40000]	lr: 4.163e-05, eta: 1 day, 2:20:51, time: 3.398, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.5449, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.1409
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 00:57:24,039 - mmseg - INFO - Iter [12300/40000]	lr: 4.155e-05, eta: 1 day, 2:17:54, time: 3.366, data_time: 0.019, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.6161, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.4378
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:00:12,408 - mmseg - INFO - Iter [12350/40000]	lr: 4.148e-05, eta: 1 day, 2:14:57, time: 3.367, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.8933, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.9939
1
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:01:13,914 - mmseg - INFO - Iter [27350/40000]	lr: 1.898e-05, eta: 5:25:28, time: 1.452, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1032, decode.acc_seg: 90.8740, src.loss_imnet_feat_dist: 0.1055, mix.decode.loss_seg: 0.1078, mix.decode.acc_seg: 89.5141
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:02:28,540 - mmseg - INFO - Iter [27400/40000]	lr: 1.890e-05, eta: 5:24:10, time: 1.492, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0951, decode.acc_seg: 91.7976, src.loss_imnet_feat_dist: 0.0954, mix.decode.loss_seg: 0.1180, mix.decode.acc_seg: 89.5685
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:03:47,941 - mmseg - INFO - Iter [27450/40000]	lr: 1.883e-05, eta: 5:22:53, time: 1.588, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1155, decode.acc_seg: 89.8513, src.loss_imnet_feat_dist: 0.0950, mix.decode.loss_seg: 0.1277, mix.decode.acc_seg: 88.6926
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:05:02,455 - mmseg - INFO - Iter [27500/40000]	lr: 1.875e-05, eta: 5:21:35, time: 1.490, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1077, decode.acc_seg: 90.9205, src.loss_imnet_feat_dist: 0.1001, mix.decode.loss_seg: 0.1177, mix.decode.acc_seg: 89.6286
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:06:15,000 - mmseg - INFO - Iter [27550/40000]	lr: 1.868e-05, eta: 5:20:16, time: 1.451, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1034, decode.acc_seg: 91.6367, src.loss_imnet_feat_dist: 0.0998, mix.decode.loss_seg: 0.1143, mix.decode.acc_seg: 89.5277
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:07:27,276 - mmseg - INFO - Iter [27600/40000]	lr: 1.860e-05, eta: 5:18:56, time: 1.446, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1042, decode.acc_seg: 90.7857, src.loss_imnet_feat_dist: 0.1013, mix.decode.loss_seg: 0.1235, mix.decode.acc_seg: 88.2075
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:08:45,741 - mmseg - INFO - Iter [27650/40000]	lr: 1.853e-05, eta: 5:17:40, time: 1.569, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1083, decode.acc_seg: 90.7010, src.loss_imnet_feat_dist: 0.0996, mix.decode.loss_seg: 0.1529, mix.decode.acc_seg: 87.2312
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:10:03,253 - mmseg - INFO - Iter [27700/40000]	lr: 1.845e-05, eta: 5:16:23, time: 1.550, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1033, decode.acc_seg: 90.9144, src.loss_imnet_feat_dist: 0.0965, mix.decode.loss_seg: 0.1429, mix.decode.acc_seg: 87.9327
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:11:20,617 - mmseg - INFO - Iter [27750/40000]	lr: 1.838e-05, eta: 5:15:06, time: 1.547, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1173, decode.acc_seg: 90.2473, src.loss_imnet_feat_dist: 0.0977, mix.decode.loss_seg: 0.1342, mix.decode.acc_seg: 88.6307
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:12:35,928 - mmseg - INFO - Iter [27800/40000]	lr: 1.830e-05, eta: 5:13:48, time: 1.506, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1056, decode.acc_seg: 91.7027, src.loss_imnet_feat_dist: 0.1008, mix.decode.loss_seg: 0.1234, mix.decode.acc_seg: 89.9788
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:13:54,866 - mmseg - INFO - Iter [27850/40000]	lr: 1.823e-05, eta: 5:12:31, time: 1.579, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0998, decode.acc_seg: 91.6292, src.loss_imnet_feat_dist: 0.1026, mix.decode.loss_seg: 0.1058, mix.decode.acc_seg: 90.0922
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:15:10,287 - mmseg - INFO - Iter [27900/40000]	lr: 1.815e-05, eta: 5:11:13, time: 1.508, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1094, decode.acc_seg: 91.6466, src.loss_imnet_feat_dist: 0.1018, mix.decode.loss_seg: 0.1439, mix.decode.acc_seg: 89.4384
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:16:25,417 - mmseg - INFO - Iter [27950/40000]	lr: 1.808e-05, eta: 5:09:55, time: 1.503, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1042, decode.acc_seg: 92.0890, src.loss_imnet_feat_dist: 0.0963, mix.decode.loss_seg: 0.1111, mix.decode.acc_seg: 90.4754
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              [                                                  ] 0/500, elapsed: 0s, ETA:
[                                 ] 1/500, 1.4 task/s, elapsed: 1s, ETA:   358s
[                                 ] 2/500, 2.3 task/s, elapsed: 1s, ETA:   219s
[                                 ] 3/500, 2.9 task/s, elapsed: 1s, ETA:   172s
[                                 ] 4/500, 3.4 task/s, elapsed: 1s, ETA:   148s
[                                 ] 5/500, 3.7 task/s, elapsed: 1s, ETA:   133s
[                                 ] 6/500, 4.0 task/s, elapsed: 1s, ETA:   122s
[                                 ] 7/500, 4.3 task/s, elapsed: 2s, ETA:   115s
[                                 ] 8/500, 4.5 task/s, elapsed: 2s, ETA:   109s
[                                 ] 9/500, 4.7 task/s, elapsed: 2s, ETA:   105s
[                                ] 10/500, 4.8 task/s, elapsed: 2s, ETA:   102s
[                                ] 11/500, 4.9 task/s, elapsed: 2s, ETA:    99s
[                                ] 12/500, 5.1 task/s, elapsed: 2s, ETA:    96s
[                                ] 13/500, 5.2 task/s, elapsed: 3s, ETA:    94s
[                                ] 14/500, 5.3 task/s, elapsed: 3s, ETA:    92s
[                                ] 15/500, 5.3 task/s, elapsed: 3s, ETA:    91s
[>                               ] 16/500, 5.4 task/s, elapsed: 3s, ETA:    89s
[>                               ] 17/500, 5.5 task/s, elapsed: 3s, ETA:    88s
[>                               ] 18/500, 5.5 task/s, elapsed: 3s, ETA:    87s
[>                               ] 19/500, 5.6 task/s, elapsed: 3s, ETA:    86s
[>                               ] 20/500, 5.6 task/s, elapsed: 4s, ETA:    85s
[>                               ] 21/500, 5.7 task/s, elapsed: 4s, ETA:    84s
[>                               ] 22/500, 5.7 task/s, elapsed: 4s, ETA:    83s
[>                               ] 23/500, 5.8 task/s, elapsed: 4s, ETA:    83s
[>                               ] 24/500, 5.8 task/s, elapsed: 4s, ETA:    82s
[>                               ] 25/500, 5.8 task/s, elapsed: 4s, ETA:    81s
[>                               ] 26/500, 5.9 task/s, elapsed: 4s, ETA:    81s
[>                               ] 27/500, 5.9 task/s, elapsed: 5s, ETA:    80s
[>                               ] 28/500, 5.9 task/s, elapsed: 5s, ETA:    79s
[>                               ] 29/500, 6.0 task/s, elapsed: 5s, ETA:    79s
[>                               ] 30/500, 6.0 task/s, elapsed: 5s, ETA:    78s
[>                               ] 31/500, 6.0 task/s, elapsed: 5s, ETA:    78s
[>>                              ] 32/500, 6.0 task/s, elapsed: 5s, ETA:    77s
[>>                              ] 33/500, 6.0 task/s, elapsed: 5s, ETA:    77s
[>>                              ] 34/500, 6.1 task/s, elapsed: 6s, ETA:    77s
[>>                              ] 35/500, 6.1 task/s, elapsed: 6s, ETA:    76s
[>>                              ] 36/500, 6.1 task/s, elapsed: 6s, ETA:    76s
[>>                              ] 37/500, 6.1 task/s, elapsed: 6s, ETA:    76s
[>>                              ] 38/500, 6.1 task/s, elapsed: 6s, ETA:    75s
[>>                              ] 39/500, 6.2 task/s, elapsed: 6s, ETA:    75s
[>>                              ] 40/500, 6.2 task/s, elapsed: 6s, ETA:    75s
[>>                              ] 41/500, 6.2 task/s, elapsed: 7s, ETA:    74s
[>>                              ] 42/500, 6.2 task/s, elapsed: 7s, ETA:    74s
[>>                              ] 43/500, 6.2 task/s, elapsed: 7s, ETA:    74s
[>>                              ] 44/500, 6.2 task/s, elapsed: 7s, ETA:    73s
[>>                              ] 45/500, 6.2 task/s, elapsed: 7s, ETA:    73s
[>>                              ] 46/500, 6.2 task/s, elapsed: 7s, ETA:    73s
[>>>                             ] 47/500, 6.2 task/s, elapsed: 8s, ETA:    73s
[>>>                             ] 48/500, 6.3 task/s, elapsed: 8s, ETA:    72s
[>>>                             ] 49/500, 6.3 task/s, elapsed: 8s, ETA:    72s
[>>>                             ] 50/500, 6.3 task/s, elapsed: 8s, ETA:    72s
[>>>                             ] 51/500, 6.3 task/s, elapsed: 8s, ETA:    72s
[>>>                             ] 52/500, 6.3 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 53/500, 6.3 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 54/500, 6.3 task/s, elapsed: 9s, ETA:    71s
[>>>                             ] 55/500, 6.3 task/s, elapsed: 9s, ETA:    70s
[>>>                             ] 56/500, 6.3 task/s, elapsed: 9s, ETA:    70s
[>>>                             ] 57/500, 6.3 task/s, elapsed: 9s, ETA:    70s
[>>>                             ] 58/500, 6.3 task/s, elapsed: 9s, ETA:    70s
[>>>                             ] 59/500, 6.3 task/s, elapsed: 9s, ETA:    70s
[>>>                             ] 60/500, 6.3 task/s, elapsed: 9s, ETA:    69s
[>>>                            ] 61/500, 6.3 task/s, elapsed: 10s, ETA:    69s
[>>>                            ] 62/500, 6.3 task/s, elapsed: 10s, ETA:    69s
[>>>                            ] 63/500, 6.4 task/s, elapsed: 10s, ETA:    69s
[>>>                            ] 64/500, 6.4 task/s, elapsed: 10s, ETA:    68s
[>>>>                           ] 65/500, 6.4 task/s, elapsed: 10s, ETA:    68s
[>>>>                           ] 66/500, 6.4 task/s, elapsed: 10s, ETA:    68s
[>>>>                           ] 67/500, 6.4 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 68/500, 6.4 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 69/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 70/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 71/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 72/500, 6.4 task/s, elapsed: 11s, ETA:    67s
[>>>>                           ] 73/500, 6.4 task/s, elapsed: 11s, ETA:    66s
[>>>>                           ] 74/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 75/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 76/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 77/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 78/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 79/500, 6.4 task/s, elapsed: 12s, ETA:    65s
[>>>>                           ] 80/500, 6.4 task/s, elapsed: 12s, ETA:    65s
[>>>>>                          ] 81/500, 6.5 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 82/500, 6.5 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 83/500, 6.5 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 84/500, 6.5 task/s, elapsed: 13s, ETA:    64s
[>>>>>                          ] 85/500, 6.5 task/s, elapsed: 13s, ETA:    64s
[>>>>>                          ] 86/500, 6.5 task/s, elapsed: 13s, ETA:    64s
[>>>>>                          ] 87/500, 6.5 task/s, elapsed: 13s, ETA:    64s
[>>>>>                          ] 88/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 89/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 90/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 91/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 92/500, 6.5 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 93/500, 6.5 task/s, elapsed: 14s, ETA:    62s
[>>>>>                          ] 94/500, 6.5 task/s, elapsed: 14s, ETA:    62s
[>>>>>                          ] 95/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>                          ] 96/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>>                         ] 97/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>>                         ] 98/500, 6.5 task/s, elapsed: 15s, ETA:    62s
[>>>>>>                         ] 99/500, 6.5 task/s, elapsed: 15s, ETA:    61s
[>>>>>>                        ] 100/500, 6.5 task/s, elapsed: 15s, ETA:    61s
[>>>>>>                        ] 101/500, 6.5 task/s, elapsed: 15s, ETA:    61s
[>>>>>>                        ] 102/500, 6.5 task/s, elapsed: 16s, ETA:    61s
[>>>>>>                        ] 103/500, 6.6 task/s, elapsed: 16s, ETA:    61s
[>>>>>>                        ] 104/500, 6.6 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 105/500, 6.6 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 106/500, 6.6 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 107/500, 6.6 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 108/500, 6.6 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 109/500, 6.6 task/s, elapsed: 17s, ETA:    60s
[>>>>>>                        ] 110/500, 6.6 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 111/500, 6.6 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 112/500, 6.6 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 113/500, 6.6 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 114/500, 6.6 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 115/500, 6.6 task/s, elapsed: 17s, ETA:    58s
[>>>>>>                        ] 116/500, 6.6 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 117/500, 6.6 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 118/500, 6.6 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 119/500, 6.6 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 120/500, 6.6 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 121/500, 6.6 task/s, elapsed: 18s, ETA:    57s
[>>>>>>>                       ] 122/500, 6.6 task/s, elapsed: 18s, ETA:    57s
[>>>>>>>                       ] 123/500, 6.6 task/s, elapsed: 19s, ETA:    57s
[>>>>>>>                       ] 124/500, 6.6 task/s, elapsed: 19s, ETA:    57s
[>>>>>>>                       ] 125/500, 6.6 task/s, elapsed: 19s, ETA:    57s
[>>>>>>>                       ] 126/500, 6.6 task/s, elapsed: 19s, ETA:    57s
[>>>>>>>                       ] 127/500, 6.6 task/s, elapsed: 19s, ETA:    56s
[>>>>>>>                       ] 128/500, 6.6 task/s, elapsed: 19s, ETA:    56s
[>>>>>>>                       ] 129/500, 6.6 task/s, elapsed: 20s, ETA:    56s
[>>>>>>>                       ] 130/500, 6.6 task/s, elapsed: 20s, ETA:    56s
[>>>>>>>                       ] 131/500, 6.6 task/s, elapsed: 20s, ETA:    56s
[>>>>>>>                       ] 132/500, 6.6 task/s, elapsed: 20s, ETA:    56s
[>>>>>>>                       ] 133/500, 6.6 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>>                      ] 134/500, 6.6 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>>                      ] 135/500, 6.6 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>>                      ] 136/500, 6.6 task/s, elapsed: 21s, ETA:    55s
[>>>>>>>>                      ] 137/500, 6.6 task/s, elapsed: 21s, ETA:    55s
[>>>>>>>>                      ] 138/500, 6.6 task/s, elapsed: 21s, ETA:    55s
[>>>>>>>>                      ] 139/500, 6.6 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 140/500, 6.6 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 141/500, 6.6 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 142/500, 6.6 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 143/500, 6.6 task/s, elapsed: 22s, ETA:    54s
[>>>>>>>>                      ] 144/500, 6.6 task/s, elapsed: 22s, ETA:    54s
[>>>>>>>>                      ] 145/500, 6.6 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 146/500, 6.6 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 147/500, 6.6 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 148/500, 6.6 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 149/500, 6.7 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>>                     ] 150/500, 6.7 task/s, elapsed: 23s, ETA:    53s
[>>>>>>>>>                     ] 151/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 152/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 153/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 154/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 155/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 156/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 157/500, 6.7 task/s, elapsed: 24s, ETA:    52s
[>>>>>>>>>                     ] 158/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 159/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 160/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 161/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 162/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 163/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 164/500, 6.7 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>                     ] 165/500, 6.7 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>                     ] 166/500, 6.7 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>>                    ] 167/500, 6.7 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>>                    ] 168/500, 6.7 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>>                    ] 169/500, 6.7 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>>                    ] 170/500, 6.7 task/s, elapsed: 25s, ETA:    49s
[>>>>>>>>>>                    ] 171/500, 6.7 task/s, elapsed: 26s, ETA:    49s
[>>>>>>>>>>                    ] 172/500, 6.7 task/s, elapsed: 26s, ETA:    49s
[>>>>>>>>>>                    ] 173/500, 6.7 task/s, elapsed: 26s, ETA:    49s
[>>>>>>>>>>                    ] 174/500, 6.7 task/s, elapsed: 26s, ETA:    49s
[>>>>>>>>>>                    ] 175/500, 6.7 task/s, elapsed: 26s, ETA:    49s
[>>>>>>>>>>                    ] 176/500, 6.7 task/s, elapsed: 26s, ETA:    48s
[>>>>>>>>>>                    ] 177/500, 6.7 task/s, elapsed: 26s, ETA:    48s
[>>>>>>>>>>                    ] 178/500, 6.7 task/s, elapsed: 27s, ETA:    48s
[>>>>>>>>>>                    ] 179/500, 6.7 task/s, elapsed: 27s, ETA:    48s
[>>>>>>>>>>                    ] 180/500, 6.7 task/s, elapsed: 27s, ETA:    48s
[>>>>>>>>>>                    ] 181/500, 6.7 task/s, elapsed: 27s, ETA:    48s
[>>>>>>>>>>                    ] 182/500, 6.7 task/s, elapsed: 27s, ETA:    48s
[>>>>>>>>>>                    ] 183/500, 6.7 task/s, elapsed: 27s, ETA:    47s
[>>>>>>>>>>>                   ] 184/500, 6.7 task/s, elapsed: 27s, ETA:    47s
[>>>>>>>>>>>                   ] 185/500, 6.7 task/s, elapsed: 28s, ETA:    47s
[>>>>>>>>>>>                   ] 186/500, 6.7 task/s, elapsed: 28s, ETA:    47s
[>>>>>>>>>>>                   ] 187/500, 6.7 task/s, elapsed: 28s, ETA:    47s
[>>>>>>>>>>>                   ] 188/500, 6.7 task/s, elapsed: 28s, ETA:    47s
[>>>>>>>>>>>                   ] 189/500, 6.7 task/s, elapsed: 28s, ETA:    46s
[>>>>>>>>>>>                   ] 190/500, 6.7 task/s, elapsed: 28s, ETA:    46s
[>>>>>>>>>>>                   ] 191/500, 6.7 task/s, elapsed: 29s, ETA:    46s
[>>>>>>>>>>>                   ] 192/500, 6.7 task/s, elapsed: 29s, ETA:    46s
[>>>>>>>>>>>                   ] 193/500, 6.7 task/s, elapsed: 29s, ETA:    46s
[>>>>>>>>>>>                   ] 194/500, 6.7 task/s, elapsed: 29s, ETA:    46s
[>>>>>>>>>>>                   ] 195/500, 6.7 task/s, elapsed: 29s, ETA:    45s
[>>>>>>>>>>>                   ] 196/500, 6.7 task/s, elapsed: 29s, ETA:    45s
[>>>>>>>>>>>                   ] 197/500, 6.7 task/s, elapsed: 29s, ETA:    45s
[>>>>>>>>>>>                   ] 198/500, 6.7 task/s, elapsed: 30s, ETA:    45s
[>>>>>>>>>>>                   ] 199/500, 6.7 task/s, elapsed: 30s, ETA:    45s
[>>>>>>>>>>>>                  ] 200/500, 6.7 task/s, elapsed: 30s, ETA:    45s
[>>>>>>>>>>>>                  ] 201/500, 6.7 task/s, elapsed: 30s, ETA:    45s
[>>>>>>>>>>>>                  ] 202/500, 6.7 task/s, elapsed: 30s, ETA:    44s
[>>>>>>>>>>>>                  ] 203/500, 6.7 task/s, elapsed: 30s, ETA:    44s
[>>>>>>>>>>>>                  ] 204/500, 6.7 task/s, elapsed: 30s, ETA:    44s
[>>>>>>>>>>>>                  ] 205/500, 6.7 task/s, elapsed: 31s, ETA:    44s
[>>>>>>>>>>>>                  ] 206/500, 6.7 task/s, elapsed: 31s, ETA:    44s
[>>>>>>>>>>>>                  ] 207/500, 6.7 task/s, elapsed: 31s, ETA:    44s
[>>>>>>>>>>>>                  ] 208/500, 6.7 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 209/500, 6.7 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 210/500, 6.7 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 211/500, 6.7 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 212/500, 6.7 task/s, elapsed: 32s, ETA:    43s
[>>>>>>>>>>>>                  ] 213/500, 6.7 task/s, elapsed: 32s, ETA:    43s
[>>>>>>>>>>>>                  ] 214/500, 6.7 task/s, elapsed: 32s, ETA:    43s
[>>>>>>>>>>>>                  ] 215/500, 6.7 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>                  ] 216/500, 6.7 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>>                 ] 217/500, 6.7 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>>                 ] 218/500, 6.7 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>>                 ] 219/500, 6.7 task/s, elapsed: 33s, ETA:    42s
[>>>>>>>>>>>>>                 ] 220/500, 6.7 task/s, elapsed: 33s, ETA:    42s
[>>>>>>>>>>>>>                 ] 221/500, 6.7 task/s, elapsed: 33s, ETA:    42s
[>>>>>>>>>>>>>                 ] 222/500, 6.7 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 223/500, 6.7 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 224/500, 6.7 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 225/500, 6.7 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 226/500, 6.7 task/s, elapsed: 34s, ETA:    41s
[>>>>>>>>>>>>>                 ] 227/500, 6.7 task/s, elapsed: 34s, ETA:    41s
[>>>>>>>>>>>>>                 ] 228/500, 6.7 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 229/500, 6.7 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 230/500, 6.7 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 231/500, 6.7 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 232/500, 6.7 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 233/500, 6.7 task/s, elapsed: 35s, ETA:    40s
[>>>>>>>>>>>>>>                ] 234/500, 6.7 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 235/500, 6.7 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 236/500, 6.7 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 237/500, 6.7 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 238/500, 6.7 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 239/500, 6.7 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 240/500, 6.7 task/s, elapsed: 36s, ETA:    39s
[>>>>>>>>>>>>>>                ] 241/500, 6.7 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 242/500, 6.7 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 243/500, 6.7 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 244/500, 6.7 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 245/500, 6.7 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 246/500, 6.7 task/s, elapsed: 37s, ETA:    38s
[>>>>>>>>>>>>>>                ] 247/500, 6.7 task/s, elapsed: 37s, ETA:    38s
[>>>>>>>>>>>>>>                ] 248/500, 6.7 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>                ] 249/500, 6.7 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 250/500, 6.7 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 251/500, 6.7 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 252/500, 6.7 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 253/500, 6.7 task/s, elapsed: 38s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 254/500, 6.7 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 255/500, 6.7 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 256/500, 6.7 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 257/500, 6.7 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 258/500, 6.7 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 259/500, 6.8 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 260/500, 6.8 task/s, elapsed: 39s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 261/500, 6.8 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 262/500, 6.8 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 263/500, 6.8 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 264/500, 6.8 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 265/500, 6.8 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 266/500, 6.8 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 267/500, 6.8 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 268/500, 6.8 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 269/500, 6.8 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 270/500, 6.8 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 271/500, 6.8 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 272/500, 6.8 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 273/500, 6.8 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 274/500, 6.8 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 275/500, 6.8 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 276/500, 6.8 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 277/500, 6.8 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 278/500, 6.8 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 279/500, 6.7 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 280/500, 6.7 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 281/500, 6.7 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>              ] 282/500, 6.7 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>              ] 283/500, 6.7 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 284/500, 6.7 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 285/500, 6.7 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 286/500, 6.7 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 287/500, 6.7 task/s, elapsed: 43s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 288/500, 6.7 task/s, elapsed: 43s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 289/500, 6.7 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 290/500, 6.7 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 291/500, 6.7 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 292/500, 6.7 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 293/500, 6.7 task/s, elapsed: 44s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 294/500, 6.7 task/s, elapsed: 44s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 295/500, 6.7 task/s, elapsed: 44s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 296/500, 6.7 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 297/500, 6.7 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 298/500, 6.7 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 299/500, 6.7 task/s, elapsed: 45s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 300/500, 6.7 task/s, elapsed: 45s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 301/500, 6.7 task/s, elapsed: 45s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 302/500, 6.7 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 303/500, 6.7 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 304/500, 6.7 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 305/500, 6.7 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 306/500, 6.7 task/s, elapsed: 46s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 307/500, 6.7 task/s, elapsed: 46s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 308/500, 6.7 task/s, elapsed: 46s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 309/500, 6.7 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 310/500, 6.7 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 311/500, 6.7 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 312/500, 6.7 task/s, elapsed: 47s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 313/500, 6.7 task/s, elapsed: 47s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 314/500, 6.7 task/s, elapsed: 47s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 315/500, 6.7 task/s, elapsed: 47s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 316/500, 6.7 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 317/500, 6.7 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 318/500, 6.7 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 319/500, 6.7 task/s, elapsed: 48s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 320/500, 6.7 task/s, elapsed: 48s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 321/500, 6.7 task/s, elapsed: 48s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 322/500, 6.7 task/s, elapsed: 48s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 323/500, 6.7 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 324/500, 6.7 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 325/500, 6.7 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 326/500, 6.7 task/s, elapsed: 49s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 327/500, 6.7 task/s, elapsed: 49s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 328/500, 6.7 task/s, elapsed: 49s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 329/500, 6.7 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 330/500, 6.7 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 331/500, 6.7 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 332/500, 6.7 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 333/500, 6.7 task/s, elapsed: 50s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 334/500, 6.7 task/s, elapsed: 50s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 335/500, 6.7 task/s, elapsed: 50s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 336/500, 6.7 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 337/500, 6.7 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 338/500, 6.7 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 339/500, 6.7 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 340/500, 6.7 task/s, elapsed: 51s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 341/500, 6.7 task/s, elapsed: 51s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 342/500, 6.7 task/s, elapsed: 51s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 343/500, 6.7 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 344/500, 6.7 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 345/500, 6.7 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 346/500, 6.7 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 347/500, 6.7 task/s, elapsed: 52s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 348/500, 6.7 task/s, elapsed: 52s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 349/500, 6.7 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 350/500, 6.7 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 351/500, 6.7 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 352/500, 6.7 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 353/500, 6.7 task/s, elapsed: 53s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 354/500, 6.7 task/s, elapsed: 53s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 355/500, 6.7 task/s, elapsed: 53s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 356/500, 6.7 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 357/500, 6.7 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 358/500, 6.7 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 359/500, 6.7 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 360/500, 6.7 task/s, elapsed: 54s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 361/500, 6.7 task/s, elapsed: 54s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 362/500, 6.7 task/s, elapsed: 54s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 363/500, 6.7 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 364/500, 6.7 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 365/500, 6.7 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 366/500, 6.7 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 367/500, 6.7 task/s, elapsed: 55s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 368/500, 6.7 task/s, elapsed: 55s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 369/500, 6.7 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 370/500, 6.7 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 371/500, 6.7 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 372/500, 6.7 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 373/500, 6.7 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 374/500, 6.7 task/s, elapsed: 56s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 375/500, 6.7 task/s, elapsed: 56s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 376/500, 6.7 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 377/500, 6.7 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 378/500, 6.7 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 379/500, 6.7 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 380/500, 6.7 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 381/500, 6.7 task/s, elapsed: 57s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 382/500, 6.7 task/s, elapsed: 57s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 383/500, 6.7 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 384/500, 6.7 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 385/500, 6.7 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 386/500, 6.7 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 387/500, 6.7 task/s, elapsed: 58s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 388/500, 6.7 task/s, elapsed: 58s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 389/500, 6.7 task/s, elapsed: 58s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 390/500, 6.7 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 391/500, 6.7 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 392/500, 6.7 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 393/500, 6.7 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 394/500, 6.7 task/s, elapsed: 59s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 395/500, 6.7 task/s, elapsed: 59s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 396/500, 6.7 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 397/500, 6.7 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 398/500, 6.7 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 399/500, 6.7 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 400/500, 6.7 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 401/500, 6.7 task/s, elapsed: 60s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 402/500, 6.7 task/s, elapsed: 60s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 403/500, 6.7 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 404/500, 6.7 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 405/500, 6.7 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 406/500, 6.7 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 407/500, 6.7 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 408/500, 6.7 task/s, elapsed: 61s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 409/500, 6.7 task/s, elapsed: 61s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 410/500, 6.7 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 411/500, 6.7 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 412/500, 6.7 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 413/500, 6.7 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 414/500, 6.7 task/s, elapsed: 62s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 415/500, 6.7 task/s, elapsed: 62s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 416/500, 6.7 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 417/500, 6.7 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 418/500, 6.7 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 419/500, 6.7 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 420/500, 6.7 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 421/500, 6.7 task/s, elapsed: 63s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 422/500, 6.7 task/s, elapsed: 63s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 423/500, 6.7 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 424/500, 6.7 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 425/500, 6.7 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 426/500, 6.7 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 427/500, 6.7 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 428/500, 6.7 task/s, elapsed: 64s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 429/500, 6.7 task/s, elapsed: 64s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 430/500, 6.7 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 431/500, 6.7 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 432/500, 6.7 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 433/500, 6.7 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 434/500, 6.7 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 435/500, 6.7 task/s, elapsed: 65s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 436/500, 6.7 task/s, elapsed: 65s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 437/500, 6.7 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 438/500, 6.7 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 439/500, 6.7 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 440/500, 6.7 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 441/500, 6.7 task/s, elapsed: 66s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 442/500, 6.7 task/s, elapsed: 66s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 443/500, 6.7 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 444/500, 6.7 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 445/500, 6.7 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 446/500, 6.7 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 447/500, 6.7 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 448/500, 6.7 task/s, elapsed: 67s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 449/500, 6.7 task/s, elapsed: 67s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 450/500, 6.7 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 451/500, 6.7 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 452/500, 6.7 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 453/500, 6.7 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 454/500, 6.7 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 455/500, 6.7 task/s, elapsed: 68s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 456/500, 6.7 task/s, elapsed: 68s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 457/500, 6.7 task/s, elapsed: 68s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 458/500, 6.7 task/s, elapsed: 68s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 459/500, 6.7 task/s, elapsed: 68s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 460/500, 6.7 task/s, elapsed: 68s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 461/500, 6.7 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 462/500, 6.7 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 463/500, 6.7 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 464/500, 6.7 task/s, elapsed: 69s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 465/500, 6.7 task/s, elapsed: 69s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 466/500, 6.7 task/s, elapsed: 69s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 467/500, 6.7 task/s, elapsed: 69s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 468/500, 6.7 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 469/500, 6.7 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 470/500, 6.7 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 471/500, 6.7 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 472/500, 6.7 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 473/500, 6.7 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 474/500, 6.7 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 475/500, 6.7 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 476/500, 6.7 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 477/500, 6.7 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 478/500, 6.7 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 479/500, 6.7 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 480/500, 6.7 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 481/500, 6.7 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 482/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 483/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 484/500, 6.7 task/s, elapsed: 72s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 485/500, 6.7 task/s, elapsed: 72s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 486/500, 6.7 task/s, elapsed: 72s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 487/500, 6.7 task/s, elapsed: 72s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 488/500, 6.7 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 489/500, 6.7 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 490/500, 6.7 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 491/500, 6.7 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 492/500, 6.7 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 493/500, 6.7 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 494/500, 6.7 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 495/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 496/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 497/500, 6.7 task/s, elapsed: 74s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 498/500, 6.7 task/s, elapsed: 74s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 499/500, 6.7 task/s, elapsed: 74s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 500/500, 6.7 task/s, elapsed: 74s, ETA:     0s2022-04-19 01:19:41,322 - mmseg - INFO - per class results:
2022-04-19 01:19:41,324 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|      road     | 82.19 | 85.46 |
|    sidewalk   | 37.04 | 76.02 |
|    building   | 88.02 | 94.95 |
|      wall     |  37.9 | 44.47 |
|     fence     |  8.33 |  9.36 |
|      pole     | 49.55 | 57.99 |
| traffic light | 54.69 | 67.35 |
|  traffic sign |  55.2 | 65.07 |
|   vegetation  |  84.6 | 93.48 |
|    terrain    |  0.0  |  0.0  |
|      sky      | 84.17 | 99.18 |
|     person    | 72.62 | 87.68 |
|     rider     | 47.86 | 67.15 |
|      car      | 86.76 | 94.62 |
|     truck     |  0.0  |  0.0  |
|      bus      | 62.27 | 84.46 |
|     train     |  0.0  |  0.0  |
|   motorcycle  | 56.77 | 69.64 |
|    bicycle    | 62.54 | 74.07 |
+---------------+-------+-------+
2022-04-19 01:19:41,324 - mmseg - INFO - Summary:
2022-04-19 01:19:41,324 - mmseg - INFO - 
+------+-------+-------+
| aAcc |  mIoU |  mAcc |
+------+-------+-------+
| 86.8 | 51.08 | 61.63 |
+------+-------+-------+
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:19:41,336 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-19 01:19:41,336 - mmseg - INFO - Iter [500/40000]	lr: 1.800e-05, eta: 5:08:38, time: 1.547, data_time: 0.013, memory: 9636, aAcc: 0.8680, mIoU: 0.5108, mAcc: 0.6163, IoU.road: 0.8219, IoU.sidewalk: 0.3704, IoU.building: 0.8802, IoU.wall: 0.3790, IoU.fence: 0.0833, IoU.pole: 0.4955, IoU.traffic light: 0.5469, IoU.traffic sign: 0.5520, IoU.vegetation: 0.8460, IoU.terrain: 0.0000, IoU.sky: 0.8417, IoU.person: 0.7262, IoU.rider: 0.4786, IoU.car: 0.8676, IoU.truck: 0.0000, IoU.bus: 0.6227, IoU.train: 0.0000, IoU.motorcycle: 0.5677, IoU.bicycle: 0.6254, Acc.road: 0.8546, Acc.sidewalk: 0.7602, Acc.building: 0.9495, Acc.wall: 0.4447, Acc.fence: 0.0936, Acc.pole: 0.5799, Acc.traffic light: 0.6735, Acc.traffic sign: 0.6507, Acc.vegetation: 0.9348, Acc.terrain: 0.0000, Acc.sky: 0.9918, Acc.person: 0.8768, Acc.rider: 0.6715, Acc.car: 0.9462, Acc.truck: 0.0000, Acc.bus: 0.8446, Acc.train: 0.0000, Acc.motorcycle: 0.6964, Acc.bicycle: 0.7407, decode.loss_seg: 0.1041, decode.acc_seg: 90.0509, src.loss_imnet_feat_dist: 0.1011, mix.decode.loss_seg: 0.1172, mix.decode.acc_seg: 89.3290
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:20:55,667 - mmseg - INFO - Iter [28050/40000]	lr: 1.793e-05, eta: 5:08:10, time: 3.858, data_time: 2.387, memory: 9636, decode.loss_seg: 0.1108, decode.acc_seg: 90.5610, src.loss_imnet_feat_dist: 0.1011, mix.decode.loss_seg: 0.1445, mix.decode.acc_seg: 88.6118
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:22:11,810 - mmseg - INFO - Iter [28100/40000]	lr: 1.785e-05, eta: 5:06:53, time: 1.523, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1049, decode.acc_seg: 91.3835, src.loss_imnet_feat_dist: 0.0998, mix.decode.loss_seg: 0.1506, mix.decode.acc_seg: 88.2099
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:23:23,319 - mmseg - INFO - Iter [28150/40000]	lr: 1.778e-05, eta: 5:05:33, time: 1.430, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1054, decode.acc_seg: 90.1546, src.loss_imnet_feat_dist: 0.1022, mix.decode.loss_seg: 0.1307, mix.decode.acc_seg: 88.0457
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:24:37,887 - mmseg - INFO - Iter [28200/40000]	lr: 1.770e-05, eta: 5:04:14, time: 1.491, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1075, decode.acc_seg: 91.6600, src.loss_imnet_feat_dist: 0.0948, mix.decode.loss_seg: 0.1269, mix.decode.acc_seg: 90.3478
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:25:55,986 - mmseg - INFO - Iter [28250/40000]	lr: 1.763e-05, eta: 5:02:57, time: 1.562, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1092, decode.acc_seg: 90.1961, src.loss_imnet_feat_dist: 0.1000, mix.decode.loss_seg: 0.1306, mix.decode.acc_seg: 88.5691
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:27:06,660 - mmseg - INFO - Iter [28300/40000]	lr: 1.755e-05, eta: 5:01:37, time: 1.413, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1113, decode.acc_seg: 90.3743, src.loss_imnet_feat_dist: 0.0987, mix.decode.loss_seg: 0.1213, mix.decode.acc_seg: 88.4850
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:28:19,959 - mmseg - INFO - Iter [28350/40000]	lr: 1.748e-05, eta: 5:00:18, time: 1.466, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1103, decode.acc_seg: 90.4182, src.loss_imnet_feat_dist: 0.1058, mix.decode.loss_seg: 0.1237, mix.decode.acc_seg: 88.2517
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:29:34,819 - mmseg - INFO - Iter [28400/40000]	lr: 1.740e-05, eta: 4:59:00, time: 1.497, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1029, decode.acc_seg: 91.5081, src.loss_imnet_feat_dist: 0.0948, mix.decode.loss_seg: 0.1154, mix.decode.acc_seg: 89.8782
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:30:54,227 - mmseg - INFO - Iter [28450/40000]	lr: 1.733e-05, eta: 4:57:43, time: 1.588, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1105, decode.acc_seg: 91.0529, src.loss_imnet_feat_dist: 0.1008, mix.decode.loss_seg: 0.1328, mix.decode.acc_seg: 88.7142
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:32:10,209 - mmseg - INFO - Iter [28500/40000]	lr: 1.725e-05, eta: 4:56:25, time: 1.520, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1088, decode.acc_seg: 90.9220, src.loss_imnet_feat_dist: 0.1006, mix.decode.loss_seg: 0.1308, mix.decode.acc_seg: 88.9306
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:33:23,280 - mmseg - INFO - Iter [28550/40000]	lr: 1.718e-05, eta: 4:55:06, time: 1.461, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1087, decode.acc_seg: 90.7222, src.loss_imnet_feat_dist: 0.1034, mix.decode.loss_seg: 0.1264, mix.decode.acc_seg: 89.9834
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:34:34,776 - mmseg - INFO - Iter [28600/40000]	lr: 1.710e-05, eta: 4:53:47, time: 1.430, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1060, decode.acc_seg: 91.2662, src.loss_imnet_feat_dist: 0.1008, mix.decode.loss_seg: 0.1199, mix.decode.acc_seg: 89.2328
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:35:51,736 - mmseg - INFO - Iter [28650/40000]	lr: 1.703e-05, eta: 4:52:29, time: 1.539, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1107, decode.acc_seg: 91.0023, src.loss_imnet_feat_dist: 0.0961, mix.decode.loss_seg: 0.1267, mix.decode.acc_seg: 89.5031
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:37:08,625 - mmseg - INFO - Iter [28700/40000]	lr: 1.695e-05, eta: 4:51:12, time: 1.538, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1189, decode.acc_seg: 91.0821, src.loss_imnet_feat_dist: 0.1072, mix.decode.loss_seg: 0.1288, mix.decode.acc_seg: 89.2852
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:38:27,856 - mmseg - INFO - Iter [28750/40000]	lr: 1.688e-05, eta: 4:49:55, time: 1.585, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1046, decode.acc_seg: 91.1233, src.loss_imnet_feat_dist: 0.0980, mix.decode.loss_seg: 0.1235, mix.decode.acc_seg: 89.2463
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:39:23,803 - mmseg - INFO - Iter [13050/40000]	lr: 4.043e-05, eta: 1 day, 1:33:40, time: 3.382, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.3341, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.6686
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:42:11,128 - mmseg - INFO - Iter [13100/40000]	lr: 4.035e-05, eta: 1 day, 1:30:43, time: 3.347, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.9984, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.0891
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:44:59,163 - mmseg - INFO - Iter [13150/40000]	lr: 4.028e-05, eta: 1 day, 1:27:47, time: 3.361, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.6184, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.2556
36, decode.loss_seg: 0.1101, decode.acc_seg: 91.0206, src.loss_imnet_feat_dist: 0.0922, mix.decode.loss_seg: 0.1258, mix.decode.acc_seg: 88.6741
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:46:13,432 - mmseg - INFO - Iter [29050/40000]	lr: 1.643e-05, eta: 4:42:12, time: 1.593, data_time: 0.014, memory: 9636, decode.loss_seg: 0.0999, decode.acc_seg: 91.4268, src.loss_imnet_feat_dist: 0.0973, mix.decode.loss_seg: 0.1289, mix.decode.acc_seg: 88.8021
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:47:29,044 - mmseg - INFO - Iter [29100/40000]	lr: 1.635e-05, eta: 4:40:54, time: 1.512, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1105, decode.acc_seg: 89.0506, src.loss_imnet_feat_dist: 0.0964, mix.decode.loss_seg: 0.1259, mix.decode.acc_seg: 87.7441
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:48:41,628 - mmseg - INFO - Iter [29150/40000]	lr: 1.628e-05, eta: 4:39:35, time: 1.452, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1060, decode.acc_seg: 90.8585, src.loss_imnet_feat_dist: 0.1062, mix.decode.loss_seg: 0.1351, mix.decode.acc_seg: 88.3934
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:49:52,155 - mmseg - INFO - Iter [29200/40000]	lr: 1.620e-05, eta: 4:38:15, time: 1.411, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1067, decode.acc_seg: 91.6773, src.loss_imnet_feat_dist: 0.1010, mix.decode.loss_seg: 0.1228, mix.decode.acc_seg: 89.8196
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:51:10,414 - mmseg - INFO - Iter [29250/40000]	lr: 1.613e-05, eta: 4:36:58, time: 1.565, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1104, decode.acc_seg: 91.5832, src.loss_imnet_feat_dist: 0.1016, mix.decode.loss_seg: 0.1224, mix.decode.acc_seg: 90.0384
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:52:28,006 - mmseg - INFO - Iter [29300/40000]	lr: 1.605e-05, eta: 4:35:41, time: 1.552, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1072, decode.acc_seg: 91.5145, src.loss_imnet_feat_dist: 0.0992, mix.decode.loss_seg: 0.1207, mix.decode.acc_seg: 89.0371
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:53:47,250 - mmseg - INFO - Iter [29350/40000]	lr: 1.598e-05, eta: 4:34:24, time: 1.585, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1019, decode.acc_seg: 91.2894, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1239, mix.decode.acc_seg: 89.6905
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:55:03,166 - mmseg - INFO - Iter [29400/40000]	lr: 1.590e-05, eta: 4:33:07, time: 1.518, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1011, decode.acc_seg: 89.4879, src.loss_imnet_feat_dist: 0.0988, mix.decode.loss_seg: 0.1174, mix.decode.acc_seg: 88.2104
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:56:21,420 - mmseg - INFO - Iter [29450/40000]	lr: 1.583e-05, eta: 4:31:50, time: 1.565, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1011, decode.acc_seg: 91.6129, src.loss_imnet_feat_dist: 0.0973, mix.decode.loss_seg: 0.1141, mix.decode.acc_seg: 89.4465
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:57:38,724 - mmseg - INFO - Iter [29500/40000]	lr: 1.575e-05, eta: 4:30:32, time: 1.546, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1068, decode.acc_seg: 91.1370, src.loss_imnet_feat_dist: 0.1004, mix.decode.loss_seg: 0.1118, mix.decode.acc_seg: 89.8874
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 01:58:57,936 - mmseg - INFO - Iter [13400/40000]	lr: 3.990e-05, eta: 1 day, 1:13:04, time: 3.329, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.7814, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.2157
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:01:46,461 - mmseg - INFO - Iter [13450/40000]	lr: 3.983e-05, eta: 1 day, 1:10:09, time: 3.370, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.0483, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.9925
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:04:34,465 - mmseg - INFO - Iter [13500/40000]	lr: 3.975e-05, eta: 1 day, 1:07:13, time: 3.360, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.0372, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.6794
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:07:22,262 - mmseg - INFO - Iter [13550/40000]	lr: 3.968e-05, eta: 1 day, 1:04:17, time: 3.356, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.2886, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.8969
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:10:09,801 - mmseg - INFO - Iter [13600/40000]	lr: 3.960e-05, eta: 1 day, 1:01:20, time: 3.351, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.6425, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.9246
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:12:58,160 - mmseg - INFO - Iter [13650/40000]	lr: 3.953e-05, eta: 1 day, 0:58:25, time: 3.367, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.5196, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.2847
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:15:46,829 - mmseg - INFO - Iter [13700/40000]	lr: 3.945e-05, eta: 1 day, 0:55:31, time: 3.373, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.0901, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.1514
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:18:37,059 - mmseg - INFO - Iter [13750/40000]	lr: 3.938e-05, eta: 1 day, 0:52:40, time: 3.405, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.7034, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.6684
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:21:26,066 - mmseg - INFO - Iter [13800/40000]	lr: 3.930e-05, eta: 1 day, 0:49:46, time: 3.380, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.4287, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.0349
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:24:13,943 - mmseg - INFO - Iter [13850/40000]	lr: 3.923e-05, eta: 1 day, 0:46:51, time: 3.358, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.0806, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.3765
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:27:01,790 - mmseg - INFO - Iter [13900/40000]	lr: 3.915e-05, eta: 1 day, 0:43:55, time: 3.357, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.0426, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.0170
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:29:50,828 - mmseg - INFO - Iter [13950/40000]	lr: 3.908e-05, eta: 1 day, 0:41:01, time: 3.381, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 18.7682, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.9857
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:32:39,482 - mmseg - INFO - Exp name: 220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02
2022-04-19 02:32:39,483 - mmseg - INFO - Iter [14000/40000]	lr: 3.900e-05, eta: 1 day, 0:38:07, time: 3.373, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.7721, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.1440
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:35:28,202 - mmseg - INFO - Iter [14050/40000]	lr: 3.893e-05, eta: 1 day, 0:35:13, time: 3.374, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.0667, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.6997
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:38:16,118 - mmseg - INFO - Iter [14100/40000]	lr: 3.885e-05, eta: 1 day, 0:32:18, time: 3.358, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.2140, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.8321
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:41:05,112 - mmseg - INFO - Iter [14150/40000]	lr: 3.878e-05, eta: 1 day, 0:29:25, time: 3.380, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.5921, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.6671
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:43:51,605 - mmseg - INFO - Iter [14200/40000]	lr: 3.870e-05, eta: 1 day, 0:26:27, time: 3.330, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.0220, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.6547
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:46:40,905 - mmseg - INFO - Iter [14250/40000]	lr: 3.863e-05, eta: 1 day, 0:23:34, time: 3.386, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 17.6047, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 55.9033
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:49:08,342 - mmseg - INFO - Iter [31550/40000]	lr: 1.268e-05, eta: 3:37:22, time: 1.520, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1047, decode.acc_seg: 90.4686, src.loss_imnet_feat_dist: 0.0987, mix.decode.loss_seg: 0.1256, mix.decode.acc_seg: 89.0700
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:50:23,501 - mmseg - INFO - Iter [31600/40000]	lr: 1.260e-05, eta: 3:36:04, time: 1.503, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1059, decode.acc_seg: 90.2637, src.loss_imnet_feat_dist: 0.0960, mix.decode.loss_seg: 0.1189, mix.decode.acc_seg: 88.4161
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:51:42,721 - mmseg - INFO - Iter [31650/40000]	lr: 1.253e-05, eta: 3:34:47, time: 1.584, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1035, decode.acc_seg: 90.9440, src.loss_imnet_feat_dist: 0.1031, mix.decode.loss_seg: 0.1295, mix.decode.acc_seg: 89.0784
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:53:00,826 - mmseg - INFO - Iter [31700/40000]	lr: 1.245e-05, eta: 3:33:30, time: 1.562, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1019, decode.acc_seg: 90.8915, src.loss_imnet_feat_dist: 0.1014, mix.decode.loss_seg: 0.1170, mix.decode.acc_seg: 88.9171
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:54:19,284 - mmseg - INFO - Iter [31750/40000]	lr: 1.238e-05, eta: 3:32:14, time: 1.569, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1010, decode.acc_seg: 91.3101, src.loss_imnet_feat_dist: 0.1010, mix.decode.loss_seg: 0.1160, mix.decode.acc_seg: 89.8562
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:55:36,894 - mmseg - INFO - Iter [31800/40000]	lr: 1.230e-05, eta: 3:30:57, time: 1.552, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1139, decode.acc_seg: 91.0787, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1390, mix.decode.acc_seg: 89.1130
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:56:56,247 - mmseg - INFO - Iter [31850/40000]	lr: 1.223e-05, eta: 3:29:40, time: 1.587, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0972, decode.acc_seg: 91.4021, src.loss_imnet_feat_dist: 0.1011, mix.decode.loss_seg: 0.1262, mix.decode.acc_seg: 89.4556
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:58:09,732 - mmseg - INFO - Iter [31900/40000]	lr: 1.215e-05, eta: 3:28:22, time: 1.470, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1023, decode.acc_seg: 90.9309, src.loss_imnet_feat_dist: 0.1003, mix.decode.loss_seg: 0.1244, mix.decode.acc_seg: 89.5916
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 02:59:22,047 - mmseg - INFO - Iter [31950/40000]	lr: 1.208e-05, eta: 3:27:03, time: 1.446, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1121, decode.acc_seg: 90.6584, src.loss_imnet_feat_dist: 0.0958, mix.decode.loss_seg: 0.1357, mix.decode.acc_seg: 87.6753
[                                                  ] 0/500, elapsed: 0s, ETA:
[                                 ] 1/500, 1.4 task/s, elapsed: 1s, ETA:   357s
[                                 ] 2/500, 2.3 task/s, elapsed: 1s, ETA:   218s
[                                 ] 3/500, 2.9 task/s, elapsed: 1s, ETA:   170s
[                                 ] 4/500, 3.4 task/s, elapsed: 1s, ETA:   148s
[                                 ] 5/500, 3.7 task/s, elapsed: 1s, ETA:   134s
[                                 ] 6/500, 4.0 task/s, elapsed: 2s, ETA:   124s
[                                 ] 7/500, 4.2 task/s, elapsed: 2s, ETA:   117s
[                                 ] 8/500, 4.4 task/s, elapsed: 2s, ETA:   111s
[                                 ] 9/500, 4.6 task/s, elapsed: 2s, ETA:   107s
[                                ] 10/500, 4.8 task/s, elapsed: 2s, ETA:   103s
[                                ] 11/500, 4.9 task/s, elapsed: 2s, ETA:   100s
[                                ] 12/500, 5.0 task/s, elapsed: 2s, ETA:    98s
[                                ] 13/500, 5.1 task/s, elapsed: 3s, ETA:    95s
[                                ] 14/500, 5.2 task/s, elapsed: 3s, ETA:    93s
[                                ] 15/500, 5.3 task/s, elapsed: 3s, ETA:    92s
[>                               ] 16/500, 5.4 task/s, elapsed: 3s, ETA:    90s
[>                               ] 17/500, 5.4 task/s, elapsed: 3s, ETA:    89s
[>                               ] 18/500, 5.5 task/s, elapsed: 3s, ETA:    88s
[>                               ] 19/500, 5.5 task/s, elapsed: 3s, ETA:    87s
[>                               ] 20/500, 5.6 task/s, elapsed: 4s, ETA:    86s
[>                               ] 21/500, 5.6 task/s, elapsed: 4s, ETA:    85s
[>                               ] 22/500, 5.7 task/s, elapsed: 4s, ETA:    84s
[>                               ] 23/500, 5.7 task/s, elapsed: 4s, ETA:    83s
[>                               ] 24/500, 5.8 task/s, elapsed: 4s, ETA:    83s
[>                               ] 25/500, 5.8 task/s, elapsed: 4s, ETA:    82s
[>                               ] 26/500, 5.8 task/s, elapsed: 4s, ETA:    81s
[>                               ] 27/500, 5.9 task/s, elapsed: 5s, ETA:    81s
[>                               ] 28/500, 5.9 task/s, elapsed: 5s, ETA:    80s
[>                               ] 29/500, 5.9 task/s, elapsed: 5s, ETA:    80s/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:00:40,851 - mmseg - INFO - Iter [14500/40000]	lr: 3.825e-05, eta: 1 day, 0:08:59, time: 3.342, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.9605, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.6888
0 task/s, elapsed: 6s, ETA:    77s
[>>                              ] 38/500, 6.0 task/s, elapsed: 6s, ETA:    77s
[>>                              ] 39/500, 6.0 task/s, elapsed: 6s, ETA:    77s
[>>                              ] 40/500, 6.0 task/s, elapsed: 7s, ETA:    76s
[>>                              ] 41/500, 6.0 task/s, elapsed: 7s, ETA:    76s
[>>                              ] 42/500, 6.1 task/s, elapsed: 7s, ETA:    76s
[>>                              ] 43/500, 6.1 task/s, elapsed: 7s, ETA:    75s
[>>                              ] 44/500, 6.1 task/s, elapsed: 7s, ETA:    75s
[>>                              ] 45/500, 6.1 task/s, elapsed: 7s, ETA:    75s
[>>                              ] 46/500, 6.1 task/s, elapsed: 8s, ETA:    75s
[>>>                             ] 47/500, 6.1 task/s, elapsed: 8s, ETA:    74s
[>>>                             ] 48/500, 6.1 task/s, elapsed: 8s, ETA:    74s
[>>>                             ] 49/500, 6.1 task/s, elapsed: 8s, ETA:    74s
[>>>                             ] 50/500, 6.1 task/s, elapsed: 8s, ETA:    74s
[>>>                             ] 51/500, 6.1 task/s, elapsed: 8s, ETA:    74s
[>>>                             ] 52/500, 6.1 task/s, elapsed: 9s, ETA:    73s
[>>>                             ] 53/500, 6.1 task/s, elapsed: 9s, ETA:    73s
[>>>                             ] 54/500, 6.1 task/s, elapsed: 9s, ETA:    73s
[>>>                             ] 55/500, 6.1 task/s, elapsed: 9s, ETA:    73s
[>>>                             ] 56/500, 6.1 task/s, elapsed: 9s, ETA:    73s
[>>>                             ] 57/500, 6.1 task/s, elapsed: 9s, ETA:    72s
[>>>                            ] 58/500, 6.1 task/s, elapsed: 10s, ETA:    72s
[>>>                            ] 59/500, 6.1 task/s, elapsed: 10s, ETA:    72s
[>>>                            ] 60/500, 6.1 task/s, elapsed: 10s, ETA:    72s
[>>>                            ] 61/500, 6.1 task/s, elapsed: 10s, ETA:    72s
[>>>                            ] 62/500, 6.1 task/s, elapsed: 10s, ETA:    72s
[>>>                            ] 63/500, 6.1 task/s, elapsed: 10s, ETA:    72s
[>>>                            ] 64/500, 6.1 task/s, elapsed: 11s, ETA:    72s
[>>>>                           ] 65/500, 6.1 task/s, elapsed: 11s, ETA:    72s
[>>>>                           ] 66/500, 6.1 task/s, elapsed: 11s, ETA:    71s
[>>>>                           ] 67/500, 6.1 task/s, elapsed: 11s, ETA:    71s
[>>>>                           ] 68/500, 6.1 task/s, elapsed: 11s, ETA:    71s
[>>>>                           ] 69/500, 6.1 task/s, elapsed: 11s, ETA:    71s
[>>>>                           ] 70/500, 6.1 task/s, elapsed: 11s, ETA:    70s
[>>>>                           ] 71/500, 6.1 task/s, elapsed: 12s, ETA:    70s
[>>>>                           ] 72/500, 6.1 task/s, elapsed: 12s, ETA:    70s
[>>>>                           ] 73/500, 6.1 task/s, elapsed: 12s, ETA:    70s
[>>>>                           ] 74/500, 6.1 task/s, elapsed: 12s, ETA:    69s
[>>>>                           ] 75/500, 6.1 task/s, elapsed: 12s, ETA:    69s
[>>>>                           ] 76/500, 6.1 task/s, elapsed: 12s, ETA:    69s
[>>>>                           ] 77/500, 6.1 task/s, elapsed: 13s, ETA:    69s
[>>>>                           ] 78/500, 6.1 task/s, elapsed: 13s, ETA:    69s
[>>>>                           ] 79/500, 6.1 task/s, elapsed: 13s, ETA:    69s
[>>>>                           ] 80/500, 6.1 task/s, elapsed: 13s, ETA:    68s
[>>>>>                          ] 81/500, 6.1 task/s, elapsed: 13s, ETA:    68s
[>>>>>                          ] 82/500, 6.2 task/s, elapsed: 13s, ETA:    68s
[>>>>>                          ] 83/500, 6.2 task/s, elapsed: 13s, ETA:    68s
[>>>>>                          ] 84/500, 6.2 task/s, elapsed: 14s, ETA:    68s
[>>>>>                          ] 85/500, 6.2 task/s, elapsed: 14s, ETA:    67s
[>>>>>                          ] 86/500, 6.2 task/s, elapsed: 14s, ETA:    67s
[>>>>>                          ] 87/500, 6.2 task/s, elapsed: 14s, ETA:    67s
[>>>>>                          ] 88/500, 6.2 task/s, elapsed: 14s, ETA:    67s
[>>>>>                          ] 89/500, 6.2 task/s, elapsed: 14s, ETA:    67s
[>>>>>                          ] 90/500, 6.2 task/s, elapsed: 15s, ETA:    66s
[>>>>>                          ] 91/500, 6.2 task/s, elapsed: 15s, ETA:    66s
[>>>>>                          ] 92/500, 6.2 task/s, elapsed: 15s, ETA:    66s
[>>>>>                          ] 93/500, 6.2 task/s, elapsed: 15s, ETA:    66s
[>>>>>                          ] 94/500, 6.2 task/s, elapsed: 15s, ETA:    66s
[>>>>>                          ] 95/500, 6.2 task/s, elapsed: 15s, ETA:    66s
[>>>>>                          ] 96/500, 6.2 task/s, elapsed: 16s, ETA:    65s
[>>>>>>                         ] 97/500, 6.2 task/s, elapsed: 16s, ETA:    65s
[>>>>>>                         ] 98/500, 6.2 task/s, elapsed: 16s, ETA:    65s
[>>>>>>                         ] 99/500, 6.2 task/s, elapsed: 16s, ETA:    65s
[>>>>>>                        ] 100/500, 6.2 task/s, elapsed: 16s, ETA:    65s
[>>>>>>                        ] 101/500, 6.2 task/s, elapsed: 16s, ETA:    64s
[>>>>>>                        ] 102/500, 6.2 task/s, elapsed: 16s, ETA:    64s
[>>>>>>                        ] 103/500, 6.2 task/s, elapsed: 17s, ETA:    64s
[>>>>>>                        ] 104/500, 6.2 task/s, elapsed: 17s, ETA:    64s
[>>>>>>                        ] 105/500, 6.2 task/s, elapsed: 17s, ETA:    64s
[>>>>>>                        ] 106/500, 6.2 task/s, elapsed: 17s, ETA:    64s
[>>>>>>                        ] 107/500, 6.2 task/s, elapsed: 17s, ETA:    63s
[>>>>>>                        ] 108/500, 6.2 task/s, elapsed: 17s, ETA:    63s
[>>>>>>                        ] 109/500, 6.2 task/s, elapsed: 18s, ETA:    63s
[>>>>>>                        ] 110/500, 6.2 task/s, elapsed: 18s, ETA:    63s
[>>>>>>                        ] 111/500, 6.2 task/s, elapsed: 18s, ETA:    63s
[>>>>>>                        ] 112/500, 6.2 task/s, elapsed: 18s, ETA:    62s
[>>>>>>                        ] 113/500, 6.2 task/s, elapsed: 18s, ETA:    62s
[>>>>>>                        ] 114/500, 6.2 task/s, elapsed: 18s, ETA:    62s
[>>>>>>                        ] 115/500, 6.2 task/s, elapsed: 18s, ETA:    62s
[>>>>>>                        ] 116/500, 6.2 task/s, elapsed: 19s, ETA:    62s
[>>>>>>>                       ] 117/500, 6.2 task/s, elapsed: 19s, ETA:    61s
[>>>>>>>                       ] 118/500, 6.2 task/s, elapsed: 19s, ETA:    61s
[>>>>>>>                       ] 119/500, 6.3 task/s, elapsed: 19s, ETA:    61s
[>>>>>>>                       ] 120/500, 6.3 task/s, elapsed: 19s, ETA:    61s
[>>>>>>>                       ] 121/500, 6.3 task/s, elapsed: 19s, ETA:    60s
[>>>>>>>                       ] 122/500, 6.3 task/s, elapsed: 19s, ETA:    60s
[>>>>>>>                       ] 123/500, 6.3 task/s, elapsed: 20s, ETA:    60s
[>>>>>>>                       ] 124/500, 6.3 task/s, elapsed: 20s, ETA:    60s
[>>>>>>>                       ] 125/500, 6.3 task/s, elapsed: 20s, ETA:    60s
[>>>>>>>                       ] 126/500, 6.3 task/s, elapsed: 20s, ETA:    60s
[>>>>>>>                       ] 127/500, 6.3 task/s, elapsed: 20s, ETA:    59s
[>>>>>>>                       ] 128/500, 6.3 task/s, elapsed: 20s, ETA:    59s
[>>>>>>>                       ] 129/500, 6.3 task/s, elapsed: 20s, ETA:    59s
[>>>>>>>                       ] 130/500, 6.3 task/s, elapsed: 21s, ETA:    59s
[>>>>>>>                       ] 131/500, 6.3 task/s, elapsed: 21s, ETA:    58s
[>>>>>>>                       ] 132/500, 6.3 task/s, elapsed: 21s, ETA:    58s
[>>>>>>>                       ] 133/500, 6.3 task/s, elapsed: 21s, ETA:    58s
[>>>>>>>>                      ] 134/500, 6.3 task/s, elapsed: 21s, ETA:    58s
[>>>>>>>>                      ] 135/500, 6.3 task/s, elapsed: 21s, ETA:    58s
[>>>>>>>>                      ] 136/500, 6.3 task/s, elapsed: 22s, ETA:    58s
[>>>>>>>>                      ] 137/500, 6.3 task/s, elapsed: 22s, ETA:    57s
[>>>>>>>>                      ] 138/500, 6.3 task/s, elapsed: 22s, ETA:    57s
[>>>>>>>>                      ] 139/500, 6.3 task/s, elapsed: 22s, ETA:    57s
[>>>>>>>>                      ] 140/500, 6.3 task/s, elapsed: 22s, ETA:    57s
[>>>>>>>>                      ] 141/500, 6.3 task/s, elapsed: 22s, ETA:    57s
[>>>>>>>>                      ] 142/500, 6.3 task/s, elapsed: 22s, ETA:    56s
[>>>>>>>>                      ] 143/500, 6.3 task/s, elapsed: 23s, ETA:    56s
[>>>>>>>>                      ] 144/500, 6.4 task/s, elapsed: 23s, ETA:    56s
[>>>>>>>>                      ] 145/500, 6.4 task/s, elapsed: 23s, ETA:    56s
[>>>>>>>>                      ] 146/500, 6.4 task/s, elapsed: 23s, ETA:    56s
[>>>>>>>>                      ] 147/500, 6.3 task/s, elapsed: 23s, ETA:    56s
[>>>>>>>>                      ] 148/500, 6.3 task/s, elapsed: 23s, ETA:    55s
[>>>>>>>>                      ] 149/500, 6.3 task/s, elapsed: 23s, ETA:    55s
[>>>>>>>>>                     ] 150/500, 6.4 task/s, elapsed: 24s, ETA:    55s
[>>>>>>>>>                     ] 151/500, 6.4 task/s, elapsed: 24s, ETA:    55s
[>>>>>>>>>                     ] 152/500, 6.4 task/s, elapsed: 24s, ETA:    55s
[>>>>>>>>>                     ] 153/500, 6.4 task/s, elapsed: 24s, ETA:    55s
[>>>>>>>>>                     ] 154/500, 6.4 task/s, elapsed: 24s, ETA:    54s
[>>>>>>>>>                     ] 155/500, 6.4 task/s, elapsed: 24s, ETA:    54s
[>>>>>>>>>                     ] 156/500, 6.4 task/s, elapsed: 25s, ETA:    54s
[>>>>>>>>>                     ] 157/500, 6.4 task/s, elapsed: 25s, ETA:    54s
[>>>>>>>>>                     ] 158/500, 6.4 task/s, elapsed: 25s, ETA:    54s
[>>>>>>>>>                     ] 159/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 160/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 161/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 162/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 163/500, 6.4 task/s, elapsed: 26s, ETA:    53s
[>>>>>>>>>                     ] 164/500, 6.4 task/s, elapsed: 26s, ETA:    53s
[>>>>>>>>>                     ] 165/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>                     ] 166/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>>                    ] 167/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>>                    ] 168/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>>                    ] 169/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>>                    ] 170/500, 6.4 task/s, elapsed: 27s, ETA:    52s
[>>>>>>>>>>                    ] 171/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 172/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 173/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 174/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 175/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 176/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 177/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 178/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 179/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 180/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 181/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 182/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 183/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 184/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 185/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 186/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 187/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 188/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 189/500, 6.4 task/s, elapsed: 29s, ETA:    48s
[>>>>>>>>>>>                   ] 190/500, 6.4 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 191/500, 6.4 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 192/500, 6.4 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 193/500, 6.4 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 194/500, 6.4 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 195/500, 6.4 task/s, elapsed: 30s, ETA:    47s
[>>>>>>>>>>>                   ] 196/500, 6.4 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>                   ] 197/500, 6.4 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>                   ] 198/500, 6.4 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>                   ] 199/500, 6.4 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>>                  ] 200/500, 6.4 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>>                  ] 201/500, 6.4 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>>                  ] 202/500, 6.4 task/s, elapsed: 31s, ETA:    46s
[>>>>>>>>>>>>                  ] 203/500, 6.4 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 204/500, 6.4 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 205/500, 6.4 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 206/500, 6.4 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 207/500, 6.4 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 208/500, 6.4 task/s, elapsed: 32s, ETA:    45s
[>>>>>>>>>>>>                  ] 209/500, 6.4 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 210/500, 6.4 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 211/500, 6.4 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 212/500, 6.4 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 213/500, 6.4 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 214/500, 6.4 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 215/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>                  ] 216/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 217/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 218/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 219/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 220/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 221/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 222/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 223/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 224/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 225/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 226/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 227/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 228/500, 6.4 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>                 ] 229/500, 6.4 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>                 ] 230/500, 6.4 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>                 ] 231/500, 6.4 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>                 ] 232/500, 6.4 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>                 ] 233/500, 6.4 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>>                ] 234/500, 6.4 task/s, elapsed: 37s, ETA:    42s
[>>>>>>>>>>>>>>                ] 235/500, 6.4 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 236/500, 6.4 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 237/500, 6.4 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 238/500, 6.4 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 239/500, 6.4 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 240/500, 6.4 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 241/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 242/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 243/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 244/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 245/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 246/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 247/500, 6.4 task/s, elapsed: 38s, ETA:    39s
[>>>>>>>>>>>>>>                ] 248/500, 6.4 task/s, elapsed: 39s, ETA:    39s
[>>>>>>>>>>>>>>                ] 249/500, 6.4 task/s, elapsed: 39s, ETA:    39s
[>>>>>>>>>>>>>>>               ] 250/500, 6.4 task/s, elapsed: 39s, ETA:    39s
[>>>>>>>>>>>>>>>               ] 251/500, 6.4 task/s, elapsed: 39s, ETA:    39s
[>>>>>>>>>>>>>>>               ] 252/500, 6.4 task/s, elapsed: 39s, ETA:    39s
[>>>>>>>>>>>>>>>               ] 253/500, 6.4 task/s, elapsed: 39s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 254/500, 6.4 task/s, elapsed: 39s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 255/500, 6.4 task/s, elapsed: 40s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 256/500, 6.4 task/s, elapsed: 40s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 257/500, 6.4 task/s, elapsed: 40s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 258/500, 6.4 task/s, elapsed: 40s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 259/500, 6.4 task/s, elapsed: 40s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 260/500, 6.4 task/s, elapsed: 40s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 261/500, 6.4 task/s, elapsed: 40s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 262/500, 6.5 task/s, elapsed: 41s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 263/500, 6.5 task/s, elapsed: 41s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 264/500, 6.5 task/s, elapsed: 41s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 265/500, 6.5 task/s, elapsed: 41s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 266/500, 6.5 task/s, elapsed: 41s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 267/500, 6.5 task/s, elapsed: 41s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 268/500, 6.5 task/s, elapsed: 41s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 269/500, 6.5 task/s, elapsed: 42s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 270/500, 6.5 task/s, elapsed: 42s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 271/500, 6.5 task/s, elapsed: 42s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 272/500, 6.5 task/s, elapsed: 42s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 273/500, 6.5 task/s, elapsed: 42s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 274/500, 6.5 task/s, elapsed: 42s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 275/500, 6.5 task/s, elapsed: 43s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 276/500, 6.5 task/s, elapsed: 43s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 277/500, 6.5 task/s, elapsed: 43s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 278/500, 6.5 task/s, elapsed: 43s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 279/500, 6.5 task/s, elapsed: 43s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 280/500, 6.5 task/s, elapsed: 43s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 281/500, 6.5 task/s, elapsed: 43s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 282/500, 6.5 task/s, elapsed: 44s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 283/500, 6.5 task/s, elapsed: 44s, ETA:    34s
[>>>>>>>>>>>>>>>>>             ] 284/500, 6.5 task/s, elapsed: 44s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 285/500, 6.5 task/s, elapsed: 44s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 286/500, 6.5 task/s, elapsed: 44s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 287/500, 6.5 task/s, elapsed: 44s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 288/500, 6.5 task/s, elapsed: 44s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 289/500, 6.5 task/s, elapsed: 45s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 290/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 291/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 292/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 293/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 294/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 295/500, 6.5 task/s, elapsed: 46s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 296/500, 6.5 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 297/500, 6.5 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 298/500, 6.5 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 299/500, 6.5 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>>            ] 300/500, 6.5 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>>            ] 301/500, 6.5 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>>            ] 302/500, 6.5 task/s, elapsed: 47s, ETA:    31s
[>>>>>>>>>>>>>>>>>>            ] 303/500, 6.5 task/s, elapsed: 47s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 304/500, 6.5 task/s, elapsed: 47s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 305/500, 6.5 task/s, elapsed: 47s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 306/500, 6.5 task/s, elapsed: 47s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 307/500, 6.5 task/s, elapsed: 47s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 308/500, 6.5 task/s, elapsed: 47s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 309/500, 6.5 task/s, elapsed: 48s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 310/500, 6.5 task/s, elapsed: 48s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 311/500, 6.5 task/s, elapsed: 48s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 312/500, 6.5 task/s, elapsed: 48s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 313/500, 6.5 task/s, elapsed: 48s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 314/500, 6.5 task/s, elapsed: 48s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 315/500, 6.5 task/s, elapsed: 48s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 316/500, 6.5 task/s, elapsed: 49s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 317/500, 6.5 task/s, elapsed: 49s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 318/500, 6.5 task/s, elapsed: 49s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 319/500, 6.5 task/s, elapsed: 49s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 320/500, 6.5 task/s, elapsed: 49s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 321/500, 6.5 task/s, elapsed: 49s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 322/500, 6.5 task/s, elapsed: 50s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 323/500, 6.5 task/s, elapsed: 50s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 324/500, 6.5 task/s, elapsed: 50s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 325/500, 6.5 task/s, elapsed: 50s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 326/500, 6.5 task/s, elapsed: 50s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 327/500, 6.5 task/s, elapsed: 50s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 328/500, 6.5 task/s, elapsed: 50s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 329/500, 6.5 task/s, elapsed: 51s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 330/500, 6.5 task/s, elapsed: 51s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 331/500, 6.5 task/s, elapsed: 51s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 332/500, 6.5 task/s, elapsed: 51s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 333/500, 6.5 task/s, elapsed: 51s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>          ] 334/500, 6.5 task/s, elapsed: 51s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 335/500, 6.5 task/s, elapsed: 51s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 336/500, 6.5 task/s, elapsed: 52s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 337/500, 6.5 task/s, elapsed: 52s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 338/500, 6.5 task/s, elapsed: 52s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 339/500, 6.5 task/s, elapsed: 52s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 340/500, 6.5 task/s, elapsed: 52s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 341/500, 6.5 task/s, elapsed: 52s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 342/500, 6.5 task/s, elapsed: 52s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 343/500, 6.5 task/s, elapsed: 53s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 344/500, 6.5 task/s, elapsed: 53s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 345/500, 6.5 task/s, elapsed: 53s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 346/500, 6.5 task/s, elapsed: 53s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 347/500, 6.5 task/s, elapsed: 53s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 348/500, 6.5 task/s, elapsed: 53s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 349/500, 6.5 task/s, elapsed: 53s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>         ] 350/500, 6.5 task/s, elapsed: 54s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>         ] 351/500, 6.5 task/s, elapsed: 54s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>         ] 352/500, 6.5 task/s, elapsed: 54s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>         ] 353/500, 6.5 task/s, elapsed: 54s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 354/500, 6.5 task/s, elapsed: 54s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 355/500, 6.5 task/s, elapsed: 54s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 356/500, 6.5 task/s, elapsed: 54s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 357/500, 6.5 task/s, elapsed: 55s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 358/500, 6.5 task/s, elapsed: 55s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 359/500, 6.5 task/s, elapsed: 55s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 360/500, 6.5 task/s, elapsed: 55s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 361/500, 6.5 task/s, elapsed: 55s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 362/500, 6.5 task/s, elapsed: 55s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 363/500, 6.5 task/s, elapsed: 55s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 364/500, 6.5 task/s, elapsed: 56s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 365/500, 6.5 task/s, elapsed: 56s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 366/500, 6.5 task/s, elapsed: 56s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 367/500, 6.5 task/s, elapsed: 56s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 368/500, 6.5 task/s, elapsed: 56s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 369/500, 6.5 task/s, elapsed: 56s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 370/500, 6.5 task/s, elapsed: 56s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 371/500, 6.5 task/s, elapsed: 57s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 372/500, 6.6 task/s, elapsed: 57s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 373/500, 6.6 task/s, elapsed: 57s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 374/500, 6.6 task/s, elapsed: 57s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 375/500, 6.6 task/s, elapsed: 57s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 376/500, 6.6 task/s, elapsed: 57s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 377/500, 6.6 task/s, elapsed: 58s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 378/500, 6.6 task/s, elapsed: 58s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 379/500, 6.6 task/s, elapsed: 58s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 380/500, 6.6 task/s, elapsed: 58s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 381/500, 6.6 task/s, elapsed: 58s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 382/500, 6.6 task/s, elapsed: 58s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 383/500, 6.6 task/s, elapsed: 58s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 384/500, 6.6 task/s, elapsed: 59s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 385/500, 6.6 task/s, elapsed: 59s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 386/500, 6.6 task/s, elapsed: 59s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 387/500, 6.6 task/s, elapsed: 59s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 388/500, 6.6 task/s, elapsed: 59s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 389/500, 6.6 task/s, elapsed: 59s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 390/500, 6.6 task/s, elapsed: 59s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 391/500, 6.6 task/s, elapsed: 60s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 392/500, 6.6 task/s, elapsed: 60s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 393/500, 6.6 task/s, elapsed: 60s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 394/500, 6.6 task/s, elapsed: 60s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 395/500, 6.6 task/s, elapsed: 60s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 396/500, 6.6 task/s, elapsed: 60s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 397/500, 6.6 task/s, elapsed: 60s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 398/500, 6.6 task/s, elapsed: 61s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 399/500, 6.6 task/s, elapsed: 61s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 400/500, 6.6 task/s, elapsed: 61s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 401/500, 6.6 task/s, elapsed: 61s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 402/500, 6.6 task/s, elapsed: 61s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 403/500, 6.6 task/s, elapsed: 61s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 404/500, 6.6 task/s, elapsed: 61s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 405/500, 6.6 task/s, elapsed: 62s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 406/500, 6.6 task/s, elapsed: 62s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 407/500, 6.6 task/s, elapsed: 62s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 408/500, 6.6 task/s, elapsed: 62s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 409/500, 6.6 task/s, elapsed: 62s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 410/500, 6.6 task/s, elapsed: 62s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 411/500, 6.6 task/s, elapsed: 62s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 412/500, 6.6 task/s, elapsed: 63s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 413/500, 6.6 task/s, elapsed: 63s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 414/500, 6.6 task/s, elapsed: 63s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 415/500, 6.6 task/s, elapsed: 63s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 416/500, 6.6 task/s, elapsed: 63s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 417/500, 6.6 task/s, elapsed: 63s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 418/500, 6.6 task/s, elapsed: 63s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 419/500, 6.6 task/s, elapsed: 64s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 420/500, 6.6 task/s, elapsed: 64s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 421/500, 6.6 task/s, elapsed: 64s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 422/500, 6.6 task/s, elapsed: 64s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 423/500, 6.6 task/s, elapsed: 64s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 424/500, 6.6 task/s, elapsed: 64s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 425/500, 6.6 task/s, elapsed: 64s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 426/500, 6.6 task/s, elapsed: 65s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 427/500, 6.6 task/s, elapsed: 65s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 428/500, 6.6 task/s, elapsed: 65s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 429/500, 6.6 task/s, elapsed: 65s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 430/500, 6.6 task/s, elapsed: 65s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 431/500, 6.6 task/s, elapsed: 65s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 432/500, 6.6 task/s, elapsed: 66s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 433/500, 6.6 task/s, elapsed: 66s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 434/500, 6.6 task/s, elapsed: 66s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 435/500, 6.6 task/s, elapsed: 66s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 436/500, 6.6 task/s, elapsed: 66s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 437/500, 6.6 task/s, elapsed: 66s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 438/500, 6.6 task/s, elapsed: 66s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 439/500, 6.6 task/s, elapsed: 67s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 440/500, 6.6 task/s, elapsed: 67s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 441/500, 6.6 task/s, elapsed: 67s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 442/500, 6.6 task/s, elapsed: 67s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 443/500, 6.6 task/s, elapsed: 67s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 444/500, 6.6 task/s, elapsed: 67s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 445/500, 6.6 task/s, elapsed: 67s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 446/500, 6.6 task/s, elapsed: 68s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 447/500, 6.6 task/s, elapsed: 68s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 448/500, 6.6 task/s, elapsed: 68s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 449/500, 6.6 task/s, elapsed: 68s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 450/500, 6.6 task/s, elapsed: 68s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 451/500, 6.6 task/s, elapsed: 68s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 452/500, 6.6 task/s, elapsed: 68s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 453/500, 6.6 task/s, elapsed: 69s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 454/500, 6.6 task/s, elapsed: 69s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 455/500, 6.6 task/s, elapsed: 69s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 456/500, 6.6 task/s, elapsed: 69s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 457/500, 6.6 task/s, elapsed: 69s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 458/500, 6.6 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 459/500, 6.6 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 460/500, 6.6 task/s, elapsed: 70s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 461/500, 6.6 task/s, elapsed: 70s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 462/500, 6.6 task/s, elapsed: 70s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 463/500, 6.6 task/s, elapsed: 70s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 464/500, 6.6 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 465/500, 6.6 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 466/500, 6.6 task/s, elapsed: 71s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 467/500, 6.6 task/s, elapsed: 71s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 468/500, 6.6 task/s, elapsed: 71s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 469/500, 6.6 task/s, elapsed: 71s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 470/500, 6.6 task/s, elapsed: 71s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 471/500, 6.6 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 472/500, 6.6 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 473/500, 6.6 task/s, elapsed: 72s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 474/500, 6.6 task/s, elapsed: 72s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 475/500, 6.6 task/s, elapsed: 72s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 476/500, 6.6 task/s, elapsed: 72s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 477/500, 6.6 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 478/500, 6.6 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 479/500, 6.6 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 480/500, 6.6 task/s, elapsed: 73s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 481/500, 6.6 task/s, elapsed: 73s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 482/500, 6.6 task/s, elapsed: 73s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 483/500, 6.6 task/s, elapsed: 73s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 484/500, 6.6 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 485/500, 6.6 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 486/500, 6.6 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 487/500, 6.6 task/s, elapsed: 74s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 488/500, 6.6 task/s, elapsed: 74s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 489/500, 6.6 task/s, elapsed: 74s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 490/500, 6.6 task/s, elapsed: 74s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 491/500, 6.6 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 492/500, 6.6 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 493/500, 6.6 task/s, elapsed: 75s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 494/500, 6.6 task/s, elapsed: 75s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 495/500, 6.6 task/s, elapsed: 75s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 496/500, 6.6 task/s, elapsed: 75s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 497/500, 6.6 task/s, elapsed: 75s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 498/500, 6.6 task/s, elapsed: 75s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 499/500, 6.6 task/s, elapsed: 75s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 500/500, 6.6 task/s, elapsed: 76s, ETA:     0s2022-04-19 03:02:34,828 - mmseg - INFO - per class results:
2022-04-19 03:02:34,830 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|      road     | 79.37 | 82.76 |
|    sidewalk   | 33.67 | 74.87 |
|    building   | 87.98 | 95.02 |
|      wall     | 38.68 | 45.13 |
|     fence     |  9.1  | 10.31 |
|      pole     | 49.97 | 58.79 |
| traffic light | 55.67 | 66.91 |
|  traffic sign |  55.4 | 65.44 |
|   vegetation  | 85.32 | 93.84 |
|    terrain    |  0.0  |  0.0  |
|      sky      | 83.55 | 99.13 |
|     person    | 72.55 | 87.43 |
|     rider     | 45.73 | 69.05 |
|      car      | 86.94 | 94.85 |
|     truck     |  0.0  |  0.0  |
|      bus      | 62.43 | 84.26 |
|     train     |  0.0  |  0.0  |
|   motorcycle  | 56.34 | 67.29 |
|    bicycle    | 60.86 | 68.99 |
+---------------+-------+-------+
2022-04-19 03:02:34,830 - mmseg - INFO - Summary:
2022-04-19 03:02:34,830 - mmseg - INFO - 
+------+-------+-------+
| aAcc |  mIoU |  mAcc |
+------+-------+-------+
| 85.8 | 50.71 | 61.27 |
+------+-------+-------+
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:02:34,847 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-19 03:02:34,847 - mmseg - INFO - Iter [500/40000]	lr: 1.200e-05, eta: 3:25:45, time: 1.453, data_time: 0.014, memory: 9636, aAcc: 0.8580, mIoU: 0.5071, mAcc: 0.6127, IoU.road: 0.7937, IoU.sidewalk: 0.3367, IoU.building: 0.8798, IoU.wall: 0.3868, IoU.fence: 0.0910, IoU.pole: 0.4997, IoU.traffic light: 0.5567, IoU.traffic sign: 0.5540, IoU.vegetation: 0.8532, IoU.terrain: 0.0000, IoU.sky: 0.8355, IoU.person: 0.7255, IoU.rider: 0.4573, IoU.car: 0.8694, IoU.truck: 0.0000, IoU.bus: 0.6243, IoU.train: 0.0000, IoU.motorcycle: 0.5634, IoU.bicycle: 0.6086, Acc.road: 0.8276, Acc.sidewalk: 0.7487, Acc.building: 0.9502, Acc.wall: 0.4513, Acc.fence: 0.1031, Acc.pole: 0.5879, Acc.traffic light: 0.6691, Acc.traffic sign: 0.6544, Acc.vegetation: 0.9384, Acc.terrain: 0.0000, Acc.sky: 0.9913, Acc.person: 0.8743, Acc.rider: 0.6905, Acc.car: 0.9485, Acc.truck: 0.0000, Acc.bus: 0.8426, Acc.train: 0.0000, Acc.motorcycle: 0.6729, Acc.bicycle: 0.6899, decode.loss_seg: 0.1034, decode.acc_seg: 91.7565, src.loss_imnet_feat_dist: 0.0983, mix.decode.loss_seg: 0.1149, mix.decode.acc_seg: 88.7489
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:03:52,874 - mmseg - INFO - Iter [32050/40000]	lr: 1.193e-05, eta: 3:24:58, time: 3.964, data_time: 2.418, memory: 9636, decode.loss_seg: 0.1015, decode.acc_seg: 91.4397, src.loss_imnet_feat_dist: 0.0987, mix.decode.loss_seg: 0.1274, mix.decode.acc_seg: 89.1517
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:05:08,119 - mmseg - INFO - Iter [32100/40000]	lr: 1.185e-05, eta: 3:23:40, time: 1.505, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1074, decode.acc_seg: 90.6370, src.loss_imnet_feat_dist: 0.0981, mix.decode.loss_seg: 0.1187, mix.decode.acc_seg: 88.9045
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:06:25,936 - mmseg - INFO - Iter [32150/40000]	lr: 1.178e-05, eta: 3:22:23, time: 1.556, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1008, decode.acc_seg: 91.0325, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1266, mix.decode.acc_seg: 88.6650
 /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:07:40,972 - mmseg - INFO - Iter [32200/40000]	lr: 1.170e-05, eta: 3:21:05, time: 1.501, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1014, decode.acc_seg: 91.4531, src.loss_imnet_feat_dist: 0.1007, mix.decode.loss_seg: 0.1324, mix.decode.acc_seg: 89.3851
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:08:56,883 - mmseg - INFO - Iter [32250/40000]	lr: 1.163e-05, eta: 3:19:47, time: 1.518, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1023, decode.acc_seg: 91.2056, src.loss_imnet_feat_dist: 0.1009, mix.decode.loss_seg: 0.1135, mix.decode.acc_seg: 89.2362
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:09:05,429 - mmseg - INFO - Iter [14650/40000]	lr: 3.803e-05, eta: 1 day, 0:00:16, time: 3.370, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.5934, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.8982
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:11:52,499 - mmseg - INFO - Iter [14700/40000]	lr: 3.795e-05, eta: 23:57:20, time: 3.341, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.7903, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.2966
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:14:42,018 - mmseg - INFO - Iter [14750/40000]	lr: 3.788e-05, eta: 23:54:28, time: 3.390, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.6402, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.1224
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:15:20,695 - mmseg - INFO - Iter [32500/40000]	lr: 1.125e-05, eta: 3:13:20, time: 1.566, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1087, decode.acc_seg: 90.5340, src.loss_imnet_feat_dist: 0.0923, mix.decode.loss_seg: 0.1132, mix.decode.acc_seg: 89.4675
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:16:35,938 - mmseg - INFO - Iter [32550/40000]	lr: 1.118e-05, eta: 3:12:02, time: 1.505, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1002, decode.acc_seg: 92.1724, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1113, mix.decode.acc_seg: 90.9059
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:17:52,725 - mmseg - INFO - Iter [32600/40000]	lr: 1.110e-05, eta: 3:10:45, time: 1.536, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1054, decode.acc_seg: 90.7394, src.loss_imnet_feat_dist: 0.0956, mix.decode.loss_seg: 0.1241, mix.decode.acc_seg: 88.9822
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:19:05,171 - mmseg - INFO - Iter [32650/40000]	lr: 1.103e-05, eta: 3:09:26, time: 1.449, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1104, decode.acc_seg: 91.8920, src.loss_imnet_feat_dist: 0.0938, mix.decode.loss_seg: 0.1197, mix.decode.acc_seg: 89.3549
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:20:20,306 - mmseg - INFO - Iter [32700/40000]	lr: 1.095e-05, eta: 3:08:08, time: 1.503, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1038, decode.acc_seg: 90.7369, src.loss_imnet_feat_dist: 0.0965, mix.decode.loss_seg: 0.1188, mix.decode.acc_seg: 89.8578
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:21:35,649 - mmseg - INFO - Iter [32750/40000]	lr: 1.088e-05, eta: 3:06:51, time: 1.507, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1042, decode.acc_seg: 90.2633, src.loss_imnet_feat_dist: 0.0936, mix.decode.loss_seg: 0.1214, mix.decode.acc_seg: 88.9361
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:22:52,411 - mmseg - INFO - Iter [32800/40000]	lr: 1.080e-05, eta: 3:05:33, time: 1.535, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0997, decode.acc_seg: 90.8313, src.loss_imnet_feat_dist: 0.0978, mix.decode.loss_seg: 0.1166, mix.decode.acc_seg: 89.2148
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:24:08,456 - mmseg - INFO - Iter [32850/40000]	lr: 1.073e-05, eta: 3:04:16, time: 1.521, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0992, decode.acc_seg: 91.5926, src.loss_imnet_feat_dist: 0.1001, mix.decode.loss_seg: 0.1150, mix.decode.acc_seg: 89.1231
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:25:24,996 - mmseg - INFO - Iter [32900/40000]	lr: 1.065e-05, eta: 3:02:58, time: 1.531, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1059, decode.acc_seg: 91.6337, src.loss_imnet_feat_dist: 0.0942, mix.decode.loss_seg: 0.1185, mix.decode.acc_seg: 90.3916
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:26:42,469 - mmseg - INFO - Iter [32950/40000]	lr: 1.058e-05, eta: 3:01:41, time: 1.549, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1033, decode.acc_seg: 91.5209, src.loss_imnet_feat_dist: 0.0958, mix.decode.loss_seg: 0.1171, mix.decode.acc_seg: 90.0207
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:27:58,281 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-19 03:27:58,281 - mmseg - INFO - Iter [33000/40000]	lr: 1.050e-05, eta: 3:00:23, time: 1.516, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0947, decode.acc_seg: 91.8460, src.loss_imnet_feat_dist: 0.0975, mix.decode.loss_seg: 0.1142, mix.decode.acc_seg: 90.4778
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:29:16,263 - mmseg - INFO - Iter [33050/40000]	lr: 1.043e-05, eta: 2:59:06, time: 1.560, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0981, decode.acc_seg: 92.3513, src.loss_imnet_feat_dist: 0.0957, mix.decode.loss_seg: 0.1136, mix.decode.acc_seg: 90.0831
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:30:33,753 - mmseg - INFO - Iter [33100/40000]	lr: 1.035e-05, eta: 2:57:49, time: 1.550, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1074, decode.acc_seg: 91.1789, src.loss_imnet_feat_dist: 0.0944, mix.decode.loss_seg: 0.1232, mix.decode.acc_seg: 89.6210
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:31:51,453 - mmseg - INFO - Iter [33150/40000]	lr: 1.028e-05, eta: 2:56:31, time: 1.554, data_time: 0.014, memory: 9636, decode.loss_seg: 0.0986, decode.acc_seg: 91.3757, src.loss_imnet_feat_dist: 0.0959, mix.decode.loss_seg: 0.1021, mix.decode.acc_seg: 89.7053
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:33:08,229 - mmseg - INFO - Iter [33200/40000]	lr: 1.020e-05, eta: 2:55:14, time: 1.536, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1091, decode.acc_seg: 91.1168, src.loss_imnet_feat_dist: 0.0955, mix.decode.loss_seg: 0.1308, mix.decode.acc_seg: 89.1449
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:34:23,289 - mmseg - INFO - Iter [33250/40000]	lr: 1.013e-05, eta: 2:53:56, time: 1.501, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1029, decode.acc_seg: 91.3459, src.loss_imnet_feat_dist: 0.0940, mix.decode.loss_seg: 0.1115, mix.decode.acc_seg: 90.0509
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:35:38,425 - mmseg - INFO - Iter [33300/40000]	lr: 1.005e-05, eta: 2:52:39, time: 1.503, data_time: 0.014, memory: 9636, decode.loss_seg: 0.0968, decode.acc_seg: 91.6991, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1568, mix.decode.acc_seg: 88.8947
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:36:54,698 - mmseg - INFO - Iter [33350/40000]	lr: 9.976e-06, eta: 2:51:21, time: 1.525, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1003, decode.acc_seg: 91.7331, src.loss_imnet_feat_dist: 0.1009, mix.decode.loss_seg: 0.1085, mix.decode.acc_seg: 89.7039
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:38:09,512 - mmseg - INFO - Iter [33400/40000]	lr: 9.901e-06, eta: 2:50:03, time: 1.496, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1013, decode.acc_seg: 91.5636, src.loss_imnet_feat_dist: 0.0959, mix.decode.loss_seg: 0.1123, mix.decode.acc_seg: 89.7233
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:39:24,876 - mmseg - INFO - Iter [33450/40000]	lr: 9.826e-06, eta: 2:48:46, time: 1.507, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1061, decode.acc_seg: 91.3420, src.loss_imnet_feat_dist: 0.1000, mix.decode.loss_seg: 0.1243, mix.decode.acc_seg: 88.9941
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:40:34,917 - mmseg - INFO - Iter [33500/40000]	lr: 9.752e-06, eta: 2:47:27, time: 1.401, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1013, decode.acc_seg: 91.4034, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1172, mix.decode.acc_seg: 89.9356
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:41:49,275 - mmseg - INFO - Iter [33550/40000]	lr: 9.676e-06, eta: 2:46:09, time: 1.487, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1082, decode.acc_seg: 91.5478, src.loss_imnet_feat_dist: 0.0934, mix.decode.loss_seg: 0.1249, mix.decode.acc_seg: 89.5611
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:42:59,633 - mmseg - INFO - Iter [33600/40000]	lr: 9.601e-06, eta: 2:44:50, time: 1.407, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1047, decode.acc_seg: 91.2809, src.loss_imnet_feat_dist: 0.1012, mix.decode.loss_seg: 0.1169, mix.decode.acc_seg: 89.1648
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:44:14,949 - mmseg - INFO - Iter [33650/40000]	lr: 9.527e-06, eta: 2:43:33, time: 1.506, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0999, decode.acc_seg: 91.3046, src.loss_imnet_feat_dist: 0.0988, mix.decode.loss_seg: 0.1179, mix.decode.acc_seg: 89.2918
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:45:26,355 - mmseg - INFO - Iter [33700/40000]	lr: 9.452e-06, eta: 2:42:14, time: 1.428, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1155, decode.acc_seg: 90.8600, src.loss_imnet_feat_dist: 0.0969, mix.decode.loss_seg: 0.1242, mix.decode.acc_seg: 89.1673
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:46:39,254 - mmseg - INFO - Iter [33750/40000]	lr: 9.377e-06, eta: 2:40:56, time: 1.458, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1006, decode.acc_seg: 92.1811, src.loss_imnet_feat_dist: 0.0904, mix.decode.loss_seg: 0.1291, mix.decode.acc_seg: 89.8429
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:47:52,605 - mmseg - INFO - Iter [33800/40000]	lr: 9.301e-06, eta: 2:39:38, time: 1.467, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1036, decode.acc_seg: 92.0538, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1181, mix.decode.acc_seg: 89.5703
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:49:12,757 - mmseg - INFO - Iter [33850/40000]	lr: 9.227e-06, eta: 2:38:22, time: 1.603, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1024, decode.acc_seg: 90.2707, src.loss_imnet_feat_dist: 0.0925, mix.decode.loss_seg: 0.1181, mix.decode.acc_seg: 87.9777
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:50:30,578 - mmseg - INFO - Iter [33900/40000]	lr: 9.152e-06, eta: 2:37:04, time: 1.556, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1011, decode.acc_seg: 91.0048, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1073, mix.decode.acc_seg: 89.3226
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:51:49,659 - mmseg - INFO - Iter [33950/40000]	lr: 9.077e-06, eta: 2:35:48, time: 1.582, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1015, decode.acc_seg: 90.6856, src.loss_imnet_feat_dist: 0.0978, mix.decode.loss_seg: 0.1149, mix.decode.acc_seg: 89.1966
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:53:07,203 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-19 03:53:07,203 - mmseg - INFO - Iter [34000/40000]	lr: 9.001e-06, eta: 2:34:30, time: 1.551, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1078, decode.acc_seg: 91.2683, src.loss_imnet_feat_dist: 0.0940, mix.decode.loss_seg: 0.1281, mix.decode.acc_seg: 89.5484
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:54:23,103 - mmseg - INFO - Iter [34050/40000]	lr: 8.926e-06, eta: 2:33:13, time: 1.518, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1066, decode.acc_seg: 91.3017, src.loss_imnet_feat_dist: 0.0978, mix.decode.loss_seg: 0.1286, mix.decode.acc_seg: 89.4449
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:55:38,206 - mmseg - INFO - Iter [34100/40000]	lr: 8.852e-06, eta: 2:31:55, time: 1.502, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0948, decode.acc_seg: 90.8655, src.loss_imnet_feat_dist: 0.0958, mix.decode.loss_seg: 0.1220, mix.decode.acc_seg: 88.6312
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:56:57,780 - mmseg - INFO - Iter [34150/40000]	lr: 8.777e-06, eta: 2:30:38, time: 1.591, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0988, decode.acc_seg: 91.4013, src.loss_imnet_feat_dist: 0.0978, mix.decode.loss_seg: 0.1178, mix.decode.acc_seg: 89.2906
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:58:16,079 - mmseg - INFO - Iter [34200/40000]	lr: 8.701e-06, eta: 2:29:21, time: 1.566, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0933, decode.acc_seg: 91.0848, src.loss_imnet_feat_dist: 0.0955, mix.decode.loss_seg: 0.1126, mix.decode.acc_seg: 88.7086
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 03:59:35,735 - mmseg - INFO - Iter [34250/40000]	lr: 8.626e-06, eta: 2:28:04, time: 1.593, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0978, decode.acc_seg: 91.5002, src.loss_imnet_feat_dist: 0.0989, mix.decode.loss_seg: 0.1212, mix.decode.acc_seg: 89.1546
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:00:52,139 - mmseg - INFO - Iter [34300/40000]	lr: 8.552e-06, eta: 2:26:47, time: 1.528, data_time: 0.014, memory: 9636, decode.loss_seg: 0.0999, decode.acc_seg: 91.4345, src.loss_imnet_feat_dist: 0.0945, mix.decode.loss_seg: 0.1203, mix.decode.acc_seg: 89.3985
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:02:04,973 - mmseg - INFO - Iter [34350/40000]	lr: 8.477e-06, eta: 2:25:29, time: 1.457, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1062, decode.acc_seg: 91.8035, src.loss_imnet_feat_dist: 0.0952, mix.decode.loss_seg: 0.1135, mix.decode.acc_seg: 89.6035
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:03:21,014 - mmseg - INFO - Iter [34400/40000]	lr: 8.401e-06, eta: 2:24:12, time: 1.521, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1132, decode.acc_seg: 90.7512, src.loss_imnet_feat_dist: 0.0973, mix.decode.loss_seg: 0.1220, mix.decode.acc_seg: 89.9988
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:04:36,666 - mmseg - INFO - Iter [34450/40000]	lr: 8.326e-06, eta: 2:22:54, time: 1.513, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1090, decode.acc_seg: 91.0434, src.loss_imnet_feat_dist: 0.0970, mix.decode.loss_seg: 0.1213, mix.decode.acc_seg: 89.1424
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:05:48,346 - mmseg - INFO - Iter [34500/40000]	lr: 8.252e-06, eta: 2:21:36, time: 1.434, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1084, decode.acc_seg: 90.7020, src.loss_imnet_feat_dist: 0.0969, mix.decode.loss_seg: 0.1204, mix.decode.acc_seg: 88.8151
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:07:03,149 - mmseg - INFO - Iter [34550/40000]	lr: 8.177e-06, eta: 2:20:18, time: 1.496, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1012, decode.acc_seg: 91.7739, src.loss_imnet_feat_dist: 0.0976, mix.decode.loss_seg: 0.1203, mix.decode.acc_seg: 88.4367
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:08:17,749 - mmseg - INFO - Iter [34600/40000]	lr: 8.101e-06, eta: 2:19:01, time: 1.492, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1019, decode.acc_seg: 91.7575, src.loss_imnet_feat_dist: 0.0957, mix.decode.loss_seg: 0.1191, mix.decode.acc_seg: 89.2469
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:09:34,500 - mmseg - INFO - Iter [34650/40000]	lr: 8.026e-06, eta: 2:17:43, time: 1.535, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1013, decode.acc_seg: 91.3272, src.loss_imnet_feat_dist: 0.0950, mix.decode.loss_seg: 0.1240, mix.decode.acc_seg: 89.5827
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:10:48,792 - mmseg - INFO - Iter [34700/40000]	lr: 7.952e-06, eta: 2:16:26, time: 1.486, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1124, decode.acc_seg: 91.3429, src.loss_imnet_feat_dist: 0.0941, mix.decode.loss_seg: 0.1288, mix.decode.acc_seg: 89.3149
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:12:08,053 - mmseg - INFO - Iter [34750/40000]	lr: 7.877e-06, eta: 2:15:09, time: 1.585, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1000, decode.acc_seg: 91.3082, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1208, mix.decode.acc_seg: 89.5580
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:13:24,966 - mmseg - INFO - Iter [34800/40000]	lr: 7.801e-06, eta: 2:13:51, time: 1.538, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1085, decode.acc_seg: 90.7435, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1233, mix.decode.acc_seg: 88.5650
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:14:40,273 - mmseg - INFO - Iter [34850/40000]	lr: 7.726e-06, eta: 2:12:34, time: 1.506, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1022, decode.acc_seg: 91.3829, src.loss_imnet_feat_dist: 0.0928, mix.decode.loss_seg: 0.1204, mix.decode.acc_seg: 89.4420
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:15:55,943 - mmseg - INFO - Iter [34900/40000]	lr: 7.651e-06, eta: 2:11:17, time: 1.513, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1073, decode.acc_seg: 90.9859, src.loss_imnet_feat_dist: 0.0978, mix.decode.loss_seg: 0.1193, mix.decode.acc_seg: 89.8370
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:17:08,046 - mmseg - INFO - Iter [34950/40000]	lr: 7.577e-06, eta: 2:09:59, time: 1.442, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0980, decode.acc_seg: 91.4494, src.loss_imnet_feat_dist: 0.0997, mix.decode.loss_seg: 0.1253, mix.decode.acc_seg: 89.4247
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:18:23,053 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-19 04:18:23,054 - mmseg - INFO - Iter [35000/40000]	lr: 7.502e-06, eta: 2:08:41, time: 1.500, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1028, decode.acc_seg: 91.6430, src.loss_imnet_feat_dist: 0.0936, mix.decode.loss_seg: 0.1228, mix.decode.acc_seg: 89.1910
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:19:41,128 - mmseg - INFO - Iter [35050/40000]	lr: 7.426e-06, eta: 2:07:24, time: 1.561, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1060, decode.acc_seg: 91.2700, src.loss_imnet_feat_dist: 0.0934, mix.decode.loss_seg: 0.1120, mix.decode.acc_seg: 89.8846
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:20:59,215 - mmseg - INFO - Iter [35100/40000]	lr: 7.351e-06, eta: 2:06:07, time: 1.562, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1061, decode.acc_seg: 91.3156, src.loss_imnet_feat_dist: 0.0992, mix.decode.loss_seg: 0.1204, mix.decode.acc_seg: 89.7614
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:22:18,523 - mmseg - INFO - Iter [35150/40000]	lr: 7.277e-06, eta: 2:04:50, time: 1.586, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1043, decode.acc_seg: 90.7004, src.loss_imnet_feat_dist: 0.0974, mix.decode.loss_seg: 0.1194, mix.decode.acc_seg: 88.3951
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:23:35,028 - mmseg - INFO - Iter [35200/40000]	lr: 7.202e-06, eta: 2:03:33, time: 1.530, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1044, decode.acc_seg: 91.4403, src.loss_imnet_feat_dist: 0.0965, mix.decode.loss_seg: 0.1119, mix.decode.acc_seg: 89.4638
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:24:52,738 - mmseg - INFO - Iter [35250/40000]	lr: 7.126e-06, eta: 2:02:15, time: 1.554, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1110, decode.acc_seg: 91.0386, src.loss_imnet_feat_dist: 0.0984, mix.decode.loss_seg: 0.1193, mix.decode.acc_seg: 89.5370
k/s, elapsed: 3s, ETA:   225s
[                                 ] 8/500, 2.2 task/s, elapsed: 4s, ETA:   220s
[                                 ] 9/500, 2.3 task/s, elapsed: 4s, ETA:   216s
[                                ] 10/500, 2.3 task/s, elapsed: 4s, ETA:   213s
[                                ] 11/500, 2.3 task/s, elapsed: 5s, ETA:   211s
[                                ] 12/500, 2.3 task/s, elapsed: 5s, ETA:   209s
[                                ] 13/500, 2.4 task/s, elapsed: 6s, ETA:   207s
[                                ] 14/500, 2.4 task/s, elapsed: 6s, ETA:   205s
[                                ] 15/500, 2.4 task/s, elapsed: 6s, ETA:   203s
[>                               ] 16/500, 2.4 task/s, elapsed: 7s, ETA:   202s
[>                               ] 17/500, 2.4 task/s, elapsed: 7s, ETA:   201s
[>                               ] 18/500, 2.4 task/s, elapsed: 7s, ETA:   199s
[>                               ] 19/500, 2.4 task/s, elapsed: 8s, ETA:   198s
[>                               ] 20/500, 2.4 task/s, elapsed: 8s, ETA:   197s
[>                               ] 21/500, 2.4 task/s, elapsed: 9s, ETA:   196s
[>                               ] 22/500, 2.4 task/s, elapsed: 9s, ETA:   195s
[>                               ] 23/500, 2.5 task/s, elapsed: 9s, ETA:   195s
[>                              ] 24/500, 2.5 task/s, elapsed: 10s, ETA:   194s
[>                              ] 25/500, 2.5 task/s, elapsed: 10s, ETA:   193s
[>                              ] 26/500, 2.5 task/s, elapsed: 11s, ETA:   192s
[>                              ] 27/500, 2.5 task/s, elapsed: 11s, ETA:   192s
[>                              ] 28/500, 2.5 task/s, elapsed: 11s, ETA:   191s
[>                              ] 29/500, 2.5 task/s, elapsed: 12s, ETA:   190s
[>                              ] 30/500, 2.5 task/s, elapsed: 12s, ETA:   189s
[>                              ] 31/500, 2.5 task/s, elapsed: 12s, ETA:   189s
[>                              ] 32/500, 2.5 task/s, elapsed: 13s, ETA:   188s
[>>                             ] 33/500, 2.5 task/s, elapsed: 13s, ETA:   188s
[>>                             ] 34/500, 2.5 task/s, elapsed: 14s, ETA:   187s
[>>                             ] 35/500, 2.5 task/s, elapsed: 14s, ETA:   186s
[>>                             ] 36/500, 2.5 task/s, elapsed: 14s, ETA:   186s
[>>                             ] 37/500, 2.5 task/s, elapsed: 15s, ETA:   185s
[>>                             ] 38/500, 2.5 task/s, elapsed: 15s, ETA:   185s
[>>                             ] 39/500, 2.5 task/s, elapsed: 16s, ETA:   184s
[>>                             ] 40/500, 2.5 task/s, elapsed: 16s, ETA:   184s
[>>                             ] 41/500, 2.5 task/s, elapsed: 16s, ETA:   183s
[>>                             ] 42/500, 2.5 task/s, elapsed: 17s, ETA:   183s
[>>                             ] 43/500, 2.5 task/s, elapsed: 17s, ETA:   182s
[>>                             ] 44/500, 2.5 task/s, elapsed: 18s, ETA:   182s
[>>                             ] 45/500, 2.5 task/s, elapsed: 18s, ETA:   181s
[>>                             ] 46/500, 2.5 task/s, elapsed: 18s, ETA:   181s
[>>                             ] 47/500, 2.5 task/s, elapsed: 19s, ETA:   180s
[>>                             ] 48/500, 2.5 task/s, elapsed: 19s, ETA:   180s
[>>>                            ] 49/500, 2.5 task/s, elapsed: 19s, ETA:   179s
[>>>                            ] 50/500, 2.5 task/s, elapsed: 20s, ETA:   179s
[>>>                            ] 51/500, 2.5 task/s, elapsed: 20s, ETA:   178s
[>>>                            ] 52/500, 2.5 task/s, elapsed: 21s, ETA:   178s
[>>>                            ] 53/500, 2.5 task/s, elapsed: 21s, ETA:   177s
[>>>                            ] 54/500, 2.5 task/s, elapsed: 21s, ETA:   177s
[>>>                            ] 55/500, 2.5 task/s, elapsed: 22s, ETA:   176s
[>>>                            ] 56/500, 2.5 task/s, elapsed: 22s, ETA:   176s
[>>>                            ] 57/500, 2.5 task/s, elapsed: 23s, ETA:   175s
[>>>                            ] 58/500, 2.5 task/s, elapsed: 23s, ETA:   175s
[>>>                            ] 59/500, 2.5 task/s, elapsed: 23s, ETA:   174s
[>>>                            ] 60/500, 2.5 task/s, elapsed: 24s, ETA:   174s
[>>>                            ] 61/500, 2.5 task/s, elapsed: 24s, ETA:   174s
[>>>                            ] 62/500, 2.5 task/s, elapsed: 25s, ETA:   173s
[>>>                            ] 63/500, 2.5 task/s, elapsed: 25s, ETA:   173s
[>>>                            ] 64/500, 2.5 task/s, elapsed: 25s, ETA:   172s
[>>>>                           ] 65/500, 2.5 task/s, elapsed: 26s, ETA:   172s
[>>>>                           ] 66/500, 2.5 task/s, elapsed: 26s, ETA:   171s
[>>>>                           ] 67/500, 2.5 task/s, elapsed: 26s, ETA:   171s
[>>>>                           ] 68/500, 2.5 task/s, elapsed: 27s, ETA:   170s
[>>>>                           ] 69/500, 2.5 task/s, elapsed: 27s, ETA:   170s
[>>>>                           ] 70/500, 2.5 task/s, elapsed: 28s, ETA:   170s
[>>>>                           ] 71/500, 2.5 task/s, elapsed: 28s, ETA:   169s
[>>>>                           ] 72/500, 2.5 task/s, elapsed: 28s, ETA:   169s
[>>>>                           ] 73/500, 2.5 task/s, elapsed: 29s, ETA:   168s
[>>>>                           ] 74/500, 2.5 task/s, elapsed: 29s, ETA:   168s
[>>>>                           ] 75/500, 2.5 task/s, elapsed: 30s, ETA:   167s
[>>>>                           ] 76/500, 2.5 task/s, elapsed: 30s, ETA:   167s
[>>>>                           ] 77/500, 2.5 task/s, elapsed: 30s, ETA:   167s
[>>>>                           ] 78/500, 2.5 task/s, elapsed: 31s, ETA:   166s
[>>>>                           ] 79/500, 2.5 task/s, elapsed: 31s, ETA:   166s
[>>>>                           ] 80/500, 2.5 task/s, elapsed: 31s, ETA:   165s
[>>>>>                          ] 81/500, 2.5 task/s, elapsed: 32s, ETA:   165s
[>>>>>                          ] 82/500, 2.5 task/s, elapsed: 32s, ETA:   164s
[>>>>>                          ] 83/500, 2.5 task/s, elapsed: 33s, ETA:   164s
[>>>>>                          ] 84/500, 2.5 task/s, elapsed: 33s, ETA:   164s
[>>>>>                          ] 85/500, 2.5 task/s, elapsed: 33s, ETA:   163s
[>>>>>                          ] 86/500, 2.5 task/s, elapsed: 34s, ETA:   163s
[>>>>>                          ] 87/500, 2.5 task/s, elapsed: 34s, ETA:   162s
[>>>>>                          ] 88/500, 2.5 task/s, elapsed: 35s, ETA:   162s
[>>>>>                          ] 89/500, 2.5 task/s, elapsed: 35s, ETA:   161s
[>>>>>                          ] 90/500, 2.5 task/s, elapsed: 35s, ETA:   161s
[>>>>>                          ] 91/500, 2.5 task/s, elapsed: 36s, ETA:   161s
[>>>>>                          ] 92/500, 2.5 task/s, elapsed: 36s, ETA:   160s
[>>>>>                          ] 93/500, 2.5 task/s, elapsed: 37s, ETA:   160s
[>>>>>                          ] 94/500, 2.5 task/s, elapsed: 37s, ETA:   159s
[>>>>>                          ] 95/500, 2.5 task/s, elapsed: 37s, ETA:   159s
[>>>>>                          ] 96/500, 2.5 task/s, elapsed: 38s, ETA:   159s
[>>>>>>                         ] 97/500, 2.5 task/s, elapsed: 38s, ETA:   158s
[>>>>>>                         ] 98/500, 2.5 task/s, elapsed: 38s, ETA:   158s
[>>>>>>                         ] 99/500, 2.5 task/s, elapsed: 39s, ETA:   157s
[>>>>>>                        ] 100/500, 2.5 task/s, elapsed: 39s, ETA:   157s
[>>>>>>                        ] 101/500, 2.5 task/s, elapsed: 40s, ETA:   156s
[>>>>>>                        ] 102/500, 2.6 task/s, elapsed: 40s, ETA:   156s
[>>>>>>                        ] 103/500, 2.6 task/s, elapsed: 40s, ETA:   156s
[>>>>>>                        ] 104/500, 2.6 task/s, elapsed: 41s, ETA:   155s
[>>>>>>                        ] 105/500, 2.6 task/s, elapsed: 41s, ETA:   155s
[>>>>>>                        ] 106/500, 2.6 task/s, elapsed: 42s, ETA:   154s
[>>>>>>                        ] 107/500, 2.6 task/s, elapsed: 42s, ETA:   154s
[>>>>>>                        ] 108/500, 2.6 task/s, elapsed: 42s, ETA:   154s
[>>>>>>                        ] 109/500, 2.6 task/s, elapsed: 43s, ETA:   153s
[>>>>>>                        ] 110/500, 2.6 task/s, elapsed: 43s, ETA:   153s
[>>>>>>                        ] 111/500, 2.6 task/s, elapsed: 43s, ETA:   152s
[>>>>>>                        ] 112/500, 2.6 task/s, elapsed: 44s, ETA:   152s
[>>>>>>                        ] 113/500, 2.6 task/s, elapsed: 44s, ETA:   152s
[>>>>>>                        ] 114/500, 2.6 task/s, elapsed: 45s, ETA:   151s
[>>>>>>                        ] 115/500, 2.6 task/s, elapsed: 45s, ETA:   151s
[>>>>>>                        ] 116/500, 2.6 task/s, elapsed: 45s, ETA:   150s
[>>>>>>>                       ] 117/500, 2.6 task/s, elapsed: 46s, ETA:   150s
[>>>>>>>                       ] 118/500, 2.6 task/s, elapsed: 46s, ETA:   150s
[>>>>>>>                       ] 119/500, 2.6 task/s, elapsed: 47s, ETA:   149s
[>>>>>>>                       ] 120/500, 2.6 task/s, elapsed: 47s, ETA:   149s
[>>>>>>>                       ] 121/500, 2.6 task/s, elapsed: 47s, ETA:   148s
[>>>>>>>                       ] 122/500, 2.6 task/s, elapsed: 48s, ETA:   148s
[>>>>>>>                       ] 123/500, 2.6 task/s, elapsed: 48s, ETA:   148s
[>>>>>>>                       ] 124/500, 2.6 task/s, elapsed: 49s, ETA:   147s
[>>>>>>>                       ] 125/500, 2.6 task/s, elapsed: 49s, ETA:   147s
[>>>>>>>                       ] 126/500, 2.6 task/s, elapsed: 49s, ETA:   146s
[>>>>>>>                       ] 127/500, 2.6 task/s, elapsed: 50s, ETA:   146s
[>>>>>>>                       ] 128/500, 2.6 task/s, elapsed: 50s, ETA:   146s
[>>>>>>>                       ] 129/500, 2.6 task/s, elapsed: 50s, ETA:   145s
[>>>>>>>                       ] 130/500, 2.6 task/s, elapsed: 51s, ETA:   145s
[>>>>>>>                       ] 131/500, 2.6 task/s, elapsed: 51s, ETA:   144s
[>>>>>>>                       ] 132/500, 2.6 task/s, elapsed: 52s, ETA:   144s
[>>>>>>>                       ] 133/500, 2.6 task/s, elapsed: 52s, ETA:   144s
[>>>>>>>>                      ] 134/500, 2.6 task/s, elapsed: 52s, ETA:   143s
[>>>>>>>>                      ] 135/500, 2.6 task/s, elapsed: 53s, ETA:   143s
[>>>>>>>>                      ] 136/500, 2.6 task/s, elapsed: 53s, ETA:   142s
[>>>>>>>>                      ] 137/500, 2.6 task/s, elapsed: 54s, ETA:   142s
[>>>>>>>>                      ] 138/500, 2.6 task/s, elapsed: 54s, ETA:   142s
[>>>>>>>>                      ] 139/500, 2.6 task/s, elapsed: 54s, ETA:   141s
[>>>>>>>>                      ] 140/500, 2.6 task/s, elapsed: 55s, ETA:   141s
[>>>>>>>>                      ] 141/500, 2.6 task/s, elapsed: 55s, ETA:   140s
[>>>>>>>>                      ] 142/500, 2.6 task/s, elapsed: 56s, ETA:   140s
[>>>>>>>>                      ] 143/500, 2.6 task/s, elapsed: 56s, ETA:   140s
[>>>>>>>>                      ] 144/500, 2.6 task/s, elapsed: 56s, ETA:   139s
[>>>>>>>>                      ] 145/500, 2.6 task/s, elapsed: 57s, ETA:   139s
[>>>>>>>>                      ] 146/500, 2.6 task/s, elapsed: 57s, ETA:   138s
[>>>>>>>>                      ] 147/500, 2.6 task/s, elapsed: 57s, ETA:   138s
[>>>>>>>>                      ] 148/500, 2.6 task/s, elapsed: 58s, ETA:   138s
[>>>>>>>>                      ] 149/500, 2.6 task/s, elapsed: 58s, ETA:   137s
[>>>>>>>>>                     ] 150/500, 2.6 task/s, elapsed: 59s, ETA:   137s
[>>>>>>>>>                     ] 151/500, 2.6 task/s, elapsed: 59s, ETA:   136s
[>>>>>>>>>                     ] 152/500, 2.6 task/s, elapsed: 59s, ETA:   136s
[>>>>>>>>>                     ] 153/500, 2.6 task/s, elapsed: 60s, ETA:   136s
[>>>>>>>>>                     ] 154/500, 2.6 task/s, elapsed: 60s, ETA:   135s
[>>>>>>>>>                     ] 155/500, 2.6 task/s, elapsed: 61s, ETA:   135s
[>>>>>>>>>                     ] 156/500, 2.6 task/s, elapsed: 61s, ETA:   134s
[>>>>>>>>>                     ] 157/500, 2.6 task/s, elapsed: 61s, ETA:   134s
[>>>>>>>>>                     ] 158/500, 2.6 task/s, elapsed: 62s, ETA:   134s
[>>>>>>>>>                     ] 159/500, 2.6 task/s, elapsed: 62s, ETA:   133s
[>>>>>>>>>                     ] 160/500, 2.6 task/s, elapsed: 62s, ETA:   133s
[>>>>>>>>>                     ] 161/500, 2.6 task/s, elapsed: 63s, ETA:   132s
[>>>>>>>>>                     ] 162/500, 2.6 task/s, elapsed: 63s, ETA:   132s
[>>>>>>>>>                     ] 163/500, 2.6 task/s, elapsed: 64s, ETA:   132s
[>>>>>>>>>                     ] 164/500, 2.6 task/s, elapsed: 64s, ETA:   131s
[>>>>>>>>>                     ] 165/500, 2.6 task/s, elapsed: 64s, ETA:   131s
[>>>>>>>>>                     ] 166/500, 2.6 task/s, elapsed: 65s, ETA:   130s
[>>>>>>>>>>                    ] 167/500, 2.6 task/s, elapsed: 65s, ETA:   130s
[>>>>>>>>>>                    ] 168/500, 2.6 task/s, elapsed: 66s, ETA:   130s
[>>>>>>>>>>                    ] 169/500, 2.6 task/s, elapsed: 66s, ETA:   129s
[>>>>>>>>>>                    ] 170/500, 2.6 task/s, elapsed: 66s, ETA:   129s
[>>>>>>>>>>                    ] 171/500, 2.6 task/s, elapsed: 67s, ETA:   128s
[>>>>>>>>>>                    ] 172/500, 2.6 task/s, elapsed: 67s, ETA:   128s
[>>>>>>>>>>                    ] 173/500, 2.6 task/s, elapsed: 68s, ETA:   128s
[>>>>>>>>>>                    ] 174/500, 2.6 task/s, elapsed: 68s, ETA:   127s
[>>>>>>>>>>                    ] 175/500, 2.6 task/s, elapsed: 68s, ETA:   127s
[>>>>>>>>>>                    ] 176/500, 2.6 task/s, elapsed: 69s, ETA:   126s
[>>>>>>>>>>                    ] 177/500, 2.6 task/s, elapsed: 69s, ETA:   126s
[>>>>>>>>>>                    ] 178/500, 2.6 task/s, elapsed: 69s, ETA:   126s
[>>>>>>>>>>                    ] 179/500, 2.6 task/s, elapsed: 70s, ETA:   125s
[>>>>>>>>>>                    ] 180/500, 2.6 task/s, elapsed: 70s, ETA:   125s
[>>>>>>>>>>                    ] 181/500, 2.6 task/s, elapsed: 71s, ETA:   125s
[>>>>>>>>>>                    ] 182/500, 2.6 task/s, elapsed: 71s, ETA:   124s
[>>>>>>>>>>                    ] 183/500, 2.6 task/s, elapsed: 71s, ETA:   124s
[>>>>>>>>>>>                   ] 184/500, 2.6 task/s, elapsed: 72s, ETA:   123s
[>>>>>>>>>>>                   ] 185/500, 2.6 task/s, elapsed: 72s, ETA:   123s
[>>>>>>>>>>>                   ] 186/500, 2.6 task/s, elapsed: 73s, ETA:   123s
[>>>>>>>>>>>                   ] 187/500, 2.6 task/s, elapsed: 73s, ETA:   122s
[>>>>>>>>>>>                   ] 188/500, 2.6 task/s, elapsed: 73s, ETA:   122s
[>>>>>>>>>>>                   ] 189/500, 2.6 task/s, elapsed: 74s, ETA:   122s
[>>>>>>>>>>>                   ] 190/500, 2.6 task/s, elapsed: 74s, ETA:   121s
[>>>>>>>>>>>                   ] 191/500, 2.6 task/s, elapsed: 75s, ETA:   121s
[>>>>>>>>>>>                   ] 192/500, 2.6 task/s, elapsed: 75s, ETA:   120s
[>>>>>>>>>>>                   ] 193/500, 2.6 task/s, elapsed: 75s, ETA:   120s
[>>>>>>>>>>>                   ] 194/500, 2.6 task/s, elapsed: 76s, ETA:   120s
[>>>>>>>>>>>                   ] 195/500, 2.6 task/s, elapsed: 76s, ETA:   119s
[>>>>>>>>>>>                   ] 196/500, 2.6 task/s, elapsed: 77s, ETA:   119s/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is require  118s
[>>>>>>>>>>>                   ] 199/500, 2.6 task/s, elapsed: 78s, ETA:   118s
[>>>>>>>>>>>>                  ] 200/500, 2.6 task/s, elapsed: 78s, ETA:   117s
[>>>>>>>>>>>>                  ] 201/500, 2.6 task/s, elapsed: 79s, ETA:   117s
[>>>>>>>>>>>>                  ] 202/500, 2.6 task/s, elapsed: 79s, ETA:   117s
[>>>>>>>>>>>>                  ] 203/500, 2.6 task/s, elapsed: 79s, ETA:   116s
[>>>>>>>>>>>>                  ] 204/500, 2.6 task/s, elapsed: 80s, ETA:   116s
[>>>>>>>>>>>>                  ] 205/500, 2.6 task/s, elapsed: 80s, ETA:   116s
[>>>>>>>>>>>>                  ] 206/500, 2.6 task/s, elapsed: 81s, ETA:   115s
[>>>>>>>>>>>>                  ] 207/500, 2.6 task/s, elapsed: 81s, ETA:   115s
[>>>>>>>>>>>>                  ] 208/500, 2.6 task/s, elapsed: 81s, ETA:   114s
[>>>>>>>>>>>>                  ] 209/500, 2.6 task/s, elapsed: 82s, ETA:   114s
[>>>>>>>>>>>>                  ] 210/500, 2.6 task/s, elapsed: 82s, ETA:   114s
[>>>>>>>>>>>>                  ] 211/500, 2.6 task/s, elapsed: 83s, ETA:   113s
[>>>>>>>>>>>>                  ] 212/500, 2.6 task/s, elapsed: 83s, ETA:   113s
[>>>>>>>>>>>>                  ] 213/500, 2.6 task/s, elapsed: 83s, ETA:   112s
[>>>>>>>>>>>>                  ] 214/500, 2.6 task/s, elapsed: 84s, ETA:   112s
[>>>>>>>>>>>>                  ] 215/500, 2.6 task/s, elapsed: 84s, ETA:   112s
[>>>>>>>>>>>>                  ] 216/500, 2.6 task/s, elapsed: 85s, ETA:   111s
[>>>>>>>>>>>>>                 ] 217/500, 2.6 task/s, elapsed: 85s, ETA:   111s
[>>>>>>>>>>>>>                 ] 218/500, 2.6 task/s, elapsed: 85s, ETA:   110s
[>>>>>>>>>>>>>                 ] 219/500, 2.6 task/s, elapsed: 86s, ETA:   110s
[>>>>>>>>>>>>>                 ] 220/500, 2.6 task/s, elapsed: 86s, ETA:   110s
[>>>>>>>>>>>>>                 ] 221/500, 2.6 task/s, elapsed: 86s, ETA:   109s
[>>>>>>>>>>>>>                 ] 222/500, 2.6 task/s, elapsed: 87s, ETA:   109s
[>>>>>>>>>>>>>                 ] 223/500, 2.6 task/s, elapsed: 87s, ETA:   108s
[>>>>>>>>>>>>>                 ] 224/500, 2.6 task/s, elapsed: 88s, ETA:   108s
[>>>>>>>>>>>>>                 ] 225/500, 2.6 task/s, elapsed: 88s, ETA:   108s
[>>>>>>>>>>>>>                 ] 226/500, 2.6 task/s, elapsed: 88s, ETA:   107s
[>>>>>>>>>>>>>                 ] 227/500, 2.6 task/s, elapsed: 89s, ETA:   107s
[>>>>>>>>>>>>>                 ] 228/500, 2.6 task/s, elapsed: 89s, ETA:   106s
[>>>>>>>>>>>>>                 ] 229/500, 2.6 task/s, elapsed: 90s, ETA:   106s
[>>>>>>>>>>>>>                 ] 230/500, 2.6 task/s, elapsed: 90s, ETA:   106s
[>>>>>>>>>>>>>                 ] 231/500, 2.6 task/s, elapsed: 90s, ETA:   105s
[>>>>>>>>>>>>>                 ] 232/500, 2.6 task/s, elapsed: 91s, ETA:   105s
[>>>>>>>>>>>>>                 ] 233/500, 2.6 task/s, elapsed: 91s, ETA:   104s
[>>>>>>>>>>>>>>                ] 234/500, 2.6 task/s, elapsed: 92s, ETA:   104s
[>>>>>>>>>>>>>>                ] 235/500, 2.6 task/s, elapsed: 92s, ETA:   104s
[>>>>>>>>>>>>>>                ] 236/500, 2.6 task/s, elapsed: 92s, ETA:   103s
[>>>>>>>>>>>>>>                ] 237/500, 2.6 task/s, elapsed: 93s, ETA:   103s
[>>>>>>>>>>>>>>                ] 238/500, 2.6 task/s, elapsed: 93s, ETA:   102s
[>>>>>>>>>>>>>>                ] 239/500, 2.6 task/s, elapsed: 93s, ETA:   102s
[>>>>>>>>>>>>>>                ] 240/500, 2.6 task/s, elapsed: 94s, ETA:   102s
[>>>>>>>>>>>>>>                ] 241/500, 2.6 task/s, elapsed: 94s, ETA:   101s
[>>>>>>>>>>>>>>                ] 242/500, 2.6 task/s, elapsed: 95s, ETA:   101s
[>>>>>>>>>>>>>>                ] 243/500, 2.6 task/s, elapsed: 95s, ETA:   100s
[>>>>>>>>>>>>>>                ] 244/500, 2.6 task/s, elapsed: 95s, ETA:   100s
[>>>>>>>>>>>>>>                ] 245/500, 2.6 task/s, elapsed: 96s, ETA:   100s
[>>>>>>>>>>>>>>                ] 246/500, 2.6 task/s, elapsed: 96s, ETA:    99s
[>>>>>>>>>>>>>>                ] 247/500, 2.6 task/s, elapsed: 97s, ETA:    99s
[>>>>>>>>>>>>>>                ] 248/500, 2.6 task/s, elapsed: 97s, ETA:    99s
[>>>>>>>>>>>>>>                ] 249/500, 2.6 task/s, elapsed: 97s, ETA:    98s
[>>>>>>>>>>>>>>>               ] 250/500, 2.6 task/s, elapsed: 98s, ETA:    98s
[>>>>>>>>>>>>>>>               ] 251/500, 2.6 task/s, elapsed: 98s, ETA:    97s
[>>>>>>>>>>>>>>>               ] 252/500, 2.6 task/s, elapsed: 98s, ETA:    97s
[>>>>>>>>>>>>>>>               ] 253/500, 2.6 task/s, elapsed: 99s, ETA:    97s
[>>>>>>>>>>>>>>>               ] 254/500, 2.6 task/s, elapsed: 99s, ETA:    96s
[>>>>>>>>>>>>>>               ] 255/500, 2.6 task/s, elapsed: 100s, ETA:    96s
[>>>>>>>>>>>>>>               ] 256/500, 2.6 task/s, elapsed: 100s, ETA:    95s
[>>>>>>>>>>>>>>               ] 257/500, 2.6 task/s, elapsed: 100s, ETA:    95s
[>>>>>>>>>>>>>>               ] 258/500, 2.6 task/s, elapsed: 101s, ETA:    95s
[>>>>>>>>>>>>>>>              ] 259/500, 2.6 task/s, elapsed: 101s, ETA:    94s
[>>>>>>>>>>>>>>>              ] 260/500, 2.6 task/s, elapsed: 102s, ETA:    94s
[>>>>>>>>>>>>>>>              ] 261/500, 2.6 task/s, elapsed: 102s, ETA:    93s
[>>>>>>>>>>>>>>>              ] 262/500, 2.6 task/s, elapsed: 102s, ETA:    93s
[>>>>>>>>>>>>>>>              ] 263/500, 2.6 task/s, elapsed: 103s, ETA:    93s
[>>>>>>>>>>>>>>>              ] 264/500, 2.6 task/s, elapsed: 103s, ETA:    92s
[>>>>>>>>>>>>>>>              ] 265/500, 2.6 task/s, elapsed: 104s, ETA:    92s
[>>>>>>>>>>>>>>>              ] 266/500, 2.6 task/s, elapsed: 104s, ETA:    91s
[>>>>>>>>>>>>>>>              ] 267/500, 2.6 task/s, elapsed: 104s, ETA:    91s
[>>>>>>>>>>>>>>>              ] 268/500, 2.6 task/s, elapsed: 105s, ETA:    91s
[>>>>>>>>>>>>>>>              ] 269/500, 2.6 task/s, elapsed: 105s, ETA:    90s
[>>>>>>>>>>>>>>>              ] 270/500, 2.6 task/s, elapsed: 105s, ETA:    90s
[>>>>>>>>>>>>>>>              ] 271/500, 2.6 task/s, elapsed: 106s, ETA:    89s
[>>>>>>>>>>>>>>>              ] 272/500, 2.6 task/s, elapsed: 106s, ETA:    89s
[>>>>>>>>>>>>>>>              ] 273/500, 2.6 task/s, elapsed: 107s, ETA:    89s
[>>>>>>>>>>>>>>>              ] 274/500, 2.6 task/s, elapsed: 107s, ETA:    88s
[>>>>>>>>>>>>>>>              ] 275/500, 2.6 task/s, elapsed: 107s, ETA:    88s
[>>>>>>>>>>>>>>>>             ] 276/500, 2.6 task/s, elapsed: 108s, ETA:    87s
[>>>>>>>>>>>>>>>>             ] 277/500, 2.6 task/s, elapsed: 108s, ETA:    87s
[>>>>>>>>>>>>>>>>             ] 278/500, 2.6 task/s, elapsed: 109s, ETA:    87s
[>>>>>>>>>>>>>>>>             ] 279/500, 2.6 task/s, elapsed: 109s, ETA:    86s
[>>>>>>>>>>>>>>>>             ] 280/500, 2.6 task/s, elapsed: 109s, ETA:    86s
[>>>>>>>>>>>>>>>>             ] 281/500, 2.6 task/s, elapsed: 110s, ETA:    86s
[>>>>>>>>>>>>>>>>             ] 282/500, 2.6 task/s, elapsed: 110s, ETA:    85s
[>>>>>>>>>>>>>>>>             ] 283/500, 2.6 task/s, elapsed: 111s, ETA:    85s
[>>>>>>>>>>>>>>>>             ] 284/500, 2.6 task/s, elapsed: 111s, ETA:    84s
[>>>>>>>>>>>>>>>>             ] 285/500, 2.6 task/s, elapsed: 111s, ETA:    84s
[>>>>>>>>>>>>>>>>             ] 286/500, 2.6 task/s, elapsed: 112s, ETA:    84s
[>>>>>>>>>>>>>>>>             ] 287/500, 2.6 task/s, elapsed: 112s, ETA:    83s
[>>>>>>>>>>>>>>>>             ] 288/500, 2.6 task/s, elapsed: 112s, ETA:    83s
[>>>>>>>>>>>>>>>>             ] 289/500, 2.6 task/s, elapsed: 113s, ETA:    82s
[>>>>>>>>>>>>>>>>             ] 290/500, 2.6 task/s, elapsed: 113s, ETA:    82s
[>>>>>>>>>>>>>>>>             ] 291/500, 2.6 task/s, elapsed: 114s, ETA:    82s
[>>>>>>>>>>>>>>>>             ] 292/500, 2.6 task/s, elapsed: 114s, ETA:    81s
[>>>>>>>>>>>>>>>>             ] 293/500, 2.6 task/s, elapsed: 114s, ETA:    81s
[>>>>>>>>>>>>>>>>>            ] 294/500, 2.6 task/s, elapsed: 115s, ETA:    80s
[>>>>>>>>>>>>>>>>>            ] 295/500, 2.6 task/s, elapsed: 115s, ETA:    80s
[>>>>>>>>>>>>>>>>>            ] 296/500, 2.6 task/s, elapsed: 116s, ETA:    80s
[>>>>>>>>>>>>>>>>>            ] 297/500, 2.6 task/s, elapsed: 116s, ETA:    79s
[>>>>>>>>>>>>>>>>>            ] 298/500, 2.6 task/s, elapsed: 116s, ETA:    79s
[>>>>>>>>>>>>>>>>>            ] 299/500, 2.6 task/s, elapsed: 117s, ETA:    78s
[>>>>>>>>>>>>>>>>>            ] 300/500, 2.6 task/s, elapsed: 117s, ETA:    78s
[>>>>>>>>>>>>>>>>>            ] 301/500, 2.6 task/s, elapsed: 117s, ETA:    78s
[>>>>>>>>>>>>>>>>>            ] 302/500, 2.6 task/s, elapsed: 118s, ETA:    77s
[>>>>>>>>>>>>>>>>>            ] 303/500, 2.6 task/s, elapsed: 118s, ETA:    77s
[>>>>>>>>>>>>>>>>>            ] 304/500, 2.6 task/s, elapsed: 119s, ETA:    77s
[>>>>>>>>>>>>>>>>>            ] 305/500, 2.6 task/s, elapsed: 119s, ETA:    76s
[>>>>>>>>>>>>>>>>>            ] 306/500, 2.6 task/s, elapsed: 119s, ETA:    76s
[>>>>>>>>>>>>>>>>>            ] 307/500, 2.6 task/s, elapsed: 120s, ETA:    75s
[>>>>>>>>>>>>>>>>>            ] 308/500, 2.6 task/s, elapsed: 120s, ETA:    75s
[>>>>>>>>>>>>>>>>>            ] 309/500, 2.6 task/s, elapsed: 121s, ETA:    75s
[>>>>>>>>>>>>>>>>>            ] 310/500, 2.6 task/s, elapsed: 121s, ETA:    74s
[>>>>>>>>>>>>>>>>>>           ] 311/500, 2.6 task/s, elapsed: 121s, ETA:    74s
[>>>>>>>>>>>>>>>>>>           ] 312/500, 2.6 task/s, elapsed: 122s, ETA:    73s
[>>>>>>>>>>>>>>>>>>           ] 313/500, 2.6 task/s, elapsed: 122s, ETA:    73s
[>>>>>>>>>>>>>>>>>>           ] 314/500, 2.6 task/s, elapsed: 123s, ETA:    73s
[>>>>>>>>>>>>>>>>>>           ] 315/500, 2.6 task/s, elapsed: 123s, ETA:    72s
[>>>>>>>>>>>>>>>>>>           ] 316/500, 2.6 task/s, elapsed: 123s, ETA:    72s
[>>>>>>>>>>>>>>>>>>           ] 317/500, 2.6 task/s, elapsed: 124s, ETA:    71s
[>>>>>>>>>>>>>>>>>>           ] 318/500, 2.6 task/s, elapsed: 124s, ETA:    71s
[>>>>>>>>>>>>>>>>>>           ] 319/500, 2.6 task/s, elapsed: 125s, ETA:    71s
[>>>>>>>>>>>>>>>>>>           ] 320/500, 2.6 task/s, elapsed: 125s, ETA:    70s
[>>>>>>>>>>>>>>>>>>           ] 321/500, 2.6 task/s, elapsed: 125s, ETA:    70s
[>>>>>>>>>>>>>>>>>>           ] 322/500, 2.6 task/s, elapsed: 126s, ETA:    70s
[>>>>>>>>>>>>>>>>>>           ] 323/500, 2.6 task/s, elapsed: 126s, ETA:    69s
[>>>>>>>>>>>>>>>>>>           ] 324/500, 2.6 task/s, elapsed: 127s, ETA:    69s
[>>>>>>>>>>>>>>>>>>           ] 325/500, 2.6 task/s, elapsed: 127s, ETA:    68s
[>>>>>>>>>>>>>>>>>>           ] 326/500, 2.6 task/s, elapsed: 127s, ETA:    68s
[>>>>>>>>>>>>>>>>>>           ] 327/500, 2.6 task/s, elapsed: 128s, ETA:    68s
[>>>>>>>>>>>>>>>>>>>          ] 328/500, 2.6 task/s, elapsed: 128s, ETA:    67s
[>>>>>>>>>>>>>>>>>>>          ] 329/500, 2.6 task/s, elapsed: 129s, ETA:    67s
[>>>>>>>>>>>>>>>>>>>          ] 330/500, 2.6 task/s, elapsed: 129s, ETA:    66s
[>>>>>>>>>>>>>>>>>>>          ] 331/500, 2.6 task/s, elapsed: 129s, ETA:    66s
[>>>>>>>>>>>>>>>>>>>          ] 332/500, 2.6 task/s, elapsed: 130s, ETA:    66s
[>>>>>>>>>>>>>>>>>>>          ] 333/500, 2.6 task/s, elapsed: 130s, ETA:    65s
[>>>>>>>>>>>>>>>>>>>          ] 334/500, 2.6 task/s, elapsed: 131s, ETA:    65s
[>>>>>>>>>>>>>>>>>>>          ] 335/500, 2.6 task/s, elapsed: 131s, ETA:    65s
[>>>>>>>>>>>>>>>>>>>          ] 336/500, 2.6 task/s, elapsed: 131s, ETA:    64s
[>>>>>>>>>>>>>>>>>>>          ] 337/500, 2.6 task/s, elapsed: 132s, ETA:    64s
[>>>>>>>>>>>>>>>>>>>          ] 338/500, 2.6 task/s, elapsed: 132s, ETA:    63s
[>>>>>>>>>>>>>>>>>>>          ] 339/500, 2.6 task/s, elapsed: 133s, ETA:    63s
[>>>>>>>>>>>>>>>>>>>          ] 340/500, 2.6 task/s, elapsed: 133s, ETA:    63s
[>>>>>>>>>>>>>>>>>>>          ] 341/500, 2.6 task/s, elapsed: 133s, ETA:    62s
[>>>>>>>>>>>>>>>>>>>          ] 342/500, 2.6 task/s, elapsed: 134s, ETA:    62s
[>>>>>>>>>>>>>>>>>>>          ] 343/500, 2.6 task/s, elapsed: 134s, ETA:    61s
[>>>>>>>>>>>>>>>>>>>          ] 344/500, 2.6 task/s, elapsed: 135s, ETA:    61s
[>>>>>>>>>>>>>>>>>>>>         ] 345/500, 2.6 task/s, elapsed: 135s, ETA:    61s
[>>>>>>>>>>>>>>>>>>>>         ] 346/500, 2.6 task/s, elapsed: 135s, ETA:    60s
[>>>>>>>>>>>>>>>>>>>>         ] 347/500, 2.6 task/s, elapsed: 136s, ETA:    60s
[>>>>>>>>>>>>>>>>>>>>         ] 348/500, 2.6 task/s, elapsed: 136s, ETA:    59s
[>>>>>>>>>>>>>>>>>>>>         ] 349/500, 2.6 task/s, elapsed: 137s, ETA:    59s
[>>>>>>>>>>>>>>>>>>>>         ] 350/500, 2.6 task/s, elapsed: 137s, ETA:    59s
[>>>>>>>>>>>>>>>>>>>>         ] 351/500, 2.6 task/s, elapsed: 137s, ETA:    58s
[>>>>>>>>>>>>>>>>>>>>         ] 352/500, 2.6 task/s, elapsed: 138s, ETA:    58s
[>>>>>>>>>>>>>>>>>>>>         ] 353/500, 2.6 task/s, elapsed: 138s, ETA:    58s
[>>>>>>>>>>>>>>>>>>>>         ] 354/500, 2.6 task/s, elapsed: 138s, ETA:    57s
[>>>>>>>>>>>>>>>>>>>>         ] 355/500, 2.6 task/s, elapsed: 139s, ETA:    57s
[>>>>>>>>>>>>>>>>>>>>         ] 356/500, 2.6 task/s, elapsed: 139s, ETA:    56s
[>>>>>>>>>>>>>>>>>>>>         ] 357/500, 2.6 task/s, elapsed: 140s, ETA:    56s
[>>>>>>>>>>>>>>>>>>>>         ] 358/500, 2.6 task/s, elapsed: 140s, ETA:    56s
[>>>>>>>>>>>>>>>>>>>>         ] 359/500, 2.6 task/s, elapsed: 140s, ETA:    55s
[>>>>>>>>>>>>>>>>>>>>         ] 360/500, 2.6 task/s, elapsed: 141s, ETA:    55s
[>>>>>>>>>>>>>>>>>>>>         ] 361/500, 2.6 task/s, elapsed: 141s, ETA:    54s
[>>>>>>>>>>>>>>>>>>>>         ] 362/500, 2.6 task/s, elapsed: 142s, ETA:    54s
[>>>>>>>>>>>>>>>>>>>>>        ] 363/500, 2.6 task/s, elapsed: 142s, ETA:    54s
[>>>>>>>>>>>>>>>>>>>>>        ] 364/500, 2.6 task/s, elapsed: 142s, ETA:    53s
[>>>>>>>>>>>>>>>>>>>>>        ] 365/500, 2.6 task/s, elapsed: 143s, ETA:    53s
[>>>>>>>>>>>>>>>>>>>>>        ] 366/500, 2.6 task/s, elapsed: 143s, ETA:    52s
[>>>>>>>>>>>>>>>>>>>>>        ] 367/500, 2.6 task/s, elapsed: 144s, ETA:    52s
[>>>>>>>>>>>>>>>>>>>>>        ] 368/500, 2.6 task/s, elapsed: 144s, ETA:    52s
[>>>>>>>>>>>>>>>>>>>>>        ] 369/500, 2.6 task/s, elapsed: 144s, ETA:    51s
[>>>>>>>>>>>>>>>>>>>>>        ] 370/500, 2.6 task/s, elapsed: 145s, ETA:    51s
[>>>>>>>>>>>>>>>>>>>>>        ] 371/500, 2.6 task/s, elapsed: 145s, ETA:    50s
[>>>>>>>>>>>>>>>>>>>>>        ] 372/500, 2.6 task/s, elapsed: 146s, ETA:    50s
[>>>>>>>>>>>>>>>>>>>>>        ] 373/500, 2.6 task/s, elapsed: 146s, ETA:    50s
[>>>>>>>>>>>>>>>>>>>>>        ] 374/500, 2.6 task/s, elapsed: 146s, ETA:    49s
[>>>>>>>>>>>>>>>>>>>>>        ] 375/500, 2.6 task/s, elapsed: 147s, ETA:    49s
[>>>>>>>>>>>>>>>>>>>>>        ] 376/500, 2.6 task/s, elapsed: 147s, ETA:    49s
[>>>>>>>>>>>>>>>>>>>>>        ] 377/500, 2.6 task/s, elapsed: 148s, ETA:    48s
[>>>>>>>>>>>>>>>>>>>>>        ] 378/500, 2.6 task/s, elapsed: 148s, ETA:    48s
[>>>>>>>>>>>>>>>>>>>>>        ] 379/500, 2.6 task/s, elapsed: 148s, ETA:    47s
[>>>>>>>>>>>>>>>>>>>>>>       ] 380/500, 2.6 task/s, elapsed: 149s, ETA:    47s
[>>>>>>>>>>>>>>>>>>>>>>       ] 381/500, 2.6 task/s, elapsed: 149s, ETA:    47s
[>>>>>>>>>>>>>>>>>>>>>>       ] 382/500, 2.6 task/s, elapsed: 149s, ETA:    46s
[>>>>>>>>>>>>>>>>>>>>>>       ] 383/500, 2.6 task/s, elapsed: 150s, ETA:    46s
[>>>>>>>>>>>>>>>>>>>>>>       ] 384/500, 2.6 task/s, elapsed: 150s, ETA:    45s
[>>>>>>>>>>>>>>>>>>>>>>       ] 385/500, 2.6 task/s, elapsed: 151s, ETA:    45s
[>>>>>>>>>>>>>>>>>>>>>>       ] 386/500, 2.6 task/s, elapsed: 151s, ETA:    45s
[>>>>>>>>>>>>>>>>>>>>>>       ] 387/500, 2.6 task/s, elapsed: 151s, ETA:    44s
[>>>>>>>>>>>>>>>>>>>>>>       ] 388/500, 2.6 task/s, elapsed: 152s, ETA:    44s
[>>>>>>>>>>>>>>>>>>>>>>       ] 389/500, 2.6 task/s, elapsed: 152s, ETA:    43s
[>>>>>>>>>>>>>>>>>>>>>>       ] 390/500, 2.6 task/s, elapsed: 153s, ETA:    43s
[>>>>>>>>>>>>>>>>>>>>>>       ] 391/500, 2.6 task/s, elapsed: 153s, ETA:    43s
[>>>>>>>>>>>>>>>>>>>>>>       ] 392/500, 2.6 task/s, elapsed: 153s, ETA:    42s
[>>>>>>>>>>>>>>>>>>>>>>       ] 393/500, 2.6 task/s, elapsed: 154s, ETA:    42s
[>>>>>>>>>>>>>>>>>>>>>>       ] 394/500, 2.6 task/s, elapsed: 154s, ETA:    42s
[>>>>>>>>>>>>>>>>>>>>>>       ] 395/500, 2.6 task/s, elapsed: 155s, ETA:    41s
[>>>>>>>>>>>>>>>>>>>>>>       ] 396/500, 2.6 task/s, elapsed: 155s, ETA:    41s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 397/500, 2.6 task/s, elapsed: 155s, ETA:    40s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 398/500, 2.6 task/s, elapsed: 156s, ETA:    40s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 399/500, 2.6 task/s, elapsed: 156s, ETA:    40s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 400/500, 2.6 task/s, elapsed: 157s, ETA:    39s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 401/500, 2.6 task/s, elapsed: 157s, ETA:    39s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 402/500, 2.6 task/s, elapsed: 158s, ETA:    38s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 403/500, 2.6 task/s, elapsed: 158s, ETA:    38s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 404/500, 2.6 task/s, elapsed: 158s, ETA:    38s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 405/500, 2.6 task/s, elapsed: 159s, ETA:    37s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 406/500, 2.6 task/s, elapsed: 159s, ETA:    37s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 407/500, 2.6 task/s, elapsed: 160s, ETA:    36s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 408/500, 2.6 task/s, elapsed: 160s, ETA:    36s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 409/500, 2.5 task/s, elapsed: 160s, ETA:    36s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 410/500, 2.5 task/s, elapsed: 161s, ETA:    35s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 411/500, 2.5 task/s, elapsed: 161s, ETA:    35s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 412/500, 2.5 task/s, elapsed: 162s, ETA:    35s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 413/500, 2.5 task/s, elapsed: 162s, ETA:    34s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 414/500, 2.5 task/s, elapsed: 162s, ETA:    34s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 415/500, 2.5 task/s, elapsed: 163s, ETA:    33s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 416/500, 2.5 task/s, elapsed: 163s, ETA:    33s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 417/500, 2.5 task/s, elapsed: 164s, ETA:    33s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 418/500, 2.5 task/s, elapsed: 164s, ETA:    32s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 419/500, 2.5 task/s, elapsed: 165s, ETA:    32s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 420/500, 2.5 task/s, elapsed: 165s, ETA:    31s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 421/500, 2.5 task/s, elapsed: 165s, ETA:    31s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 422/500, 2.5 task/s, elapsed: 166s, ETA:    31s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 423/500, 2.5 task/s, elapsed: 166s, ETA:    30s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 424/500, 2.5 task/s, elapsed: 167s, ETA:    30s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 425/500, 2.5 task/s, elapsed: 167s, ETA:    29s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 426/500, 2.5 task/s, elapsed: 167s, ETA:    29s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 427/500, 2.5 task/s, elapsed: 168s, ETA:    29s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 428/500, 2.5 task/s, elapsed: 168s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 429/500, 2.5 task/s, elapsed: 169s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 430/500, 2.5 task/s, elapsed: 169s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 431/500, 2.5 task/s, elapsed: 169s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 432/500, 2.5 task/s, elapsed: 170s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 433/500, 2.5 task/s, elapsed: 170s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 434/500, 2.5 task/s, elapsed: 171s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 435/500, 2.5 task/s, elapsed: 171s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 436/500, 2.5 task/s, elapsed: 171s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 437/500, 2.5 task/s, elapsed: 172s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 438/500, 2.5 task/s, elapsed: 172s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 439/500, 2.5 task/s, elapsed: 173s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 440/500, 2.5 task/s, elapsed: 173s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 441/500, 2.5 task/s, elapsed: 173s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 442/500, 2.5 task/s, elapsed: 174s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 443/500, 2.5 task/s, elapsed: 174s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 444/500, 2.5 task/s, elapsed: 175s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 445/500, 2.5 task/s, elapsed: 175s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 446/500, 2.5 task/s, elapsed: 175s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 447/500, 2.5 task/s, elapsed: 176s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 448/500, 2.5 task/s, elapsed: 176s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 449/500, 2.5 task/s, elapsed: 177s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 450/500, 2.5 task/s, elapsed: 177s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 451/500, 2.5 task/s, elapsed: 177s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 452/500, 2.5 task/s, elapsed: 178s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 453/500, 2.5 task/s, elapsed: 178s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 454/500, 2.5 task/s, elapsed: 179s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 455/500, 2.5 task/s, elapsed: 179s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 456/500, 2.5 task/s, elapsed: 179s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 457/500, 2.5 task/s, elapsed: 180s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 458/500, 2.5 task/s, elapsed: 180s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 459/500, 2.5 task/s, elapsed: 180s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 460/500, 2.5 task/s, elapsed: 181s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 461/500, 2.5 task/s, elapsed: 181s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 462/500, 2.5 task/s, elapsed: 182s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 463/500, 2.5 task/s, elapsed: 182s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 464/500, 2.5 task/s, elapsed: 182s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 465/500, 2.5 task/s, elapsed: 183s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 466/500, 2.5 task/s, elapsed: 183s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 467/500, 2.5 task/s, elapsed: 184s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 468/500, 2.5 task/s, elapsed: 184s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 469/500, 2.5 task/s, elapsed: 184s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 470/500, 2.5 task/s, elapsed: 185s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 471/500, 2.5 task/s, elapsed: 185s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 472/500, 2.5 task/s, elapsed: 186s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 473/500, 2.5 task/s, elapsed: 186s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 474/500, 2.5 task/s, elapsed: 186s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 475/500, 2.5 task/s, elapsed: 187s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 476/500, 2.5 task/s, elapsed: 187s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 477/500, 2.5 task/s, elapsed: 188s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 478/500, 2.5 task/s, elapsed: 188s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 479/500, 2.5 task/s, elapsed: 188s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 480/500, 2.5 task/s, elapsed: 189s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 481/500, 2.5 task/s, elapsed: 189s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 482/500, 2.5 task/s, elapsed: 190s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 483/500, 2.5 task/s, elapsed: 190s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 484/500, 2.5 task/s, elapsed: 190s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 485/500, 2.5 task/s, elapsed: 191s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 486/500, 2.5 task/s, elapsed: 191s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 487/500, 2.5 task/s, elapsed: 192s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 488/500, 2.5 task/s, elapsed: 192s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 489/500, 2.5 task/s, elapsed: 192s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 490/500, 2.5 task/s, elapsed: 193s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 491/500, 2.5 task/s, elapsed: 193s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 492/500, 2.5 task/s, elapsed: 194s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 493/500, 2.5 task/s, elapsed: 194s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 494/500, 2.5 task/s, elapsed: 194s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 495/500, 2.5 task/s, elapsed: 195s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 496/500, 2.5 task/s, elapsed: 195s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 497/500, 2.5 task/s, elapsed: 196s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 498/500, 2.5 task/s, elapsed: 196s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 499/500, 2.5 task/s, elapsed: 196s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 500/500, 2.5 task/s, elapsed: 197s, ETA:     0s/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:28:30,780 - mmseg - INFO - Iter [35400/40000]	lr: 6.902e-06, eta: 1:58:22, time: 1.409, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1013, decode.acc_seg: 91.7086, src.loss_imnet_feat_dist: 0.0967, mix.decode.loss_seg: 0.1188, mix.decode.acc_seg: 89.7202
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:29:48,185 - mmseg - INFO - Iter [35450/40000]	lr: 6.826e-06, eta: 1:57:05, time: 1.548, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0964, decode.acc_seg: 91.1697, src.loss_imnet_feat_dist: 0.0935, mix.decode.loss_seg: 0.1073, mix.decode.acc_seg: 90.0608
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:31:06,430 - mmseg - INFO - Iter [35500/40000]	lr: 6.751e-06, eta: 1:55:48, time: 1.565, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1075, decode.acc_seg: 90.7138, src.loss_imnet_feat_dist: 0.0934, mix.decode.loss_seg: 0.1193, mix.decode.acc_seg: 89.3362
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:31:40,360 - mmseg - INFO - Iter [16050/40000]	lr: 3.593e-05, eta: 22:45:16, time: 8.205, data_time: 4.844, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.8720, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 63.1574
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:34:28,403 - mmseg - INFO - Iter [16100/40000]	lr: 3.585e-05, eta: 22:42:20, time: 3.361, data_time: 0.019, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.3326, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.6798
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:37:17,023 - mmseg - INFO - Iter [16150/40000]	lr: 3.578e-05, eta: 22:39:26, time: 3.372, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.0657, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.1363
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:40:04,845 - mmseg - INFO - Iter [16200/40000]	lr: 3.570e-05, eta: 22:36:30, time: 3.356, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.5983, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.3320
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:42:56,700 - mmseg - INFO - Iter [16250/40000]	lr: 3.563e-05, eta: 22:33:40, time: 3.437, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.3206, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.9218
[                                                  ] 0/500, elapsed: 0s, ETA:
[                                 ] 1/500, 1.5 task/s, elapsed: 1s, ETA:   344s
[                                 ] 2/500, 2.3 task/s, elapsed: 1s, ETA:   215s
[                                 ] 3/500, 3.0 task/s, elapsed: 1s, ETA:   168s
[                                 ] 4/500, 3.5 task/s, elapsed: 1s, ETA:   143s
[                                 ] 5/500, 3.8 task/s, elapsed: 1s, ETA:   129s
[                                 ] 6/500, 4.1 task/s, elapsed: 1s, ETA:   119s
[                                 ] 7/500, 4.4 task/s, elapsed: 2s, ETA:   112s
[                                 ] 8/500, 4.6 task/s, elapsed: 2s, ETA:   106s
[                                 ] 9/500, 4.8 task/s, elapsed: 2s, ETA:   102s
[                                ] 10/500, 5.0 task/s, elapsed: 2s, ETA:    98s
[                                ] 11/500, 5.1 task/s, elapsed: 2s, ETA:    96s
[                                ] 12/500, 5.2 task/s, elapsed: 2s, ETA:    93s
[                                ] 13/500, 5.3 task/s, elapsed: 2s, ETA:    92s
[                                ] 14/500, 5.4 task/s, elapsed: 3s, ETA:    90s
[                                ] 15/500, 5.5 task/s, elapsed: 3s, ETA:    89s
[>                               ] 16/500, 5.5 task/s, elapsed: 3s, ETA:    88s
[>                               ] 17/500, 5.6 task/s, elapsed: 3s, ETA:    87s
[>                               ] 18/500, 5.6 task/s, elapsed: 3s, ETA:    86s
[>                               ] 19/500, 5.7 task/s, elapsed: 3s, ETA:    85s
[>                               ] 20/500, 5.7 task/s, elapsed: 3s, ETA:    84s
[>                               ] 21/500, 5.8 task/s, elapsed: 4s, ETA:    83s
[>                               ] 22/500, 5.8 task/s, elapsed: 4s, ETA:    82s
[>                               ] 23/500, 5.9 task/s, elapsed: 4s, ETA:    81s
[>                               ] 24/500, 5.9 task/s, elapsed: 4s, ETA:    81s
[>                               ] 25/500, 5.9 task/s, elapsed: 4s, ETA:    80s
[>                               ] 26/500, 6.0 task/s, elapsed: 4s, ETA:    79s
[>                               ] 27/500, 6.0 task/s, elapsed: 5s, ETA:    79s
[>                               ] 28/500, 6.0 task/s, elapsed: 5s, ETA:    78s
[>                               ] 29/500, 6.1 task/s, elapsed: 5s, ETA:    78s
[>                               ] 30/500, 6.1 task/s, elapsed: 5s, ETA:    77s
[>                               ] 31/500, 6.1 task/s, elapsed: 5s, ETA:    77s
[>>                              ] 32/500, 6.1 task/s, elapsed: 5s, ETA:    77s
[>>                              ] 33/500, 6.1 task/s, elapsed: 5s, ETA:    76s
[>>                              ] 34/500, 6.1 task/s, elapsed: 6s, ETA:    76s
[>>                              ] 35/500, 6.1 task/s, elapsed: 6s, ETA:    76s
[>>                              ] 36/500, 6.2 task/s, elapsed: 6s, ETA:    75s
[>>                              ] 37/500, 6.2 task/s, elapsed: 6s, ETA:    75s
[>>                              ] 38/500, 6.2 task/s, elapsed: 6s, ETA:    75s
[>>                              ] 39/500, 6.2 task/s, elapsed: 6s, ETA:    75s
[>>                              ] 40/500, 6.2 task/s, elapsed: 6s, ETA:    74s
[>>                              ] 41/500, 6.2 task/s, elapsed: 7s, ETA:    74s
[>>                              ] 42/500, 6.2 task/s, elapsed: 7s, ETA:    74s
[>>                              ] 43/500, 6.2 task/s, elapsed: 7s, ETA:    74s
[>>                              ] 44/500, 6.2 task/s, elapsed: 7s, ETA:    73s
[>>                              ] 45/500, 6.2 task/s, elapsed: 7s, ETA:    73s
[>>                              ] 46/500, 6.2 task/s, elapsed: 7s, ETA:    73s
[>>>                             ] 47/500, 6.2 task/s, elapsed: 8s, ETA:    73s
[>>>                             ] 48/500, 6.2 task/s, elapsed: 8s, ETA:    72s
[>>>                             ] 49/500, 6.3 task/s, elapsed: 8s, ETA:    72s
[>>>                             ] 50/500, 6.3 task/s, elapsed: 8s, ETA:    72s
[>>>                             ] 51/500, 6.3 task/s, elapsed: 8s, ETA:    72s
[>>>                             ] 52/500, 6.3 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 53/500, 6.3 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 54/500, 6.3 task/s, elapsed: 9s, ETA:    71s
[>>>                             ] 55/500, 6.3 task/s, elapsed: 9s, ETA:    71s
[>>>                             ] 56/500, 6.3 task/s, elapsed: 9s, ETA:    71s
[>>>                             ] 57/500, 6.3 task/s, elapsed: 9s, ETA:    70s
[>>>                             ] 58/500, 6.3 task/s, elapsed: 9s, ETA:    70s
[>>>                             ] 59/500, 6.3 task/s, elapsed: 9s, ETA:    70s
[>>>                            ] 60/500, 6.3 task/s, elapsed: 10s, ETA:    70s
[>>>                            ] 61/500, 6.3 task/s, elapsed: 10s, ETA:    70s
[>>>                            ] 62/500, 6.3 task/s, elapsed: 10s, ETA:    69s
[>>>                            ] 63/500, 6.3 task/s, elapsed: 10s, ETA:    69s
[>>>                            ] 64/500, 6.3 task/s, elapsed: 10s, ETA:    69s
[>>>>                           ] 65/500, 6.3 task/s, elapsed: 10s, ETA:    69s
[>>>>                           ] 66/500, 6.3 task/s, elapsed: 10s, ETA:    69s
[>>>>                           ] 67/500, 6.3 task/s, elapsed: 11s, ETA:    69s
[>>>>                           ] 68/500, 6.3 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 69/500, 6.3 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 70/500, 6.3 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 71/500, 6.3 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 72/500, 6.3 task/s, elapsed: 11s, ETA:    68s
[>>>>                           ] 73/500, 6.3 task/s, elapsed: 12s, ETA:    67s
[>>>>                           ] 74/500, 6.3 task/s, elapsed: 12s, ETA:    67s
[>>>>                           ] 75/500, 6.3 task/s, elapsed: 12s, ETA:    67s
[>>>>                           ] 76/500, 6.3 task/s, elapsed: 12s, ETA:    67s
[>>>>                           ] 77/500, 6.4 task/s, elapsed: 12s, ETA:    67s
[>>>>                           ] 78/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 79/500, 6.4 task/s, elapsed: 12s, ETA:    66s
[>>>>                           ] 80/500, 6.4 task/s, elapsed: 13s, ETA:    66s
[>>>>>                          ] 81/500, 6.4 task/s, elapsed: 13s, ETA:    66s
[>>>>>                          ] 82/500, 6.4 task/s, elapsed: 13s, ETA:    66s
[>>>>>                          ] 83/500, 6.4 task/s, elapsed: 13s, ETA:    66s
[>>>>>                          ] 84/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 85/500, 6.4 task/s, elapsed: 13s, ETA:    65s
[>>>>>                          ] 86/500, 6.4 task/s, elapsed: 14s, ETA:    65s
[>>>>>                          ] 87/500, 6.4 task/s, elapsed: 14s, ETA:    65s
[>>>>>                          ] 88/500, 6.4 task/s, elapsed: 14s, ETA:    65s
[>>>>>                          ] 89/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 90/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 91/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 92/500, 6.4 task/s, elapsed: 14s, ETA:    64s
[>>>>>                          ] 93/500, 6.4 task/s, elapsed: 15s, ETA:    64s
[>>>>>                          ] 94/500, 6.4 task/s, elapsed: 15s, ETA:    63s
[>>>>>                          ] 95/500, 6.4 task/s, elapsed: 15s, ETA:    63s
[>>>>>                          ] 96/500, 6.4 task/s, elapsed: 15s, ETA:    63s
[>>>>>>                         ] 97/500, 6.4 task/s, elapsed: 15s, ETA:    63s
[>>>>>>                         ] 98/500, 6.4 task/s, elapsed: 15s, ETA:    63s
[>>>>>>                         ] 99/500, 6.4 task/s, elapsed: 15s, ETA:    63s
[>>>>>>                        ] 100/500, 6.4 task/s, elapsed: 16s, ETA:    62s
[>>>>>>                        ] 101/500, 6.4 task/s, elapsed: 16s, ETA:    62s
[>>>>>>                        ] 102/500, 6.4 task/s, elapsed: 16s, ETA:    62s
[>>>>>>                        ] 103/500, 6.4 task/s, elapsed: 16s, ETA:    62s
[>>>>>>                        ] 104/500, 6.4 task/s, elapsed: 16s, ETA:    62s
[>>>>>>                        ] 105/500, 6.4 task/s, elapsed: 16s, ETA:    61s
[>>>>>>                        ] 106/500, 6.4 task/s, elapsed: 16s, ETA:    61s
[>>>>>>                        ] 107/500, 6.4 task/s, elapsed: 17s, ETA:    61s
[>>>>>>                        ] 108/500, 6.4 task/s, elapsed: 17s, ETA:    61s
[>>>>>>                        ] 109/500, 6.4 task/s, elapsed: 17s, ETA:    61s
[>>>>>>                        ] 110/500, 6.4 task/s, elapsed: 17s, ETA:    61s
[>>>>>>                        ] 111/500, 6.4 task/s, elapsed: 17s, ETA:    60s
[>>>>>>                        ] 112/500, 6.4 task/s, elapsed: 17s, ETA:    60s
[>>>>>>                        ] 113/500, 6.4 task/s, elapsed: 18s, ETA:    60s
[>>>>>>                        ] 114/500, 6.4 task/s, elapsed: 18s, ETA:    60s
[>>>>>>                        ] 115/500, 6.4 task/s, elapsed: 18s, ETA:    60s
[>>>>>>                        ] 116/500, 6.4 task/s, elapsed: 18s, ETA:    60s
[>>>>>>>                       ] 117/500, 6.5 task/s, elapsed: 18s, ETA:    59s
[>>>>>>>                       ] 118/500, 6.5 task/s, elapsed: 18s, ETA:    59s
[>>>>>>>                       ] 119/500, 6.5 task/s, elapsed: 18s, ETA:    59s
[>>>>>>>                       ] 120/500, 6.5 task/s, elapsed: 19s, ETA:    59s
[>>>>>>>                       ] 121/500, 6.5 task/s, elapsed: 19s, ETA:    59s
[>>>>>>>                       ] 122/500, 6.5 task/s, elapsed: 19s, ETA:    58s
[>>>>>>>                       ] 123/500, 6.5 task/s, elapsed: 19s, ETA:    58s
[>>>>>>>                       ] 124/500, 6.5 task/s, elapsed: 19s, ETA:    58s
[>>>>>>>                       ] 125/500, 6.5 task/s, elapsed: 19s, ETA:    58s
[>>>>>>>                       ] 126/500, 6.5 task/s, elapsed: 20s, ETA:    58s
[>>>>>>>                       ] 127/500, 6.5 task/s, elapsed: 20s, ETA:    58s
[>>>>>>>                       ] 128/500, 6.5 task/s, elapsed: 20s, ETA:    58s
[>>>>>>>                       ] 129/500, 6.5 task/s, elapsed: 20s, ETA:    57s
[>>>>>>>                       ] 130/500, 6.5 task/s, elapsed: 20s, ETA:    57s
[>>>>>>>                       ] 131/500, 6.5 task/s, elapsed: 20s, ETA:    57s
[>>>>>>>                       ] 132/500, 6.5 task/s, elapsed: 20s, ETA:    57s
[>>>>>>>                       ] 133/500, 6.5 task/s, elapsed: 21s, ETA:    57s
[>>>>>>>>                      ] 134/500, 6.5 task/s, elapsed: 21s, ETA:    57s
[>>>>>>>>                      ] 135/500, 6.5 task/s, elapsed: 21s, ETA:    57s
[>>>>>>>>                      ] 136/500, 6.5 task/s, elapsed: 21s, ETA:    56s
[>>>>>>>>                      ] 137/500, 6.5 task/s, elapsed: 21s, ETA:    56s
[>>>>>>>>                      ] 138/500, 6.5 task/s, elapsed: 21s, ETA:    56s
[>>>>>>>>                      ] 139/500, 6.5 task/s, elapsed: 22s, ETA:    56s
[>>>>>>>>                      ] 140/500, 6.5 task/s, elapsed: 22s, ETA:    56s
[>>>>>>>>                      ] 141/500, 6.5 task/s, elapsed: 22s, ETA:    56s
[>>>>>>>>                      ] 142/500, 6.4 task/s, elapsed: 22s, ETA:    56s
[>>>>>>>>                      ] 143/500, 6.4 task/s, elapsed: 22s, ETA:    55s
[>>>>>>>>                      ] 144/500, 6.4 task/s, elapsed: 22s, ETA:    55s
[>>>>>>>>                      ] 145/500, 6.4 task/s, elapsed: 22s, ETA:    55s
[>>>>>>>>                      ] 146/500, 6.4 task/s, elapsed: 23s, ETA:    55s
[>>>>>>>>                      ] 147/500, 6.4 task/s, elapsed: 23s, ETA:    55s
[>>>>>>>>                      ] 148/500, 6.4 task/s, elapsed: 23s, ETA:    55s
[>>>>>>>>                      ] 149/500, 6.4 task/s, elapsed: 23s, ETA:    54s
[>>>>>>>>>                     ] 150/500, 6.4 task/s, elapsed: 23s, ETA:    54s
[>>>>>>>>>                     ] 151/500, 6.4 task/s, elapsed: 23s, ETA:    54s
[>>>>>>>>>                     ] 152/500, 6.4 task/s, elapsed: 24s, ETA:    54s
[>>>>>>>>>                     ] 153/500, 6.4 task/s, elapsed: 24s, ETA:    54s
[>>>>>>>>>                     ] 154/500, 6.4 task/s, elapsed: 24s, ETA:    54s
[>>>>>>>>>                     ] 155/500, 6.4 task/s, elapsed: 24s, ETA:    54s
[>>>>>>>>>                     ] 156/500, 6.4 task/s, elapsed: 24s, ETA:    53s
[>>>>>>>>>                     ] 157/500, 6.4 task/s, elapsed: 24s, ETA:    53s
[>>>>>>>>>                     ] 158/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 159/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 160/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 161/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 162/500, 6.4 task/s, elapsed: 25s, ETA:    53s
[>>>>>>>>>                     ] 163/500, 6.4 task/s, elapsed: 25s, ETA:    52s
[>>>>>>>>>                     ] 164/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>                     ] 165/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>                     ] 166/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>>                    ] 167/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>>                    ] 168/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>>                    ] 169/500, 6.4 task/s, elapsed: 26s, ETA:    52s
[>>>>>>>>>>                    ] 170/500, 6.4 task/s, elapsed: 26s, ETA:    51s
[>>>>>>>>>>                    ] 171/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 172/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 173/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 174/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 175/500, 6.4 task/s, elapsed: 27s, ETA:    51s
[>>>>>>>>>>                    ] 176/500, 6.4 task/s, elapsed: 27s, ETA:    50s
[>>>>>>>>>>                    ] 177/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 178/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 179/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 180/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 181/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 182/500, 6.4 task/s, elapsed: 28s, ETA:    50s
[>>>>>>>>>>                    ] 183/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 184/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 185/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 186/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 187/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 188/500, 6.4 task/s, elapsed: 29s, ETA:    49s
[>>>>>>>>>>>                   ] 189/500, 6.4 task/s, elapsed: 29s, ETA:    48s
[>>>>>>>>>>>                   ] 190/500, 6.4 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 191/500, 6.4 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 192/500, 6.4 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 193/500, 6.4 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 194/500, 6.4 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 195/500, 6.4 task/s, elapsed: 30s, ETA:    48s
[>>>>>>>>>>>                   ] 196/500, 6.4 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>                   ] 197/500, 6.4 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>                   ] 198/500, 6.4 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>                   ] 199/500, 6.4 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>>                  ] 200/500, 6.4 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>>                  ] 201/500, 6.4 task/s, elapsed: 31s, ETA:    47s
[>>>>>>>>>>>>                  ] 202/500, 6.4 task/s, elapsed: 31s, ETA:    46s
[>>>>>>>>>>>>                  ] 203/500, 6.4 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 204/500, 6.4 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 205/500, 6.4 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 206/500, 6.4 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 207/500, 6.4 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 208/500, 6.4 task/s, elapsed: 32s, ETA:    46s
[>>>>>>>>>>>>                  ] 209/500, 6.4 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 210/500, 6.4 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 211/500, 6.4 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 212/500, 6.4 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 213/500, 6.4 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 214/500, 6.4 task/s, elapsed: 33s, ETA:    45s
[>>>>>>>>>>>>                  ] 215/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>                  ] 216/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 217/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 218/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 219/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 220/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 221/500, 6.4 task/s, elapsed: 34s, ETA:    44s
[>>>>>>>>>>>>>                 ] 222/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 223/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 224/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 225/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 226/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 227/500, 6.4 task/s, elapsed: 35s, ETA:    43s
[>>>>>>>>>>>>>                 ] 228/500, 6.4 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>                 ] 229/500, 6.4 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>                 ] 230/500, 6.4 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>                 ] 231/500, 6.4 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>                 ] 232/500, 6.4 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>                 ] 233/500, 6.4 task/s, elapsed: 36s, ETA:    42s
[>>>>>>>>>>>>>>                ] 234/500, 6.4 task/s, elapsed: 36s, ETA:    41s
[>>>>>>>>>>>>>>                ] 235/500, 6.4 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 236/500, 6.4 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 237/500, 6.4 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 238/500, 6.4 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 239/500, 6.4 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 240/500, 6.4 task/s, elapsed: 37s, ETA:    41s
[>>>>>>>>>>>>>>                ] 241/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 242/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 243/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 244/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 245/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 246/500, 6.4 task/s, elapsed: 38s, ETA:    40s
[>>>>>>>>>>>>>>                ] 247/500, 6.4 task/s, elapsed: 38s, ETA:    39s
[>>>>>>>>>>>>>>                ] 248/500, 6.4 task/s, elapsed: 39s, ETA:    39s
[>>>>>>>>>>>>>>                ] 249/500, 6.4 task/s, elapsed: 39s, ETA:    39s
[>>>>>>>>>>>>>>>               ] 250/500, 6.4 task/s, elapsed: 39s, ETA:    39s
[>>>>>>>>>>>>>>>               ] 251/500, 6.4 task/s, elapsed: 39s, ETA:    39s
[>>>>>>>>>>>>>>>               ] 252/500, 6.4 task/s, elapsed: 39s, ETA:    39s
[>>>>>>>>>>>>>>>               ] 253/500, 6.4 task/s, elapsed: 39s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 254/500, 6.4 task/s, elapsed: 40s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 255/500, 6.4 task/s, elapsed: 40s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 256/500, 6.4 task/s, elapsed: 40s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 257/500, 6.4 task/s, elapsed: 40s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 258/500, 6.4 task/s, elapsed: 40s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 259/500, 6.4 task/s, elapsed: 40s, ETA:    38s
[>>>>>>>>>>>>>>>               ] 260/500, 6.4 task/s, elapsed: 40s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 261/500, 6.4 task/s, elapsed: 41s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 262/500, 6.4 task/s, elapsed: 41s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 263/500, 6.4 task/s, elapsed: 41s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 264/500, 6.4 task/s, elapsed: 41s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 265/500, 6.4 task/s, elapsed: 41s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 266/500, 6.4 task/s, elapsed: 41s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 267/500, 6.4 task/s, elapsed: 42s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 268/500, 6.4 task/s, elapsed: 42s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 269/500, 6.4 task/s, elapsed: 42s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 270/500, 6.4 task/s, elapsed: 42s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 271/500, 6.4 task/s, elapsed: 42s, ETA:    36s
[>>>>>>>>>>>>>>>>              ] 272/500, 6.4 task/s, elapsed: 42s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 273/500, 6.4 task/s, elapsed: 42s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 274/500, 6.4 task/s, elapsed: 43s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 275/500, 6.4 task/s, elapsed: 43s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 276/500, 6.4 task/s, elapsed: 43s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 277/500, 6.4 task/s, elapsed: 43s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 278/500, 6.4 task/s, elapsed: 43s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 279/500, 6.4 task/s, elapsed: 43s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 280/500, 6.4 task/s, elapsed: 43s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 281/500, 6.4 task/s, elapsed: 44s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 282/500, 6.4 task/s, elapsed: 44s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 283/500, 6.4 task/s, elapsed: 44s, ETA:    34s
[>>>>>>>>>>>>>>>>>             ] 284/500, 6.4 task/s, elapsed: 44s, ETA:    34s
[>>>>>>>>>>>>>>>>>             ] 285/500, 6.4 task/s, elapsed: 44s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 286/500, 6.4 task/s, elapsed: 44s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 287/500, 6.4 task/s, elapsed: 45s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 288/500, 6.4 task/s, elapsed: 45s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 289/500, 6.4 task/s, elapsed: 45s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 290/500, 6.4 task/s, elapsed: 45s, ETA:    33s
[>>>>>>>>>>>>>>>>>             ] 291/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 292/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 293/500, 6.5 task/s, elapsed: 45s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 294/500, 6.5 task/s, elapsed: 46s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 295/500, 6.5 task/s, elapsed: 46s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 296/500, 6.5 task/s, elapsed: 46s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 297/500, 6.4 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 298/500, 6.4 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 299/500, 6.4 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>>            ] 300/500, 6.5 task/s, elapsed: 46s, ETA:    31s
[>>>>>>>>>>>>>>>>>>            ] 301/500, 6.5 task/s, elapsed: 47s, ETA:    31s
[>>>>>>>>>>>>>>>>>>            ] 302/500, 6.5 task/s, elapsed: 47s, ETA:    31s
[>>>>>>>>>>>>>>>>>>            ] 303/500, 6.5 task/s, elapsed: 47s, ETA:    31s
[>>>>>>>>>>>>>>>>>>            ] 304/500, 6.5 task/s, elapsed: 47s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 305/500, 6.5 task/s, elapsed: 47s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 306/500, 6.5 task/s, elapsed: 47s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 307/500, 6.5 task/s, elapsed: 48s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 308/500, 6.5 task/s, elapsed: 48s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 309/500, 6.5 task/s, elapsed: 48s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 310/500, 6.5 task/s, elapsed: 48s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 311/500, 6.5 task/s, elapsed: 48s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 312/500, 6.5 task/s, elapsed: 48s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 313/500, 6.5 task/s, elapsed: 48s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 314/500, 6.5 task/s, elapsed: 49s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 315/500, 6.5 task/s, elapsed: 49s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 316/500, 6.5 task/s, elapsed: 49s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 317/500, 6.5 task/s, elapsed: 49s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 318/500, 6.5 task/s, elapsed: 49s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 319/500, 6.5 task/s, elapsed: 49s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 320/500, 6.5 task/s, elapsed: 50s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 321/500, 6.5 task/s, elapsed: 50s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 322/500, 6.5 task/s, elapsed: 50s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>           ] 323/500, 6.5 task/s, elapsed: 50s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 324/500, 6.5 task/s, elapsed: 50s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 325/500, 6.5 task/s, elapsed: 50s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 326/500, 6.5 task/s, elapsed: 50s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 327/500, 6.5 task/s, elapsed: 51s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 328/500, 6.5 task/s, elapsed: 51s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 329/500, 6.5 task/s, elapsed: 51s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 330/500, 6.5 task/s, elapsed: 51s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 331/500, 6.5 task/s, elapsed: 51s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 332/500, 6.5 task/s, elapsed: 51s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 333/500, 6.5 task/s, elapsed: 51s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>          ] 334/500, 6.5 task/s, elapsed: 52s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>          ] 335/500, 6.5 task/s, elapsed: 52s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 336/500, 6.5 task/s, elapsed: 52s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 337/500, 6.5 task/s, elapsed: 52s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 338/500, 6.5 task/s, elapsed: 52s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 339/500, 6.5 task/s, elapsed: 52s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 340/500, 6.5 task/s, elapsed: 52s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 341/500, 6.5 task/s, elapsed: 53s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 342/500, 6.5 task/s, elapsed: 53s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 343/500, 6.5 task/s, elapsed: 53s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 344/500, 6.5 task/s, elapsed: 53s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 345/500, 6.5 task/s, elapsed: 53s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 346/500, 6.5 task/s, elapsed: 53s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 347/500, 6.5 task/s, elapsed: 54s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 348/500, 6.5 task/s, elapsed: 54s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 349/500, 6.5 task/s, elapsed: 54s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>         ] 350/500, 6.5 task/s, elapsed: 54s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>         ] 351/500, 6.5 task/s, elapsed: 54s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>         ] 352/500, 6.5 task/s, elapsed: 54s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>         ] 353/500, 6.5 task/s, elapsed: 54s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>         ] 354/500, 6.5 task/s, elapsed: 55s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>         ] 355/500, 6.5 task/s, elapsed: 55s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 356/500, 6.5 task/s, elapsed: 55s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 357/500, 6.5 task/s, elapsed: 55s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 358/500, 6.5 task/s, elapsed: 55s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 359/500, 6.5 task/s, elapsed: 55s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 360/500, 6.5 task/s, elapsed: 55s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 361/500, 6.5 task/s, elapsed: 56s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 362/500, 6.5 task/s, elapsed: 56s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 363/500, 6.5 task/s, elapsed: 56s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 364/500, 6.5 task/s, elapsed: 56s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 365/500, 6.5 task/s, elapsed: 56s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 366/500, 6.5 task/s, elapsed: 56s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>>        ] 367/500, 6.5 task/s, elapsed: 56s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 368/500, 6.5 task/s, elapsed: 57s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 369/500, 6.5 task/s, elapsed: 57s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 370/500, 6.5 task/s, elapsed: 57s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 371/500, 6.5 task/s, elapsed: 57s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 372/500, 6.5 task/s, elapsed: 57s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 373/500, 6.5 task/s, elapsed: 57s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 374/500, 6.5 task/s, elapsed: 57s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 375/500, 6.5 task/s, elapsed: 58s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 376/500, 6.5 task/s, elapsed: 58s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 377/500, 6.5 task/s, elapsed: 58s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 378/500, 6.5 task/s, elapsed: 58s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 379/500, 6.5 task/s, elapsed: 58s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 380/500, 6.5 task/s, elapsed: 58s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 381/500, 6.5 task/s, elapsed: 58s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 382/500, 6.5 task/s, elapsed: 59s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 383/500, 6.5 task/s, elapsed: 59s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 384/500, 6.5 task/s, elapsed: 59s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 385/500, 6.5 task/s, elapsed: 59s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 386/500, 6.5 task/s, elapsed: 59s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 387/500, 6.5 task/s, elapsed: 59s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 388/500, 6.5 task/s, elapsed: 60s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 389/500, 6.5 task/s, elapsed: 60s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 390/500, 6.5 task/s, elapsed: 60s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 391/500, 6.5 task/s, elapsed: 60s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 392/500, 6.5 task/s, elapsed: 60s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 393/500, 6.5 task/s, elapsed: 60s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 394/500, 6.5 task/s, elapsed: 60s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 395/500, 6.5 task/s, elapsed: 61s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 396/500, 6.5 task/s, elapsed: 61s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 397/500, 6.5 task/s, elapsed: 61s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 398/500, 6.5 task/s, elapsed: 61s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 399/500, 6.5 task/s, elapsed: 61s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 400/500, 6.5 task/s, elapsed: 61s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 401/500, 6.5 task/s, elapsed: 61s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 402/500, 6.5 task/s, elapsed: 62s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 403/500, 6.5 task/s, elapsed: 62s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 404/500, 6.5 task/s, elapsed: 62s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 405/500, 6.5 task/s, elapsed: 62s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 406/500, 6.5 task/s, elapsed: 62s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 407/500, 6.5 task/s, elapsed: 62s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 408/500, 6.5 task/s, elapsed: 62s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 409/500, 6.5 task/s, elapsed: 63s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 410/500, 6.5 task/s, elapsed: 63s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 411/500, 6.5 task/s, elapsed: 63s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 412/500, 6.5 task/s, elapsed: 63s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 413/500, 6.5 task/s, elapsed: 63s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 414/500, 6.5 task/s, elapsed: 63s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 415/500, 6.5 task/s, elapsed: 64s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 416/500, 6.5 task/s, elapsed: 64s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 417/500, 6.5 task/s, elapsed: 64s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 418/500, 6.5 task/s, elapsed: 64s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 419/500, 6.5 task/s, elapsed: 64s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 420/500, 6.5 task/s, elapsed: 64s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 421/500, 6.5 task/s, elapsed: 64s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 422/500, 6.5 task/s, elapsed: 65s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 423/500, 6.5 task/s, elapsed: 65s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 424/500, 6.5 task/s, elapsed: 65s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 425/500, 6.5 task/s, elapsed: 65s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 426/500, 6.5 task/s, elapsed: 65s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 427/500, 6.5 task/s, elapsed: 65s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 428/500, 6.5 task/s, elapsed: 65s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 429/500, 6.5 task/s, elapsed: 66s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 430/500, 6.5 task/s, elapsed: 66s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 431/500, 6.5 task/s, elapsed: 66s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 432/500, 6.5 task/s, elapsed: 66s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 433/500, 6.5 task/s, elapsed: 66s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 434/500, 6.5 task/s, elapsed: 66s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 435/500, 6.5 task/s, elapsed: 66s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 436/500, 6.5 task/s, elapsed: 67s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 437/500, 6.5 task/s, elapsed: 67s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 438/500, 6.5 task/s, elapsed: 67s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 439/500, 6.5 task/s, elapsed: 67s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 440/500, 6.5 task/s, elapsed: 67s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 441/500, 6.5 task/s, elapsed: 67s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 442/500, 6.5 task/s, elapsed: 68s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 443/500, 6.5 task/s, elapsed: 68s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 444/500, 6.5 task/s, elapsed: 68s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 445/500, 6.5 task/s, elapsed: 68s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 446/500, 6.5 task/s, elapsed: 68s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 447/500, 6.5 task/s, elapsed: 68s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 448/500, 6.5 task/s, elapsed: 68s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 449/500, 6.5 task/s, elapsed: 69s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 450/500, 6.5 task/s, elapsed: 69s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 451/500, 6.5 task/s, elapsed: 69s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 452/500, 6.5 task/s, elapsed: 69s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 453/500, 6.5 task/s, elapsed: 69s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 454/500, 6.5 task/s, elapsed: 69s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 455/500, 6.5 task/s, elapsed: 69s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 456/500, 6.5 task/s, elapsed: 70s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 457/500, 6.5 task/s, elapsed: 70s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 458/500, 6.5 task/s, elapsed: 70s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 459/500, 6.5 task/s, elapsed: 70s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 460/500, 6.5 task/s, elapsed: 70s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 461/500, 6.5 task/s, elapsed: 70s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 462/500, 6.5 task/s, elapsed: 71s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 463/500, 6.5 task/s, elapsed: 71s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 464/500, 6.5 task/s, elapsed: 71s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 465/500, 6.5 task/s, elapsed: 71s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 466/500, 6.5 task/s, elapsed: 71s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 467/500, 6.5 task/s, elapsed: 71s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 468/500, 6.6 task/s, elapsed: 71s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 469/500, 6.6 task/s, elapsed: 72s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 470/500, 6.6 task/s, elapsed: 72s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 471/500, 6.6 task/s, elapsed: 72s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 472/500, 6.6 task/s, elapsed: 72s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 473/500, 6.6 task/s, elapsed: 72s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 474/500, 6.6 task/s, elapsed: 72s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 475/500, 6.6 task/s, elapsed: 73s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 476/500, 6.6 task/s, elapsed: 73s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 477/500, 6.6 task/s, elapsed: 73s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 478/500, 6.6 task/s, elapsed: 73s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 479/500, 6.6 task/s, elapsed: 73s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 480/500, 6.5 task/s, elapsed: 73s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 481/500, 6.5 task/s, elapsed: 73s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 482/500, 6.5 task/s, elapsed: 74s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 483/500, 6.6 task/s, elapsed: 74s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 484/500, 6.6 task/s, elapsed: 74s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 485/500, 6.6 task/s, elapsed: 74s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 486/500, 6.6 task/s, elapsed: 74s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 487/500, 6.6 task/s, elapsed: 74s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 488/500, 6.6 task/s, elapsed: 74s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 489/500, 6.6 task/s, elapsed: 75s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 490/500, 6.6 task/s, elapsed: 75s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 491/500, 6.6 task/s, elapsed: 75s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 492/500, 6.6 task/s, elapsed: 75s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 493/500, 6.6 task/s, elapsed: 75s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 494/500, 6.6 task/s, elapsed: 75s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 495/500, 6.6 task/s, elapsed: 76s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 496/500, 6.6 task/s, elapsed: 76s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 497/500, 6.6 task/s, elapsed: 76s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 498/500, 6.6 task/s, elapsed: 76s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 499/500, 6.6 task/s, elapsed: 76s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 500/500, 6.6 task/s, elapsed: 76s, ETA:     0s/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:45:44,096 - mmseg - INFO - Iter [16300/40000]	lr: 3.555e-05, eta: 22:30:44, time: 3.348, data_time: 0.019, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.4226, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 63.6631
3 | 88.65 |
|     rider     | 47.15 | 68.53 |
|      car      | 86.71 | 94.08 |
|     truck     |  0.0  |  0.0  |
|      bus      | 61.51 | 83.74 |
|     train     |  0.0  |  0.0  |
|   motorcycle  | 56.11 | 69.74 |
|    bicycle    | 61.78 | 70.63 |
+---------------+-------+-------+
2022-04-19 04:45:18,790 - mmseg - INFO - Summary:
2022-04-19 04:45:18,790 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 86.47 | 51.04 | 61.41 |
+-------+-------+-------+
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:45:18,802 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-19 04:45:18,802 - mmseg - INFO - Iter [500/40000]	lr: 6.002e-06, eta: 1:42:51, time: 1.477, data_time: 0.013, memory: 9636, aAcc: 0.8647, mIoU: 0.5104, mAcc: 0.6141, IoU.road: 0.8096, IoU.sidewalk: 0.3493, IoU.building: 0.8807, IoU.wall: 0.3907, IoU.fence: 0.0874, IoU.pole: 0.4960, IoU.traffic light: 0.5533, IoU.traffic sign: 0.5554, IoU.vegetation: 0.8573, IoU.terrain: 0.0000, IoU.sky: 0.8629, IoU.person: 0.7223, IoU.rider: 0.4715, IoU.car: 0.8671, IoU.truck: 0.0000, IoU.bus: 0.6151, IoU.train: 0.0000, IoU.motorcycle: 0.5611, IoU.bicycle: 0.6178, Acc.road: 0.8462, Acc.sidewalk: 0.7394, Acc.building: 0.9506, Acc.wall: 0.4635, Acc.fence: 0.0999, Acc.pole: 0.5734, Acc.traffic light: 0.6580, Acc.traffic sign: 0.6511, Acc.vegetation: 0.9416, Acc.terrain: 0.0000, Acc.sky: 0.9909, Acc.person: 0.8865, Acc.rider: 0.6853, Acc.car: 0.9408, Acc.truck: 0.0000, Acc.bus: 0.8374, Acc.train: 0.0000, Acc.motorcycle: 0.6974, Acc.bicycle: 0.7063, decode.loss_seg: 0.1145, decode.acc_seg: 91.2749, src.loss_imnet_feat_dist: 0.0996, mix.decode.loss_seg: 0.1419, mix.decode.acc_seg: 89.6515
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:46:32,856 - mmseg - INFO - Iter [36050/40000]	lr: 5.926e-06, eta: 1:41:47, time: 3.897, data_time: 2.431, memory: 9636, decode.loss_seg: 0.1037, decode.acc_seg: 91.3678, src.loss_imnet_feat_dist: 0.0973, mix.decode.loss_seg: 0.1212, mix.decode.acc_seg: 89.1659
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:47:48,472 - mmseg - INFO - Iter [36100/40000]	lr: 5.851e-06, eta: 1:40:29, time: 1.512, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1019, decode.acc_seg: 91.1405, src.loss_imnet_feat_dist: 0.0909, mix.decode.loss_seg: 0.1203, mix.decode.acc_seg: 89.0474
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:49:06,632 - mmseg - INFO - Iter [36150/40000]	lr: 5.777e-06, eta: 1:39:12, time: 1.563, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1050, decode.acc_seg: 91.8179, src.loss_imnet_feat_dist: 0.0995, mix.decode.loss_seg: 0.1159, mix.decode.acc_seg: 90.1346
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:50:20,249 - mmseg - INFO - Iter [36200/40000]	lr: 5.702e-06, eta: 1:37:55, time: 1.472, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1066, decode.acc_seg: 90.8742, src.loss_imnet_feat_dist: 0.0916, mix.decode.loss_seg: 0.1174, mix.decode.acc_seg: 89.8794
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:51:37,587 - mmseg - INFO - Iter [36250/40000]	lr: 5.627e-06, eta: 1:36:37, time: 1.547, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1024, decode.acc_seg: 90.8847, src.loss_imnet_feat_dist: 0.0968, mix.decode.loss_seg: 0.1074, mix.decode.acc_seg: 90.1953
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:52:55,827 - mmseg - INFO - Iter [36300/40000]	lr: 5.551e-06, eta: 1:35:20, time: 1.565, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1016, decode.acc_seg: 90.9609, src.loss_imnet_feat_dist: 0.0925, mix.decode.loss_seg: 0.1173, mix.decode.acc_seg: 88.4680
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:54:15,115 - mmseg - INFO - Iter [36350/40000]	lr: 5.476e-06, eta: 1:34:03, time: 1.586, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1114, decode.acc_seg: 90.4680, src.loss_imnet_feat_dist: 0.0971, mix.decode.loss_seg: 0.1193, mix.decode.acc_seg: 89.2006
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:55:32,210 - mmseg - INFO - Iter [36400/40000]	lr: 5.402e-06, eta: 1:32:46, time: 1.542, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1066, decode.acc_seg: 90.7081, src.loss_imnet_feat_dist: 0.0991, mix.decode.loss_seg: 0.1227, mix.decode.acc_seg: 89.0468
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:56:45,263 - mmseg - INFO - Iter [36450/40000]	lr: 5.327e-06, eta: 1:31:28, time: 1.461, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1043, decode.acc_seg: 91.1652, src.loss_imnet_feat_dist: 0.0966, mix.decode.loss_seg: 0.1134, mix.decode.acc_seg: 89.3180
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:58:01,794 - mmseg - INFO - Iter [36500/40000]	lr: 5.251e-06, eta: 1:30:11, time: 1.531, data_time: 0.014, memory: 9636, decode.loss_seg: 0.0922, decode.acc_seg: 91.2232, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1156, mix.decode.acc_seg: 89.1315
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 04:59:20,822 - mmseg - INFO - Iter [36550/40000]	lr: 5.176e-06, eta: 1:28:53, time: 1.581, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1019, decode.acc_seg: 91.8652, src.loss_imnet_feat_dist: 0.0923, mix.decode.loss_seg: 0.1049, mix.decode.acc_seg: 89.4690
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:00:31,358 - mmseg - INFO - Iter [36600/40000]	lr: 5.102e-06, eta: 1:27:35, time: 1.411, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1009, decode.acc_seg: 91.2875, src.loss_imnet_feat_dist: 0.0932, mix.decode.loss_seg: 0.1121, mix.decode.acc_seg: 89.5984
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:01:45,814 - mmseg - INFO - Iter [36650/40000]	lr: 5.027e-06, eta: 1:26:18, time: 1.489, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1020, decode.acc_seg: 91.3160, src.loss_imnet_feat_dist: 0.0969, mix.decode.loss_seg: 0.1223, mix.decode.acc_seg: 88.8976
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:02:56,978 - mmseg - INFO - Iter [36700/40000]	lr: 4.951e-06, eta: 1:25:00, time: 1.423, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1051, decode.acc_seg: 91.2905, src.loss_imnet_feat_dist: 0.0992, mix.decode.loss_seg: 0.1184, mix.decode.acc_seg: 88.1321
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:04:12,474 - mmseg - INFO - Iter [36750/40000]	lr: 4.876e-06, eta: 1:23:43, time: 1.510, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1034, decode.acc_seg: 90.6223, src.loss_imnet_feat_dist: 0.0973, mix.decode.loss_seg: 0.1203, mix.decode.acc_seg: 88.8965
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:05:30,308 - mmseg - INFO - Iter [36800/40000]	lr: 4.802e-06, eta: 1:22:25, time: 1.557, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1014, decode.acc_seg: 90.8458, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1146, mix.decode.acc_seg: 88.8600
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:06:50,149 - mmseg - INFO - Iter [36850/40000]	lr: 4.727e-06, eta: 1:21:08, time: 1.597, data_time: 0.014, memory: 9636, decode.loss_seg: 0.0964, decode.acc_seg: 91.6473, src.loss_imnet_feat_dist: 0.0939, mix.decode.loss_seg: 0.1141, mix.decode.acc_seg: 89.4150
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:08:07,768 - mmseg - INFO - Iter [36900/40000]	lr: 4.651e-06, eta: 1:19:51, time: 1.552, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1132, decode.acc_seg: 90.5787, src.loss_imnet_feat_dist: 0.0943, mix.decode.loss_seg: 0.1136, mix.decode.acc_seg: 89.1583
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:09:27,173 - mmseg - INFO - Iter [36950/40000]	lr: 4.576e-06, eta: 1:18:34, time: 1.588, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1020, decode.acc_seg: 91.0670, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1197, mix.decode.acc_seg: 89.4493
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:10:45,026 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-19 05:10:45,026 - mmseg - INFO - Iter [37000/40000]	lr: 4.502e-06, eta: 1:17:17, time: 1.557, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1000, decode.acc_seg: 91.3589, src.loss_imnet_feat_dist: 0.0940, mix.decode.loss_seg: 0.1062, mix.decode.acc_seg: 89.4924
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:11:59,485 - mmseg - INFO - Iter [37050/40000]	lr: 4.427e-06, eta: 1:15:59, time: 1.489, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1044, decode.acc_seg: 91.6742, src.loss_imnet_feat_dist: 0.0941, mix.decode.loss_seg: 0.1267, mix.decode.acc_seg: 89.1068
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:13:11,747 - mmseg - INFO - Iter [37100/40000]	lr: 4.351e-06, eta: 1:14:42, time: 1.445, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1001, decode.acc_seg: 91.6439, src.loss_imnet_feat_dist: 0.0935, mix.decode.loss_seg: 0.1197, mix.decode.acc_seg: 89.1741
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:14:26,331 - mmseg - INFO - Iter [37150/40000]	lr: 4.276e-06, eta: 1:13:24, time: 1.492, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0949, decode.acc_seg: 90.7458, src.loss_imnet_feat_dist: 0.0947, mix.decode.loss_seg: 0.1252, mix.decode.acc_seg: 88.6739
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:15:37,067 - mmseg - INFO - Iter [37200/40000]	lr: 4.202e-06, eta: 1:12:06, time: 1.415, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1133, decode.acc_seg: 90.6375, src.loss_imnet_feat_dist: 0.0973, mix.decode.loss_seg: 0.1517, mix.decode.acc_seg: 87.9343
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:16:49,167 - mmseg - INFO - Iter [37250/40000]	lr: 4.127e-06, eta: 1:10:49, time: 1.442, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1026, decode.acc_seg: 91.0778, src.loss_imnet_feat_dist: 0.0962, mix.decode.loss_seg: 0.1262, mix.decode.acc_seg: 88.3465
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:18:01,082 - mmseg - INFO - Iter [37300/40000]	lr: 4.051e-06, eta: 1:09:31, time: 1.438, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1048, decode.acc_seg: 90.7827, src.loss_imnet_feat_dist: 0.0960, mix.decode.loss_seg: 0.1188, mix.decode.acc_seg: 88.3094
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:19:17,919 - mmseg - INFO - Iter [37350/40000]	lr: 3.976e-06, eta: 1:08:14, time: 1.537, data_time: 0.014, memory: 9636, decode.loss_seg: 0.0913, decode.acc_seg: 91.5403, src.loss_imnet_feat_dist: 0.0931, mix.decode.loss_seg: 0.1070, mix.decode.acc_seg: 89.5964
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:20:32,891 - mmseg - INFO - Iter [37400/40000]	lr: 3.901e-06, eta: 1:06:56, time: 1.499, data_time: 0.014, memory: 9636, decode.loss_seg: 0.0954, decode.acc_seg: 91.9140, src.loss_imnet_feat_dist: 0.0960, mix.decode.loss_seg: 0.1081, mix.decode.acc_seg: 90.2339
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:21:51,972 - mmseg - INFO - Iter [37450/40000]	lr: 3.827e-06, eta: 1:05:39, time: 1.582, data_time: 0.014, memory: 9636, decode.loss_seg: 0.0919, decode.acc_seg: 91.2084, src.loss_imnet_feat_dist: 0.0921, mix.decode.loss_seg: 0.1106, mix.decode.acc_seg: 89.7199
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:23:02,356 - mmseg - INFO - Iter [37500/40000]	lr: 3.752e-06, eta: 1:04:21, time: 1.408, data_time: 0.014, memory: 9636, decode.loss_seg: 0.1072, decode.acc_seg: 91.3202, src.loss_imnet_feat_dist: 0.0960, mix.decode.loss_seg: 0.1212, mix.decode.acc_seg: 89.1323
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:24:14,166 - mmseg - INFO - Iter [37550/40000]	lr: 3.676e-06, eta: 1:03:04, time: 1.436, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1147, decode.acc_seg: 90.2770, src.loss_imnet_feat_dist: 0.0931, mix.decode.loss_seg: 0.1186, mix.decode.acc_seg: 88.6833
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:25:30,593 - mmseg - INFO - Iter [37600/40000]	lr: 3.601e-06, eta: 1:01:47, time: 1.529, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1049, decode.acc_seg: 91.8212, src.loss_imnet_feat_dist: 0.0967, mix.decode.loss_seg: 0.1097, mix.decode.acc_seg: 89.7182
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:26:49,637 - mmseg - INFO - Iter [37650/40000]	lr: 3.527e-06, eta: 1:00:30, time: 1.581, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0952, decode.acc_seg: 91.7615, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1066, mix.decode.acc_seg: 90.4777
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:28:05,809 - mmseg - INFO - Iter [37700/40000]	lr: 3.452e-06, eta: 0:59:12, time: 1.523, data_time: 0.014, memory: 9636, decode.loss_seg: 0.0935, decode.acc_seg: 91.0619, src.loss_imnet_feat_dist: 0.0922, mix.decode.loss_seg: 0.1059, mix.decode.acc_seg: 88.9468
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:29:23,620 - mmseg - INFO - Iter [37750/40000]	lr: 3.376e-06, eta: 0:57:55, time: 1.556, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1087, decode.acc_seg: 89.8750, src.loss_imnet_feat_dist: 0.0960, mix.decode.loss_seg: 0.1203, mix.decode.acc_seg: 88.2221
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:30:35,764 - mmseg - INFO - Iter [37800/40000]	lr: 3.301e-06, eta: 0:56:37, time: 1.443, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1050, decode.acc_seg: 90.8371, src.loss_imnet_feat_dist: 0.0913, mix.decode.loss_seg: 0.1157, mix.decode.acc_seg: 90.0210
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:31:51,722 - mmseg - INFO - Iter [37850/40000]	lr: 3.227e-06, eta: 0:55:20, time: 1.519, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0966, decode.acc_seg: 91.3474, src.loss_imnet_feat_dist: 0.0929, mix.decode.loss_seg: 0.1110, mix.decode.acc_seg: 89.5889
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:33:07,843 - mmseg - INFO - Iter [37900/40000]	lr: 3.152e-06, eta: 0:54:03, time: 1.522, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1030, decode.acc_seg: 90.9559, src.loss_imnet_feat_dist: 0.0939, mix.decode.loss_seg: 0.1110, mix.decode.acc_seg: 89.4635
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:34:23,832 - mmseg - INFO - Iter [37950/40000]	lr: 3.076e-06, eta: 0:52:46, time: 1.520, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1028, decode.acc_seg: 91.5183, src.loss_imnet_feat_dist: 0.0982, mix.decode.loss_seg: 0.1158, mix.decode.acc_seg: 89.6366
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:35:41,344 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-19 05:35:41,344 - mmseg - INFO - Iter [38000/40000]	lr: 3.001e-06, eta: 0:51:28, time: 1.550, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1011, decode.acc_seg: 91.1353, src.loss_imnet_feat_dist: 0.0917, mix.decode.loss_seg: 0.1200, mix.decode.acc_seg: 89.5200
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:36:55,171 - mmseg - INFO - Iter [38050/40000]	lr: 2.927e-06, eta: 0:50:11, time: 1.477, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1068, decode.acc_seg: 91.6395, src.loss_imnet_feat_dist: 0.0916, mix.decode.loss_seg: 0.1232, mix.decode. 53.9678
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:39:15,100 - mmseg - INFO - Iter [17250/40000]	lr: 3.413e-05, eta: 21:35:46, time: 3.402, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.2128, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.5239
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:42:01,592 - mmseg - INFO - Iter [17300/40000]	lr: 3.405e-05, eta: 21:32:49, time: 3.330, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.4845, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.9410
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:44:51,082 - mmseg - INFO - Iter [17350/40000]	lr: 3.398e-05, eta: 21:29:57, time: 3.390, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 24.0697, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.8622
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:47:38,542 - mmseg - INFO - Iter [17400/40000]	lr: 3.390e-05, eta: 21:27:01, time: 3.349, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.0833, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.8448
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:50:28,358 - mmseg - INFO - Iter [17450/40000]	lr: 3.383e-05, eta: 21:24:09, time: 3.396, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.8422, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.5234
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:50:54,598 - mmseg - INFO - Iter [38600/40000]	lr: 2.101e-06, eta: 0:36:01, time: 1.537, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0956, decode.acc_seg: 91.3206, src.loss_imnet_feat_dist: 0.0961, mix.decode.loss_seg: 0.1084, mix.decode.acc_seg: 90.0410
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:52:08,597 - mmseg - INFO - Iter [38650/40000]	lr: 2.026e-06, eta: 0:34:44, time: 1.480, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1005, decode.acc_seg: 91.1627, src.loss_imnet_feat_dist: 0.0920, mix.decode.loss_seg: 0.1154, mix.decode.acc_seg: 89.3651
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:53:25,000 - mmseg - INFO - Iter [38700/40000]	lr: 1.952e-06, eta: 0:33:27, time: 1.528, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1054, decode.acc_seg: 90.7779, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: 0.1188, mix.decode.acc_seg: 89.4430
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:54:42,469 - mmseg - INFO - Iter [38750/40000]	lr: 1.877e-06, eta: 0:32:10, time: 1.549, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1018, decode.acc_seg: 91.0303, src.loss_imnet_feat_dist: 0.0904, mix.decode.loss_seg: 0.1174, mix.decode.acc_seg: 89.0964
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:55:58,957 - mmseg - INFO - Iter [38800/40000]	lr: 1.801e-06, eta: 0:30:52, time: 1.530, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0930, decode.acc_seg: 91.1554, src.loss_imnet_feat_dist: 0.0923, mix.decode.loss_seg: 0.1039, mix.decode.acc_seg: 88.7585
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:57:18,099 - mmseg - INFO - Iter [38850/40000]	lr: 1.726e-06, eta: 0:29:35, time: 1.583, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0998, decode.acc_seg: 90.1167, src.loss_imnet_feat_dist: 0.0906, mix.decode.loss_seg: 0.1146, mix.decode.acc_seg: 88.9350
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:58:35,553 - mmseg - INFO - Iter [38900/40000]	lr: 1.652e-06, eta: 0:28:18, time: 1.549, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1015, decode.acc_seg: 91.2208, src.loss_imnet_feat_dist: 0.1003, mix.decode.loss_seg: 0.1160, mix.decode.acc_seg: 88.4874
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 05:58:52,890 - mmseg - INFO - Iter [17600/40000]	lr: 3.360e-05, eta: 21:15:27, time: 3.345, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.4388, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.7787
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:01:42,879 - mmseg - INFO - Iter [17650/40000]	lr: 3.353e-05, eta: 21:12:35, time: 3.400, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.3714, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.8741
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:04:30,992 - mmseg - INFO - Iter [17700/40000]	lr: 3.345e-05, eta: 21:09:40, time: 3.362, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.5849, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.2867
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:07:23,147 - mmseg - INFO - Iter [17750/40000]	lr: 3.338e-05, eta: 21:06:51, time: 3.443, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.3613, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.5861
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:10:11,939 - mmseg - INFO - Iter [17800/40000]	lr: 3.330e-05, eta: 21:03:58, time: 3.376, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.8577, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.1271
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:13:01,112 - mmseg - INFO - Iter [17850/40000]	lr: 3.323e-05, eta: 21:01:05, time: 3.383, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.9549, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.6094
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:16:13,100 - mmseg - INFO - Iter [39600/40000]	lr: 6.015e-07, eta: 0:10:17, time: 1.548, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1093, decode.acc_seg: 91.0702, src.loss_imnet_feat_dist: 0.0956, mix.decode.loss_seg: 0.1104, mix.decode.acc_seg: 89.9432
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:17:27,210 - mmseg - INFO - Iter [39650/40000]	lr: 5.265e-07, eta: 0:09:00, time: 1.482, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0928, decode.acc_seg: 91.6419, src.loss_imnet_feat_dist: 0.0948, mix.decode.loss_seg: 0.1159, mix.decode.acc_seg: 89.8385
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:18:37,529 - mmseg - INFO - Iter [39700/40000]	lr: 4.515e-07, eta: 0:07:42, time: 1.406, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0964, decode.acc_seg: 91.0626, src.loss_imnet_feat_dist: 0.0938, mix.decode.loss_seg: 0.1119, mix.decode.acc_seg: 89.4526
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:19:54,411 - mmseg - INFO - Iter [39750/40000]	lr: 3.765e-07, eta: 0:06:25, time: 1.538, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1005, decode.acc_seg: 92.1291, src.loss_imnet_feat_dist: 0.0945, mix.decode.loss_seg: 0.1154, mix.decode.acc_seg: 89.8978
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:21:07,894 - mmseg - INFO - Iter [39800/40000]	lr: 3.015e-07, eta: 0:05:08, time: 1.470, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1003, decode.acc_seg: 90.4824, src.loss_imnet_feat_dist: 0.0936, mix.decode.loss_seg: 0.1157, mix.decode.acc_seg: 89.1948
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       /itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:22:19,696 - mmseg - INFO - Iter [39850/40000]	lr: 2.265e-07, eta: 0:03:51, time: 1.436, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1035, decode.acc_seg: 91.1853, src.loss_imnet_feat_dist: 0.0943, mix.decode.loss_seg: 0.1101, mix.decode.acc_seg: 89.4033
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:23:29,580 - mmseg - INFO - Iter [39900/40000]	lr: 1.515e-07, eta: 0:02:34, time: 1.398, data_time: 0.013, memory: 9636, decode.loss_seg: 0.1030, decode.acc_seg: 90.6625, src.loss_imnet_feat_dist: 0.0980, mix.decode.loss_seg: 0.1076, mix.decode.acc_seg: 88.8762
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:24:41,311 - mmseg - INFO - Iter [39950/40000]	lr: 7.650e-08, eta: 0:01:17, time: 1.435, data_time: 0.013, memory: 9636, decode.loss_seg: 0.0972, decode.acc_seg: 91.5863, src.loss_imnet_feat_dist: 0.0974, mix.decode.loss_seg: 0.1188, mix.decode.acc_seg: 89.1531
[                                                  ] 0/500, elapsed: 0s, ETA:
[                                 ] 1/500, 1.4 task/s, elapsed: 1s, ETA:   352s
[                                 ] 2/500, 2.3 task/s, elapsed: 1s, ETA:   217s
[                                 ] 3/500, 3.0 task/s, elapsed: 1s, ETA:   167s
[                                 ] 4/500, 3.5 task/s, elapsed: 1s, ETA:   142s
[                                 ] 5/500, 3.9 task/s, elapsed: 1s, ETA:   127s
[                                 ] 6/500, 4.2 task/s, elapsed: 1s, ETA:   117s
[                                 ] 7/500, 4.5 task/s, elapsed: 2s, ETA:   110s
[                                 ] 8/500, 4.7 task/s, elapsed: 2s, ETA:   105s
[                                 ] 9/500, 4.9 task/s, elapsed: 2s, ETA:   101s
[                                ] 10/500, 5.0 task/s, elapsed: 2s, ETA:    97s
[                                ] 11/500, 5.2 task/s, elapsed: 2s, ETA:    95s
[                                ] 12/500, 5.3 task/s, elapsed: 2s, ETA:    92s
[                                ] 13/500, 5.4 task/s, elapsed: 2s, ETA:    91s
[                                ] 14/500, 5.5 task/s, elapsed: 3s, ETA:    89s
[                                ] 15/500, 5.5 task/s, elapsed: 3s, ETA:    87s
[>                               ] 16/500, 5.6 task/s, elapsed: 3s, ETA:    86s
[>                               ] 17/500, 5.7 task/s, elapsed: 3s, ETA:    85s
[>                               ] 18/500, 5.7 task/s, elapsed: 3s, ETA:    84s
[>                               ] 19/500, 5.8 task/s, elapsed: 3s, ETA:    83s
[>                               ] 20/500, 5.8 task/s, elapsed: 3s, ETA:    82s
[>                               ] 21/500, 5.9 task/s, elapsed: 4s, ETA:    81s
[>                               ] 22/500, 5.9 task/s, elapsed: 4s, ETA:    81s
[>                               ] 23/500, 6.0 task/s, elapsed: 4s, ETA:    80s
[>                               ] 24/500, 6.0 task/s, elapsed: 4s, ETA:    79s
[>                               ] 25/500, 6.0 task/s, elapsed: 4s, ETA:    79s
[>                               ] 26/500, 6.1 task/s, elapsed: 4s, ETA:    78s
[>                               ] 27/500, 6.1 task/s, elapsed: 4s, ETA:    78s
[>                               ] 28/500, 6.1 task/s, elapsed: 5s, ETA:    77s
[>                               ] 29/500, 6.1 task/s, elapsed: 5s, ETA:    77s
[>                               ] 30/500, 6.2 task/s, elapsed: 5s, ETA:    76s
[>                               ] 31/500, 6.2 task/s, elapsed: 5s, ETA:    76s
[>>                              ] 32/500, 6.2 task/s, elapsed: 5s, ETA:    76s
[>>                              ] 33/500, 6.2 task/s, elapsed: 5s, ETA:    75s
[>>                              ] 34/500, 6.2 task/s, elapsed: 5s, ETA:    75s
[>>                              ] 35/500, 6.2 task/s, elapsed: 6s, ETA:    74s
[>>                              ] 36/500, 6.3 task/s, elapsed: 6s, ETA:    74s
[>>                              ] 37/500, 6.3 task/s, elapsed: 6s, ETA:    74s
[>>                              ] 38/500, 6.3 task/s, elapsed: 6s, ETA:    74s
[>>                              ] 39/500, 6.3 task/s, elapsed: 6s, ETA:    73s
[>>                              ] 40/500, 6.3 task/s, elapsed: 6s, ETA:    73s
[>>                              ] 41/500, 6.3 task/s, elapsed: 6s, ETA:    73s
[>>                              ] 42/500, 6.3 task/s, elapsed: 7s, ETA:    72s
[>>                              ] 43/500, 6.3 task/s, elapsed: 7s, ETA:    72s
[>>                              ] 44/500, 6.3 task/s, elapsed: 7s, ETA:    72s
[>>                              ] 45/500, 6.4 task/s, elapsed: 7s, ETA:    72s
[>>                              ] 46/500, 6.4 task/s, elapsed: 7s, ETA:    71s
[>>>                             ] 47/500, 6.4 task/s, elapsed: 7s, ETA:    71s
[>>>                             ] 48/500, 6.4 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 49/500, 6.4 task/s, elapsed: 8s, ETA:    71s
[>>>                             ] 50/500, 6.4 task/s, elapsed: 8s, ETA:    70s
[>>>                             ] 51/500, 6.4 task/s, elapsed: 8s, ETA:    70s
[>>>                             ] 52/500, 6.4 task/s, elapsed: 8s, ETA:    70s
[>>>                             ] 53/500, 6.4 task/s, elapsed: 8s, ETA:    70s
[>>>                             ] 54/500, 6.4 task/s, elapsed: 8s, ETA:    70s
[>>>                             ] 55/500, 6.4 task/s, elapsed: 9s, ETA:    69s
[>>>                             ] 56/500, 6.4 task/s, elapsed: 9s, ETA:    69s
[>>>                             ] 57/500, 6.4 task/s, elapsed: 9s, ETA:    69s
[>>>                             ] 58/500, 6.4 task/s, elapsed: 9s, ETA:    69s
[>>>                             ] 59/500, 6.4 task/s, elapsed: 9s, ETA:    68s
[>>>                             ] 60/500, 6.5 task/s, elapsed: 9s, ETA:    68s
[>>>                             ] 61/500, 6.5 task/s, elapsed: 9s, ETA:    68s
[>>>                            ] 62/500, 6.5 task/s, elapsed: 10s, ETA:    68s
[>>>                            ] 63/500, 6.5 task/s, elapsed: 10s, ETA:    68s
[>>>                            ] 64/500, 6.5 task/s, elapsed: 10s, ETA:    67s
[>>>>                           ] 65/500, 6.5 task/s, elapsed: 10s, ETA:    67s
[>>>>                           ] 66/500, 6.5 task/s, elapsed: 10s, ETA:    67s
[>>>>                           ] 67/500, 6.5 task/s, elapsed: 10s, ETA:    67s
[>>>>                           ] 68/500, 6.5 task/s, elapsed: 10s, ETA:    67s
[>>>>                           ] 69/500, 6.5 task/s, elapsed: 11s, ETA:    66s
[>>>>                           ] 70/500, 6.5 task/s, elapsed: 11s, ETA:    66s
[>>>>                           ] 71/500, 6.5 task/s, elapsed: 11s, ETA:    66s
[>>>>                           ] 72/500, 6.5 task/s, elapsed: 11s, ETA:    66s
[>>>>                           ] 73/500, 6.5 task/s, elapsed: 11s, ETA:    66s
[>>>>                           ] 74/500, 6.5 task/s, elapsed: 11s, ETA:    65s
[>>>>                           ] 75/500, 6.5 task/s, elapsed: 12s, ETA:    65s
[>>>>                           ] 76/500, 6.5 task/s, elapsed: 12s, ETA:    65s
[>>>>                           ] 77/500, 6.5 task/s, elapsed: 12s, ETA:    65s
[>>>>                           ] 78/500, 6.5 task/s, elapsed: 12s, ETA:    65s
[>>>>                           ] 79/500, 6.5 task/s, elapsed: 12s, ETA:    64s
[>>>>                           ] 80/500, 6.5 task/s, elapsed: 12s, ETA:    64s
[>>>>>                          ] 81/500, 6.5 task/s, elapsed: 12s, ETA:    64s
[>>>>>                          ] 82/500, 6.5 task/s, elapsed: 13s, ETA:    64s
[>>>>>                          ] 83/500, 6.5 task/s, elapsed: 13s, ETA:    64s
[>>>>>                          ] 84/500, 6.5 task/s, elapsed: 13s, ETA:    64s
[>>>>>                          ] 85/500, 6.5 task/s, elapsed: 13s, ETA:    63s
[>>>>>                          ] 86/500, 6.6 task/s, elapsed: 13s, ETA:    63s
[>>>>>                          ] 87/500, 6.6 task/s, elapsed: 13s, ETA:    63s
[>>>>>                          ] 88/500, 6.6 task/s, elapsed: 13s, ETA:    63s
[>>>>>                          ] 89/500, 6.6 task/s, elapsed: 14s, ETA:    63s
[>>>>>                          ] 90/500, 6.6 task/s, elapsed: 14s, ETA:    62s
[>>>>>                          ] 91/500, 6.6 task/s, elapsed: 14s, ETA:    62s
[>>>>>                          ] 92/500, 6.6 task/s, elapsed: 14s, ETA:    62s
[>>>>>                          ] 93/500, 6.6 task/s, elapsed: 14s, ETA:    62s
[>>>>>                          ] 94/500, 6.6 task/s, elapsed: 14s, ETA:    62s
[>>>>>                          ] 95/500, 6.6 task/s, elapsed: 14s, ETA:    62s
[>>>>>                          ] 96/500, 6.6 task/s, elapsed: 15s, ETA:    61s
[>>>>>>                         ] 97/500, 6.6 task/s, elapsed: 15s, ETA:    61s
[>>>>>>                         ] 98/500, 6.6 task/s, elapsed: 15s, ETA:    61s
[>>>>>>                         ] 99/500, 6.6 task/s, elapsed: 15s, ETA:    61s
[>>>>>>                        ] 100/500, 6.6 task/s, elapsed: 15s, ETA:    61s
[>>>>>>                        ] 101/500, 6.6 task/s, elapsed: 15s, ETA:    60s
[>>>>>>                        ] 102/500, 6.6 task/s, elapsed: 15s, ETA:    60s
[>>>>>>                        ] 103/500, 6.6 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 104/500, 6.6 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 105/500, 6.6 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 106/500, 6.6 task/s, elapsed: 16s, ETA:    60s
[>>>>>>                        ] 107/500, 6.6 task/s, elapsed: 16s, ETA:    59s
[>>>>>>                        ] 108/500, 6.6 task/s, elapsed: 16s, ETA:    59s
[>>>>>>                        ] 109/500, 6.6 task/s, elapsed: 16s, ETA:    59s
[>>>>>>                        ] 110/500, 6.6 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 111/500, 6.6 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 112/500, 6.6 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 113/500, 6.6 task/s, elapsed: 17s, ETA:    59s
[>>>>>>                        ] 114/500, 6.6 task/s, elapsed: 17s, ETA:    58s
[>>>>>>                        ] 115/500, 6.6 task/s, elapsed: 17s, ETA:    58s
[>>>>>>                        ] 116/500, 6.6 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 117/500, 6.6 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 118/500, 6.6 task/s, elapsed: 18s, ETA:    58s
[>>>>>>>                       ] 119/500, 6.6 task/s, elapsed: 18s, ETA:    57s
[>>>>>>>                       ] 120/500, 6.6 task/s, elapsed: 18s, ETA:    57s
[>>>>>>>                       ] 121/500, 6.6 task/s, elapsed: 18s, ETA:    57s
[>>>>>>>                       ] 122/500, 6.6 task/s, elapsed: 18s, ETA:    57s
[>>>>>>>                       ] 123/500, 6.6 task/s, elapsed: 19s, ETA:    57s
[>>>>>>>                       ] 124/500, 6.6 task/s, elapsed: 19s, ETA:    57s
[>>>>>>>                       ] 125/500, 6.6 task/s, elapsed: 19s, ETA:    56s
[>>>>>>>                       ] 126/500, 6.6 task/s, elapsed: 19s, ETA:    56s
[>>>>>>>                       ] 127/500, 6.6 task/s, elapsed: 19s, ETA:    56s
[>>>>>>>                       ] 128/500, 6.6 task/s, elapsed: 19s, ETA:    56s
[>>>>>>>                       ] 129/500, 6.6 task/s, elapsed: 19s, ETA:    56s
[>>>>>>>                       ] 130/500, 6.7 task/s, elapsed: 20s, ETA:    56s
[>>>>>>>                       ] 131/500, 6.7 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>                       ] 132/500, 6.7 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>                       ] 133/500, 6.7 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>>                      ] 134/500, 6.7 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>>                      ] 135/500, 6.7 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>>                      ] 136/500, 6.7 task/s, elapsed: 20s, ETA:    55s
[>>>>>>>>                      ] 137/500, 6.7 task/s, elapsed: 21s, ETA:    55s
[>>>>>>>>                      ] 138/500, 6.7 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 139/500, 6.7 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 140/500, 6.7 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 141/500, 6.7 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 142/500, 6.7 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 143/500, 6.7 task/s, elapsed: 21s, ETA:    54s
[>>>>>>>>                      ] 144/500, 6.7 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 145/500, 6.7 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 146/500, 6.7 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 147/500, 6.7 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 148/500, 6.7 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>                      ] 149/500, 6.7 task/s, elapsed: 22s, ETA:    53s
[>>>>>>>>>                     ] 150/500, 6.7 task/s, elapsed: 22s, ETA:    52s
[>>>>>>>>>                     ] 151/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 152/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 153/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 154/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 155/500, 6.7 task/s, elapsed: 23s, ETA:    52s
[>>>>>>>>>                     ] 156/500, 6.7 task/s, elapsed: 23s, ETA:    51s
[>>>>>>>>>                     ] 157/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 158/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 159/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 160/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 161/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 162/500, 6.7 task/s, elapsed: 24s, ETA:    51s
[>>>>>>>>>                     ] 163/500, 6.7 task/s, elapsed: 24s, ETA:    50s
[>>>>>>>>>                     ] 164/500, 6.7 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>                     ] 165/500, 6.7 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>                     ] 166/500, 6.7 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>>                    ] 167/500, 6.7 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>>                    ] 168/500, 6.7 task/s, elapsed: 25s, ETA:    50s
[>>>>>>>>>>                    ] 169/500, 6.7 task/s, elapsed: 25s, ETA:    49s
[>>>>>>>>>>                    ] 170/500, 6.7 task/s, elapsed: 25s, ETA:    49s
[>>>>>>>>>>                    ] 171/500, 6.7 task/s, elapsed: 26s, ETA:    49s
[>>>>>>>>>>                    ] 172/500, 6.7 task/s, elapsed: 26s, ETA:    49s
[>>>>>>>>>>                    ] 173/500, 6.7 task/s, elapsed: 26s, ETA:    49s
[>>>>>>>>>>                    ] 174/500, 6.7 task/s, elapsed: 26s, ETA:    49s
[>>>>>>>>>>                    ] 175/500, 6.7 task/s, elapsed: 26s, ETA:    49s
[>>>>>>>>>>                    ] 176/500, 6.7 task/s, elapsed: 26s, ETA:    48s
[>>>>>>>>>>                    ] 177/500, 6.7 task/s, elapsed: 26s, ETA:    48s
[>>>>>>>>>>                    ] 178/500, 6.7 task/s, elapsed: 27s, ETA:    48s
[>>>>>>>>>>                    ] 179/500, 6.7 task/s, elapsed: 27s, ETA:    48s
[>>>>>>>>>>                    ] 180/500, 6.7 task/s, elapsed: 27s, ETA:    48s
[>>>>>>>>>>                    ] 181/500, 6.7 task/s, elapsed: 27s, ETA:    48s
[>>>>>>>>>>                    ] 182/500, 6.7 task/s, elapsed: 27s, ETA:    47s
[>>>>>>>>>>                    ] 183/500, 6.7 task/s, elapsed: 27s, ETA:    47s
[>>>>>>>>>>>                   ] 184/500, 6.7 task/s, elapsed: 27s, ETA:    47s
[>>>>>>>>>>>                   ] 185/500, 6.7 task/s, elapsed: 28s, ETA:    47s
[>>>>>>>>>>>                   ] 186/500, 6.7 task/s, elapsed: 28s, ETA:    47s
[>>>>>>>>>>>                   ] 187/500, 6.7 task/s, elapsed: 28s, ETA:    47s
[>>>>>>>>>>>                   ] 188/500, 6.7 task/s, elapsed: 28s, ETA:    47s
[>>>>>>>>>>>                   ] 189/500, 6.7 task/s, elapsed: 28s, ETA:    46s
[>>>>>>>>>>>                   ] 190/500, 6.7 task/s, elapsed: 28s, ETA:    46s
[>>>>>>>>>>>                   ] 191/500, 6.7 task/s, elapsed: 28s, ETA:    46s
[>>>>>>>>>>>                   ] 192/500, 6.7 task/s, elapsed: 29s, ETA:    46s
[>>>>>>>>>>>                   ] 193/500, 6.7 task/s, elapsed: 29s, ETA:    46s
[>>>>>>>>>>>                   ] 194/500, 6.7 task/s, elapsed: 29s, ETA:    46s
[>>>>>>>>>>>                   ] 195/500, 6.7 task/s, elapsed: 29s, ETA:    45s
[>>>>>>>>>>>                   ] 196/500, 6.7 task/s, elapsed: 29s, ETA:    45s
[>>>>>>>>>>>                   ] 197/500, 6.7 task/s, elapsed: 29s, ETA:    45s
[>>>>>>>>>>>                   ] 198/500, 6.7 task/s, elapsed: 29s, ETA:    45s
[>>>>>>>>>>>                   ] 199/500, 6.7 task/s, elapsed: 30s, ETA:    45s
[>>>>>>>>>>>>                  ] 200/500, 6.7 task/s, elapsed: 30s, ETA:    45s
[>>>>>>>>>>>>                  ] 201/500, 6.7 task/s, elapsed: 30s, ETA:    44s
[>>>>>>>>>>>>                  ] 202/500, 6.7 task/s, elapsed: 30s, ETA:    44s
[>>>>>>>>>>>>                  ] 203/500, 6.7 task/s, elapsed: 30s, ETA:    44s
[>>>>>>>>>>>>                  ] 204/500, 6.7 task/s, elapsed: 30s, ETA:    44s
[>>>>>>>>>>>>                  ] 205/500, 6.7 task/s, elapsed: 31s, ETA:    44s
[>>>>>>>>>>>>                  ] 206/500, 6.7 task/s, elapsed: 31s, ETA:    44s
[>>>>>>>>>>>>                  ] 207/500, 6.7 task/s, elapsed: 31s, ETA:    44s
[>>>>>>>>>>>>                  ] 208/500, 6.7 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 209/500, 6.7 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 210/500, 6.7 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 211/500, 6.7 task/s, elapsed: 31s, ETA:    43s
[>>>>>>>>>>>>                  ] 212/500, 6.7 task/s, elapsed: 32s, ETA:    43s
[>>>>>>>>>>>>                  ] 213/500, 6.7 task/s, elapsed: 32s, ETA:    43s
[>>>>>>>>>>>>                  ] 214/500, 6.7 task/s, elapsed: 32s, ETA:    43s
[>>>>>>>>>>>>                  ] 215/500, 6.7 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>                  ] 216/500, 6.7 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>>                 ] 217/500, 6.7 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>>                 ] 218/500, 6.7 task/s, elapsed: 32s, ETA:    42s
[>>>>>>>>>>>>>                 ] 219/500, 6.7 task/s, elapsed: 33s, ETA:    42s
[>>>>>>>>>>>>>                 ] 220/500, 6.7 task/s, elapsed: 33s, ETA:    42s
[>>>>>>>>>>>>>                 ] 221/500, 6.7 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 222/500, 6.7 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 223/500, 6.7 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 224/500, 6.7 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 225/500, 6.7 task/s, elapsed: 33s, ETA:    41s
[>>>>>>>>>>>>>                 ] 226/500, 6.7 task/s, elapsed: 34s, ETA:    41s
[>>>>>>>>>>>>>                 ] 227/500, 6.7 task/s, elapsed: 34s, ETA:    41s
[>>>>>>>>>>>>>                 ] 228/500, 6.7 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 229/500, 6.7 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 230/500, 6.7 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 231/500, 6.7 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 232/500, 6.7 task/s, elapsed: 34s, ETA:    40s
[>>>>>>>>>>>>>                 ] 233/500, 6.7 task/s, elapsed: 35s, ETA:    40s
[>>>>>>>>>>>>>>                ] 234/500, 6.7 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 235/500, 6.7 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 236/500, 6.7 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 237/500, 6.7 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 238/500, 6.7 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 239/500, 6.7 task/s, elapsed: 35s, ETA:    39s
[>>>>>>>>>>>>>>                ] 240/500, 6.7 task/s, elapsed: 36s, ETA:    39s
[>>>>>>>>>>>>>>                ] 241/500, 6.7 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 242/500, 6.7 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 243/500, 6.7 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 244/500, 6.7 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 245/500, 6.7 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 246/500, 6.7 task/s, elapsed: 36s, ETA:    38s
[>>>>>>>>>>>>>>                ] 247/500, 6.7 task/s, elapsed: 37s, ETA:    38s
[>>>>>>>>>>>>>>                ] 248/500, 6.7 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>                ] 249/500, 6.7 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 250/500, 6.7 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 251/500, 6.7 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 252/500, 6.7 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 253/500, 6.7 task/s, elapsed: 37s, ETA:    37s
[>>>>>>>>>>>>>>>               ] 254/500, 6.7 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 255/500, 6.7 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 256/500, 6.7 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 257/500, 6.7 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 258/500, 6.7 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 259/500, 6.7 task/s, elapsed: 38s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 260/500, 6.7 task/s, elapsed: 39s, ETA:    36s
[>>>>>>>>>>>>>>>               ] 261/500, 6.7 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 262/500, 6.7 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 263/500, 6.7 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 264/500, 6.7 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 265/500, 6.7 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>               ] 266/500, 6.8 task/s, elapsed: 39s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 267/500, 6.8 task/s, elapsed: 40s, ETA:    35s
[>>>>>>>>>>>>>>>>              ] 268/500, 6.8 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 269/500, 6.8 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 270/500, 6.8 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 271/500, 6.8 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 272/500, 6.8 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 273/500, 6.8 task/s, elapsed: 40s, ETA:    34s
[>>>>>>>>>>>>>>>>              ] 274/500, 6.8 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 275/500, 6.8 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 276/500, 6.8 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 277/500, 6.8 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 278/500, 6.8 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 279/500, 6.8 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 280/500, 6.8 task/s, elapsed: 41s, ETA:    33s
[>>>>>>>>>>>>>>>>              ] 281/500, 6.8 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>              ] 282/500, 6.8 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>              ] 283/500, 6.8 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 284/500, 6.8 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 285/500, 6.8 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 286/500, 6.8 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 287/500, 6.8 task/s, elapsed: 42s, ETA:    32s
[>>>>>>>>>>>>>>>>>             ] 288/500, 6.8 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 289/500, 6.8 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 290/500, 6.8 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 291/500, 6.8 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 292/500, 6.8 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 293/500, 6.8 task/s, elapsed: 43s, ETA:    31s
[>>>>>>>>>>>>>>>>>             ] 294/500, 6.8 task/s, elapsed: 43s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 295/500, 6.8 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 296/500, 6.8 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 297/500, 6.8 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 298/500, 6.8 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>             ] 299/500, 6.8 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 300/500, 6.8 task/s, elapsed: 44s, ETA:    30s
[>>>>>>>>>>>>>>>>>>            ] 301/500, 6.8 task/s, elapsed: 44s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 302/500, 6.8 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 303/500, 6.8 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 304/500, 6.8 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 305/500, 6.8 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 306/500, 6.8 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 307/500, 6.8 task/s, elapsed: 45s, ETA:    29s
[>>>>>>>>>>>>>>>>>>            ] 308/500, 6.8 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 309/500, 6.8 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 310/500, 6.8 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 311/500, 6.8 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 312/500, 6.8 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 313/500, 6.8 task/s, elapsed: 46s, ETA:    28s
[>>>>>>>>>>>>>>>>>>            ] 314/500, 6.8 task/s, elapsed: 46s, ETA:    27s
[>>>>>>>>>>>>>>>>>>            ] 315/500, 6.8 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>            ] 316/500, 6.8 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 317/500, 6.8 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 318/500, 6.8 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 319/500, 6.8 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 320/500, 6.8 task/s, elapsed: 47s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>           ] 321/500, 6.8 task/s, elapsed: 47s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 322/500, 6.8 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 323/500, 6.8 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 324/500, 6.8 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 325/500, 6.8 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 326/500, 6.8 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 327/500, 6.8 task/s, elapsed: 48s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>           ] 328/500, 6.8 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 329/500, 6.8 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 330/500, 6.8 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 331/500, 6.8 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 332/500, 6.8 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>           ] 333/500, 6.8 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 334/500, 6.8 task/s, elapsed: 49s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>          ] 335/500, 6.8 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 336/500, 6.8 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 337/500, 6.8 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 338/500, 6.8 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 339/500, 6.8 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 340/500, 6.8 task/s, elapsed: 50s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>          ] 341/500, 6.8 task/s, elapsed: 50s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 342/500, 6.8 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 343/500, 6.8 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 344/500, 6.8 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 345/500, 6.8 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 346/500, 6.8 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 347/500, 6.8 task/s, elapsed: 51s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>          ] 348/500, 6.8 task/s, elapsed: 51s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>          ] 349/500, 6.8 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 350/500, 6.8 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 351/500, 6.8 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 352/500, 6.8 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 353/500, 6.8 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 354/500, 6.8 task/s, elapsed: 52s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>         ] 355/500, 6.8 task/s, elapsed: 52s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 356/500, 6.8 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 357/500, 6.8 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 358/500, 6.8 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 359/500, 6.8 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 360/500, 6.8 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 361/500, 6.8 task/s, elapsed: 53s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>         ] 362/500, 6.8 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 363/500, 6.8 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 364/500, 6.8 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 365/500, 6.8 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>         ] 366/500, 6.8 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 367/500, 6.8 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 368/500, 6.8 task/s, elapsed: 54s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>        ] 369/500, 6.8 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 370/500, 6.8 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 371/500, 6.8 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 372/500, 6.8 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 373/500, 6.8 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 374/500, 6.8 task/s, elapsed: 55s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>        ] 375/500, 6.8 task/s, elapsed: 55s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 376/500, 6.8 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 377/500, 6.8 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 378/500, 6.8 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 379/500, 6.8 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 380/500, 6.8 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 381/500, 6.8 task/s, elapsed: 56s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>        ] 382/500, 6.8 task/s, elapsed: 56s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>        ] 383/500, 6.8 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 384/500, 6.8 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 385/500, 6.8 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 386/500, 6.8 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 387/500, 6.8 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 388/500, 6.8 task/s, elapsed: 57s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 389/500, 6.8 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 390/500, 6.8 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 391/500, 6.8 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 392/500, 6.8 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 393/500, 6.8 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 394/500, 6.8 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 395/500, 6.8 task/s, elapsed: 58s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 396/500, 6.8 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 397/500, 6.8 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 398/500, 6.8 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>       ] 399/500, 6.8 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 400/500, 6.8 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 401/500, 6.8 task/s, elapsed: 59s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 402/500, 6.8 task/s, elapsed: 59s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 403/500, 6.8 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 404/500, 6.8 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 405/500, 6.8 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 406/500, 6.8 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 407/500, 6.8 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 408/500, 6.8 task/s, elapsed: 60s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 409/500, 6.8 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 410/500, 6.8 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 411/500, 6.8 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 412/500, 6.8 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 413/500, 6.8 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 414/500, 6.7 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 415/500, 6.7 task/s, elapsed: 61s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>      ] 416/500, 6.7 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 417/500, 6.7 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 418/500, 6.7 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 419/500, 6.7 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 420/500, 6.7 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 421/500, 6.7 task/s, elapsed: 62s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 422/500, 6.7 task/s, elapsed: 63s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 423/500, 6.7 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 424/500, 6.7 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 425/500, 6.7 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 426/500, 6.7 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 427/500, 6.7 task/s, elapsed: 63s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 428/500, 6.7 task/s, elapsed: 64s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 429/500, 6.7 task/s, elapsed: 64s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 430/500, 6.7 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 431/500, 6.7 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 432/500, 6.7 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 433/500, 6.7 task/s, elapsed: 64s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 434/500, 6.7 task/s, elapsed: 65s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 435/500, 6.7 task/s, elapsed: 65s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 436/500, 6.7 task/s, elapsed: 65s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 437/500, 6.7 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 438/500, 6.7 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 439/500, 6.7 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 440/500, 6.7 task/s, elapsed: 65s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 441/500, 6.7 task/s, elapsed: 66s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 442/500, 6.7 task/s, elapsed: 66s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 443/500, 6.7 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 444/500, 6.7 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 445/500, 6.7 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 446/500, 6.7 task/s, elapsed: 66s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 447/500, 6.7 task/s, elapsed: 67s, ETA:     8s/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = tpsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 452/500, 6.7 task/s, elapsed: 67s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 453/500, 6.7 task/s, elapsed: 68s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 454/500, 6.7 task/s, elapsed: 68s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 455/500, 6.7 task/s, elapsed: 68s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 456/500, 6.7 task/s, elapsed: 68s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 457/500, 6.7 task/s, elapsed: 68s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 458/500, 6.7 task/s, elapsed: 68s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 459/500, 6.7 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 460/500, 6.7 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 461/500, 6.7 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 462/500, 6.7 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 463/500, 6.7 task/s, elapsed: 69s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 464/500, 6.7 task/s, elapsed: 69s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 465/500, 6.7 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 466/500, 6.7 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 467/500, 6.7 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 468/500, 6.7 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 469/500, 6.7 task/s, elapsed: 70s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 470/500, 6.7 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 471/500, 6.7 task/s, elapsed: 70s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 472/500, 6.7 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 473/500, 6.7 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 474/500, 6.7 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 475/500, 6.7 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 476/500, 6.7 task/s, elapsed: 71s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 477/500, 6.7 task/s, elapsed: 71s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 478/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 479/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 480/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 481/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 482/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 483/500, 6.7 task/s, elapsed: 72s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 484/500, 6.7 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 485/500, 6.7 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 486/500, 6.7 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 487/500, 6.7 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 488/500, 6.7 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 489/500, 6.7 task/s, elapsed: 73s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 490/500, 6.7 task/s, elapsed: 73s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 491/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 492/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 493/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 494/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 495/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 496/500, 6.7 task/s, elapsed: 74s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 497/500, 6.7 task/s, elapsed: 75s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 498/500, 6.7 task/s, elapsed: 75s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 499/500, 6.7 task/s, elapsed: 75s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 500/500, 6.7 task/s, elapsed: 75s, ETA:     0s2022-04-19 06:27:55,748 - mmseg - INFO - per class results:
2022-04-19 06:27:55,750 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|      road     | 82.09 | 86.02 |
|    sidewalk   | 36.71 | 72.67 |
|    building   | 88.04 | 95.11 |
|      wall     | 38.95 | 46.17 |
|     fence     |  8.74 | 10.16 |
|      pole     | 49.76 | 58.04 |
| traffic light |  55.5 | 66.21 |
|  traffic sign | 55.46 | 64.98 |
|   vegetation  | 85.17 | 94.09 |
|    terrain    |  0.0  |  0.0  |
|      sky      | 86.26 | 99.13 |
|     person    | 72.84 | 87.81 |
|     rider     | 44.22 | 70.07 |
|      car      | 86.61 | 94.61 |
|     truck     |  0.0  |  0.0  |
|      bus      | 59.21 | 79.21 |
|     train     |  0.0  |  0.0  |
|   motorcycle  | 54.34 |  65.8 |
|    bicycle    | 60.64 | 68.45 |
+---------------+-------+-------+
2022-04-19 06:27:55,750 - mmseg - INFO - Summary:
2022-04-19 06:27:55,750 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 86.93 | 50.76 | 60.97 |
+-------+-------+-------+
2022-04-19 06:27:55,752 - mmseg - INFO - Saving checkpoint at 40000 iterations
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:28:17,304 - mmseg - INFO - Exp name: 220418_1315_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_b6618
2022-04-19 06:28:17,305 - mmseg - INFO - Iter [500/40000]	lr: 1.500e-09, eta: 0:00:00, time: 1.499, data_time: 0.014, memory: 9636, aAcc: 0.8693, mIoU: 0.5076, mAcc: 0.6097, IoU.road: 0.8209, IoU.sidewalk: 0.3671, IoU.building: 0.8804, IoU.wall: 0.3895, IoU.fence: 0.0874, IoU.pole: 0.4976, IoU.traffic light: 0.5550, IoU.traffic sign: 0.5546, IoU.vegetation: 0.8517, IoU.terrain: 0.0000, IoU.sky: 0.8626, IoU.person: 0.7284, IoU.rider: 0.4422, IoU.car: 0.8661, IoU.truck: 0.0000, IoU.bus: 0.5921, IoU.train: 0.0000, IoU.motorcycle: 0.5434, IoU.bicycle: 0.6064, Acc.road: 0.8602, Acc.sidewalk: 0.7267, Acc.building: 0.9511, Acc.wall: 0.4617, Acc.fence: 0.1016, Acc.pole: 0.5804, Acc.traffic light: 0.6621, Acc.traffic sign: 0.6498, Acc.vegetation: 0.9409, Acc.terrain: 0.0000, Acc.sky: 0.9913, Acc.person: 0.8781, Acc.rider: 0.7007, Acc.car: 0.9461, Acc.truck: 0.0000, Acc.bus: 0.7921, Acc.train: 0.0000, Acc.motorcycle: 0.6580, Acc.bicycle: 0.6845, decode.loss_seg: 0.1168, decode.acc_seg: 90.6410, src.loss_imnet_feat_dist: 0.0956, mix.decode.loss_seg: 0.1178, mix.decode.acc_seg: 89.3126
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:29:53,883 - mmseg - INFO - Iter [18150/40000]	lr: 3.278e-05, eta: 20:43:46, time: 3.373, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.2259, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.6675
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:32:41,880 - mmseg - INFO - Iter [18200/40000]	lr: 3.270e-05, eta: 20:40:52, time: 3.360, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.7654, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.5300
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:35:32,325 - mmseg - INFO - Iter [18250/40000]	lr: 3.263e-05, eta: 20:38:01, time: 3.409, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.2109, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.2285
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:38:19,409 - mmseg - INFO - Iter [18300/40000]	lr: 3.255e-05, eta: 20:35:05, time: 3.342, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.3374, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.4104
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:41:09,458 - mmseg - INFO - Iter [18350/40000]	lr: 3.248e-05, eta: 20:32:14, time: 3.401, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 24.2046, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.4324
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:43:56,657 - mmseg - INFO - Iter [18400/40000]	lr: 3.240e-05, eta: 20:29:19, time: 3.344, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.1834, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.0235
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:46:46,025 - mmseg - INFO - Iter [18450/40000]	lr: 3.233e-05, eta: 20:26:27, time: 3.387, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.0810, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 55.8151
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:49:34,344 - mmseg - INFO - Iter [18500/40000]	lr: 3.225e-05, eta: 20:23:33, time: 3.366, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.9225, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.4991
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:52:22,604 - mmseg - INFO - Iter [18550/40000]	lr: 3.218e-05, eta: 20:20:40, time: 3.365, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.0392, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.8817
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:55:12,022 - mmseg - INFO - Iter [18600/40000]	lr: 3.210e-05, eta: 20:17:47, time: 3.388, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.2250, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.9356
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 06:58:04,867 - mmseg - INFO - Iter [18650/40000]	lr: 3.203e-05, eta: 20:14:59, time: 3.457, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.4995, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.2299
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:00:53,735 - mmseg - INFO - Iter [18700/40000]	lr: 3.195e-05, eta: 20:12:06, time: 3.377, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.0777, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.3860
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:03:41,846 - mmseg - INFO - Iter [18750/40000]	lr: 3.188e-05, eta: 20:09:12, time: 3.362, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.6895, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.3295
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:06:29,019 - mmseg - INFO - Iter [18800/40000]	lr: 3.180e-05, eta: 20:06:18, time: 3.343, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 18.6129, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.2712
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:09:19,140 - mmseg - INFO - Iter [18850/40000]	lr: 3.173e-05, eta: 20:03:26, time: 3.402, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 17.6705, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.5925
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:12:08,186 - mmseg - INFO - Iter [18900/40000]	lr: 3.165e-05, eta: 20:00:34, time: 3.381, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.3499, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.7723
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:14:57,544 - mmseg - INFO - Iter [18950/40000]	lr: 3.158e-05, eta: 19:57:42, time: 3.387, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 18.8143, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.1510
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:17:45,808 - mmseg - INFO - Exp name: 220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02
2022-04-19 07:17:45,809 - mmseg - INFO - Iter [19000/40000]	lr: 3.150e-05, eta: 19:54:48, time: 3.365, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 18.1240, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 55.1187
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:20:35,051 - mmseg - INFO - Iter [19050/40000]	lr: 3.143e-05, eta: 19:51:56, time: 3.385, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.9650, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.7355
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:23:24,317 - mmseg - INFO - Iter [19100/40000]	lr: 3.135e-05, eta: 19:49:04, time: 3.385, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.4705, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.4109
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:26:13,525 - mmseg - INFO - Iter [19150/40000]	lr: 3.128e-05, eta: 19:46:12, time: 3.384, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.5751, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.4370
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:29:00,843 - mmseg - INFO - Iter [19200/40000]	lr: 3.120e-05, eta: 19:43:17, time: 3.346, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.6784, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.8184
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:31:50,432 - mmseg - INFO - Iter [19250/40000]	lr: 3.113e-05, eta: 19:40:25, time: 3.392, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.4897, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.7512
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:34:37,696 - mmseg - INFO - Iter [19300/40000]	lr: 3.105e-05, eta: 19:37:31, time: 3.345, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.8602, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.6709
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:37:27,033 - mmseg - INFO - Iter [19350/40000]	lr: 3.098e-05, eta: 19:34:39, time: 3.387, data_time: 0.019, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.8761, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.3988
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:40:17,304 - mmseg - INFO - Iter [19400/40000]	lr: 3.090e-05, eta: 19:31:48, time: 3.405, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.5673, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.8153
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:43:07,060 - mmseg - INFO - Iter [19450/40000]	lr: 3.083e-05, eta: 19:28:56, time: 3.395, data_time: 0.019, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.5048, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.3655
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:45:53,906 - mmseg - INFO - Iter [19500/40000]	lr: 3.075e-05, eta: 19:26:02, time: 3.337, data_time: 0.019, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.3489, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.6272
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:48:43,875 - mmseg - INFO - Iter [19550/40000]	lr: 3.068e-05, eta: 19:23:10, time: 3.399, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.6275, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.0750
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:51:32,057 - mmseg - INFO - Iter [19600/40000]	lr: 3.060e-05, eta: 19:20:17, time: 3.364, data_time: 0.019, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.4286, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.5048
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:54:22,746 - mmseg - INFO - Iter [19650/40000]	lr: 3.053e-05, eta: 19:17:27, time: 3.414, data_time: 0.019, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.9800, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.5498
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 07:57:10,165 - mmseg - INFO - Iter [19700/40000]	lr: 3.045e-05, eta: 19:14:33, time: 3.348, data_time: 0.019, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.5987, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 55.8905
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:00:00,521 - mmseg - INFO - Iter [19750/40000]	lr: 3.038e-05, eta: 19:11:42, time: 3.407, data_time: 0.019, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.4545, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.7067
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:02:48,437 - mmseg - INFO - Iter [19800/40000]	lr: 3.030e-05, eta: 19:08:48, time: 3.358, data_time: 0.019, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.9435, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 63.2601
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:05:38,422 - mmseg - INFO - Iter [19850/40000]	lr: 3.023e-05, eta: 19:05:57, time: 3.400, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.6153, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.9491
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:08:26,026 - mmseg - INFO - Iter [19900/40000]	lr: 3.015e-05, eta: 19:03:03, time: 3.352, data_time: 0.019, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.4837, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.8046
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:11:15,430 - mmseg - INFO - Iter [19950/40000]	lr: 3.008e-05, eta: 19:00:12, time: 3.388, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.0570, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.7896
[                                                  ] 0/500, elapsed: 0s, ETA:
[                                 ] 1/500, 1.3 task/s, elapsed: 1s, ETA:   381s
[                                 ] 2/500, 1.7 task/s, elapsed: 1s, ETA:   287s
[                                 ] 3/500, 1.9 task/s, elapsed: 2s, ETA:   255s
[                                 ] 4/500, 2.1 task/s, elapsed: 2s, ETA:   239s
[                                 ] 5/500, 2.2 task/s, elapsed: 2s, ETA:   229s
[                                 ] 6/500, 2.2 task/s, elapsed: 3s, ETA:   223s
[                                 ] 7/500, 2.3 task/s, elapsed: 3s, ETA:   218s
[                                 ] 8/500, 2.3 task/s, elapsed: 3s, ETA:   214s
[                                 ] 9/500, 2.3 task/s, elapsed: 4s, ETA:   211s
[                                ] 10/500, 2.3 task/s, elapsed: 4s, ETA:   209s
[                                ] 11/500, 2.4 task/s, elapsed: 5s, ETA:   207s
[                                ] 12/500, 2.4 task/s, elapsed: 5s, ETA:   205s
[                                ] 13/500, 2.4 task/s, elapsed: 5s, ETA:   203s
[                                ] 14/500, 2.4 task/s, elapsed: 6s, ETA:   202s
[                                ] 15/500, 2.4 task/s, elapsed: 6s, ETA:   201s
[>                               ] 16/500, 2.4 task/s, elapsed: 7s, ETA:   199s
[>                               ] 17/500, 2.4 task/s, elapsed: 7s, ETA:   198s
[>                               ] 18/500, 2.4 task/s, elapsed: 7s, ETA:   197s
[>                               ] 19/500, 2.4 task/s, elapsed: 8s, ETA:   196s
[>                               ] 20/500, 2.5 task/s, elapsed: 8s, ETA:   196s
[>                               ] 21/500, 2.5 task/s, elapsed: 9s, ETA:   195s
[>                               ] 22/500, 2.5 task/s, elapsed: 9s, ETA:   194s
[>                               ] 23/500, 2.5 task/s, elapsed: 9s, ETA:   193s
[>                              ] 24/500, 2.5 task/s, elapsed: 10s, ETA:   192s
[>                              ] 25/500, 2.5 task/s, elapsed: 10s, ETA:   192s
[>                              ] 26/500, 2.5 task/s, elapsed: 10s, ETA:   191s
[>                              ] 27/500, 2.5 task/s, elapsed: 11s, ETA:   190s
[>                              ] 28/500, 2.5 task/s, elapsed: 11s, ETA:   190s
[>                              ] 29/500, 2.5 task/s, elapsed: 12s, ETA:   189s
[>                              ] 30/500, 2.5 task/s, elapsed: 12s, ETA:   189s
[>                              ] 31/500, 2.5 task/s, elapsed: 12s, ETA:   188s
[>                              ] 32/500, 2.5 task/s, elapsed: 13s, ETA:   187s
[>>                             ] 33/500, 2.5 task/s, elapsed: 13s, ETA:   187s
[>>                             ] 34/500, 2.5 task/s, elapsed: 14s, ETA:   186s
[>>                             ] 35/500, 2.5 task/s, elapsed: 14s, ETA:   186s
[>>                             ] 36/500, 2.5 task/s, elapsed: 14s, ETA:   185s
[>>                             ] 37/500, 2.5 task/s, elapsed: 15s, ETA:   185s
[>>                             ] 38/500, 2.5 task/s, elapsed: 15s, ETA:   184s
[>>                             ] 39/500, 2.5 task/s, elapsed: 16s, ETA:   184s
[>>                             ] 40/500, 2.5 task/s, elapsed: 16s, ETA:   183s
[>>                             ] 41/500, 2.5 task/s, elapsed: 16s, ETA:   183s
[>>                             ] 42/500, 2.5 task/s, elapsed: 17s, ETA:   182s
[>>                             ] 43/500, 2.5 task/s, elapsed: 17s, ETA:   182s
[>>                             ] 44/500, 2.5 task/s, elapsed: 17s, ETA:   181s
[>>                             ] 45/500, 2.5 task/s, elapsed: 18s, ETA:   181s
[>>                             ] 46/500, 2.5 task/s, elapsed: 18s, ETA:   180s
[>>                             ] 47/500, 2.5 task/s, elapsed: 19s, ETA:   180s
[>>                             ] 48/500, 2.5 task/s, elapsed: 19s, ETA:   179s
[>>>                            ] 49/500, 2.5 task/s, elapsed: 19s, ETA:   179s
[>>>                            ] 50/500, 2.5 task/s, elapsed: 20s, ETA:   178s
[>>>                            ] 51/500, 2.5 task/s, elapsed: 20s, ETA:   178s
[>>>                            ] 52/500, 2.5 task/s, elapsed: 21s, ETA:   177s
[>>>                            ] 53/500, 2.5 task/s, elapsed: 21s, ETA:   177s
[>>>                            ] 54/500, 2.5 task/s, elapsed: 21s, ETA:   177s
[>>>                            ] 55/500, 2.5 task/s, elapsed: 22s, ETA:   176s
[>>>                            ] 56/500, 2.5 task/s, elapsed: 22s, ETA:   176s
[>>>                            ] 57/500, 2.5 task/s, elapsed: 23s, ETA:   175s
[>>>                            ] 58/500, 2.5 task/s, elapsed: 23s, ETA:   175s
[>>>                            ] 59/500, 2.5 task/s, elapsed: 23s, ETA:   174s
[>>>                            ] 60/500, 2.5 task/s, elapsed: 24s, ETA:   174s
[>>>                            ] 61/500, 2.5 task/s, elapsed: 24s, ETA:   173s
[>>>                            ] 62/500, 2.5 task/s, elapsed: 24s, ETA:   173s
[>>>                            ] 63/500, 2.5 task/s, elapsed: 25s, ETA:   172s
[>>>                            ] 64/500, 2.5 task/s, elapsed: 25s, ETA:   172s
[>>>>                           ] 65/500, 2.5 task/s, elapsed: 26s, ETA:   172s
[>>>>                           ] 66/500, 2.5 task/s, elapsed: 26s, ETA:   171s
[>>>>                           ] 67/500, 2.5 task/s, elapsed: 26s, ETA:   171s
[>>>>                           ] 68/500, 2.5 task/s, elapsed: 27s, ETA:   170s
[>>>>                           ] 69/500, 2.5 task/s, elapsed: 27s, ETA:   170s
[>>>>                           ] 70/500, 2.5 task/s, elapsed: 28s, ETA:   169s
[>>>>                           ] 71/500, 2.5 task/s, elapsed: 28s, ETA:   169s
[>>>>                           ] 72/500, 2.5 task/s, elapsed: 28s, ETA:   169s
[>>>>                           ] 73/500, 2.5 task/s, elapsed: 29s, ETA:   168s
[>>>>                           ] 74/500, 2.5 task/s, elapsed: 29s, ETA:   168s
[>>>>                           ] 75/500, 2.5 task/s, elapsed: 30s, ETA:   167s
[>>>>                           ] 76/500, 2.5 task/s, elapsed: 30s, ETA:   167s
[>>>>                           ] 77/500, 2.5 task/s, elapsed: 30s, ETA:   166s
[>>>>                           ] 78/500, 2.5 task/s, elapsed: 31s, ETA:   166s
[>>>>                           ] 79/500, 2.5 task/s, elapsed: 31s, ETA:   166s
[>>>>                           ] 80/500, 2.5 task/s, elapsed: 31s, ETA:   165s
[>>>>>                          ] 81/500, 2.5 task/s, elapsed: 32s, ETA:   165s
[>>>>>                          ] 82/500, 2.5 task/s, elapsed: 32s, ETA:   164s
[>>>>>                          ] 83/500, 2.5 task/s, elapsed: 33s, ETA:   164s
[>>>>>                          ] 84/500, 2.5 task/s, elapsed: 33s, ETA:   164s
[>>>>>                          ] 85/500, 2.5 task/s, elapsed: 33s, ETA:   163s
[>>>>>                          ] 86/500, 2.5 task/s, elapsed: 34s, ETA:   163s
[>>>>>                          ] 87/500, 2.5 task/s, elapsed: 34s, ETA:   162s
[>>>>>                          ] 88/500, 2.5 task/s, elapsed: 35s, ETA:   162s
[>>>>>                          ] 89/500, 2.5 task/s, elapsed: 35s, ETA:   162s
[>>>>>                          ] 90/500, 2.5 task/s, elapsed: 35s, ETA:   161s
[>>>>>                          ] 91/500, 2.5 task/s, elapsed: 36s, ETA:   161s
[>>>>>                          ] 92/500, 2.5 task/s, elapsed: 36s, ETA:   161s
[>>>>>                          ] 93/500, 2.5 task/s, elapsed: 37s, ETA:   161s
[>>>>>                          ] 94/500, 2.5 task/s, elapsed: 37s, ETA:   161s
[>>>>>                          ] 95/500, 2.5 task/s, elapsed: 38s, ETA:   161s
[>>>>>                          ] 96/500, 2.5 task/s, elapsed: 38s, ETA:   161s
[>>>>>>                         ] 97/500, 2.5 task/s, elapsed: 39s, ETA:   161s
[>>>>>>                         ] 98/500, 2.5 task/s, elapsed: 39s, ETA:   161s
[>>>>>>                         ] 99/500, 2.5 task/s, elapsed: 40s, ETA:   161s
[>>>>>>                        ] 100/500, 2.5 task/s, elapsed: 40s, ETA:   160s
[>>>>>>                        ] 101/500, 2.5 task/s, elapsed: 41s, ETA:   160s
[>>>>>>                        ] 102/500, 2.5 task/s, elapsed: 41s, ETA:   160s
[>>>>>>                        ] 103/500, 2.5 task/s, elapsed: 41s, ETA:   160s
[>>>>>>                        ] 104/500, 2.5 task/s, elapsed: 42s, ETA:   159s
[>>>>>>                        ] 105/500, 2.5 task/s, elapsed: 42s, ETA:   159s
[>>>>>>                        ] 106/500, 2.5 task/s, elapsed: 43s, ETA:   159s
[>>>>>>                        ] 107/500, 2.5 task/s, elapsed: 43s, ETA:   158s
[>>>>>>                        ] 108/500, 2.5 task/s, elapsed: 43s, ETA:   158s
[>>>>>>                        ] 109/500, 2.5 task/s, elapsed: 44s, ETA:   157s
[>>>>>>                        ] 110/500, 2.5 task/s, elapsed: 44s, ETA:   157s
[>>>>>>                        ] 111/500, 2.5 task/s, elapsed: 45s, ETA:   156s
[>>>>>>                        ] 112/500, 2.5 task/s, elapsed: 45s, ETA:   156s
[>>>>>>                        ] 113/500, 2.5 task/s, elapsed: 45s, ETA:   155s
[>>>>>>                        ] 114/500, 2.5 task/s, elapsed: 46s, ETA:   155s
[>>>>>>                        ] 115/500, 2.5 task/s, elapsed: 46s, ETA:   155s
[>>>>>>                        ] 116/500, 2.5 task/s, elapsed: 47s, ETA:   154s
[>>>>>>>                       ] 117/500, 2.5 task/s, elapsed: 47s, ETA:   154s
[>>>>>>>                       ] 118/500, 2.5 task/s, elapsed: 47s, ETA:   153s
[>>>>>>>                       ] 119/500, 2.5 task/s, elapsed: 48s, ETA:   153s
[>>>>>>>                       ] 120/500, 2.5 task/s, elapsed: 48s, ETA:   152s
[>>>>>>>                       ] 121/500, 2.5 task/s, elapsed: 49s, ETA:   152s
[>>>>>>>                       ] 122/500, 2.5 task/s, elapsed: 49s, ETA:   151s
[>>>>>>>                       ] 123/500, 2.5 task/s, elapsed: 49s, ETA:   151s
[>>>>>>>                       ] 124/500, 2.5 task/s, elapsed: 50s, ETA:   151s
[>>>>>>>                       ] 125/500, 2.5 task/s, elapsed: 50s, ETA:   150s
[>>>>>>>                       ] 126/500, 2.5 task/s, elapsed: 50s, ETA:   150s
[>>>>>>>                       ] 127/500, 2.5 task/s, elapsed: 51s, ETA:   149s
[>>>>>>>                       ] 128/500, 2.5 task/s, elapsed: 51s, ETA:   149s
[>>>>>>>                       ] 129/500, 2.5 task/s, elapsed: 52s, ETA:   148s
[>>>>>>>                       ] 130/500, 2.5 task/s, elapsed: 52s, ETA:   148s
[>>>>>>>                       ] 131/500, 2.5 task/s, elapsed: 52s, ETA:   148s
[>>>>>>>                       ] 132/500, 2.5 task/s, elapsed: 53s, ETA:   147s
[>>>>>>>                       ] 133/500, 2.5 task/s, elapsed: 53s, ETA:   147s
[>>>>>>>>                      ] 134/500, 2.5 task/s, elapsed: 54s, ETA:   146s
[>>>>>>>>                      ] 135/500, 2.5 task/s, elapsed: 54s, ETA:   146s
[>>>>>>>>                      ] 136/500, 2.5 task/s, elapsed: 54s, ETA:   145s
[>>>>>>>>                      ] 137/500, 2.5 task/s, elapsed: 55s, ETA:   145s
[>>>>>>>>                      ] 138/500, 2.5 task/s, elapsed: 55s, ETA:   145s
[>>>>>>>>                      ] 139/500, 2.5 task/s, elapsed: 55s, ETA:   144s
[>>>>>>>>                      ] 140/500, 2.5 task/s, elapsed: 56s, ETA:   144s
[>>>>>>>>                      ] 141/500, 2.5 task/s, elapsed: 56s, ETA:   143s
[>>>>>>>>                      ] 142/500, 2.5 task/s, elapsed: 57s, ETA:   143s
[>>>>>>>>                      ] 143/500, 2.5 task/s, elapsed: 57s, ETA:   142s
[>>>>>>>>                      ] 144/500, 2.5 task/s, elapsed: 57s, ETA:   142s
[>>>>>>>>                      ] 145/500, 2.5 task/s, elapsed: 58s, ETA:   142s
[>>>>>>>>                      ] 146/500, 2.5 task/s, elapsed: 58s, ETA:   141s
[>>>>>>>>                      ] 147/500, 2.5 task/s, elapsed: 59s, ETA:   141s
[>>>>>>>>                      ] 148/500, 2.5 task/s, elapsed: 59s, ETA:   140s
[>>>>>>>>                      ] 149/500, 2.5 task/s, elapsed: 59s, ETA:   140s
[>>>>>>>>>                     ] 150/500, 2.5 task/s, elapsed: 60s, ETA:   139s
[>>>>>>>>>                     ] 151/500, 2.5 task/s, elapsed: 60s, ETA:   139s
[>>>>>>>>>                     ] 152/500, 2.5 task/s, elapsed: 61s, ETA:   139s
[>>>>>>>>>                     ] 153/500, 2.5 task/s, elapsed: 61s, ETA:   138s
[>>>>>>>>>                     ] 154/500, 2.5 task/s, elapsed: 61s, ETA:   138s
[>>>>>>>>>                     ] 155/500, 2.5 task/s, elapsed: 62s, ETA:   137s
[>>>>>>>>>                     ] 156/500, 2.5 task/s, elapsed: 62s, ETA:   137s
[>>>>>>>>>                     ] 157/500, 2.5 task/s, elapsed: 62s, ETA:   137s
[>>>>>>>>>                     ] 158/500, 2.5 task/s, elapsed: 63s, ETA:   136s
[>>>>>>>>>                     ] 159/500, 2.5 task/s, elapsed: 63s, ETA:   136s
[>>>>>>>>>                     ] 160/500, 2.5 task/s, elapsed: 64s, ETA:   135s
[>>>>>>>>>                     ] 161/500, 2.5 task/s, elapsed: 64s, ETA:   135s
[>>>>>>>>>                     ] 162/500, 2.5 task/s, elapsed: 64s, ETA:   134s
[>>>>>>>>>                     ] 163/500, 2.5 task/s, elapsed: 65s, ETA:   134s
[>>>>>>>>>                     ] 164/500, 2.5 task/s, elapsed: 65s, ETA:   134s
[>>>>>>>>>                     ] 165/500, 2.5 task/s, elapsed: 66s, ETA:   133s
[>>>>>>>>>                     ] 166/500, 2.5 task/s, elapsed: 66s, ETA:   133s
[>>>>>>>>>>                    ] 167/500, 2.5 task/s, elapsed: 66s, ETA:   132s
[>>>>>>>>>>                    ] 168/500, 2.5 task/s, elapsed: 67s, ETA:   132s
[>>>>>>>>>>                    ] 169/500, 2.5 task/s, elapsed: 67s, ETA:   132s
[>>>>>>>>>>                    ] 170/500, 2.5 task/s, elapsed: 68s, ETA:   131s
[>>>>>>>>>>                    ] 171/500, 2.5 task/s, elapsed: 68s, ETA:   131s
[>>>>>>>>>>                    ] 172/500, 2.5 task/s, elapsed: 68s, ETA:   130s
[>>>>>>>>>>                    ] 173/500, 2.5 task/s, elapsed: 69s, ETA:   130s
[>>>>>>>>>>                    ] 174/500, 2.5 task/s, elapsed: 69s, ETA:   129s
[>>>>>>>>>>                    ] 175/500, 2.5 task/s, elapsed: 69s, ETA:   129s
[>>>>>>>>>>                    ] 176/500, 2.5 task/s, elapsed: 70s, ETA:   129s
[>>>>>>>>>>                    ] 177/500, 2.5 task/s, elapsed: 70s, ETA:   128s
[>>>>>>>>>>                    ] 178/500, 2.5 task/s, elapsed: 71s, ETA:   128s
[>>>>>>>>>>                    ] 179/500, 2.5 task/s, elapsed: 71s, ETA:   127s
[>>>>>>>>>>                    ] 180/500, 2.5 task/s, elapsed: 71s, ETA:   127s
[>>>>>>>>>>                    ] 181/500, 2.5 task/s, elapsed: 72s, ETA:   127s
[>>>>>>>>>>                    ] 182/500, 2.5 task/s, elapsed: 72s, ETA:   126s
[>>>>>>>>>>                    ] 183/500, 2.5 task/s, elapsed: 73s, ETA:   126s
[>>>>>>>>>>>                   ] 184/500, 2.5 task/s, elapsed: 73s, ETA:   125s
[>>>>>>>>>>>                   ] 185/500, 2.5 task/s, elapsed: 73s, ETA:   125s
[>>>>>>>>>>>                   ] 186/500, 2.5 task/s, elapsed: 74s, ETA:   124s
[>>>>>>>>>>>                   ] 187/500, 2.5 task/s, elapsed: 74s, ETA:   124s
[>>>>>>>>>>>                   ] 188/500, 2.5 task/s, elapsed: 75s, ETA:   124s
[>>>>>>>>>>>                   ] 189/500, 2.5 task/s, elapsed: 75s, ETA:   123s
[>>>>>>>>>>>                   ] 190/500, 2.5 task/s, elapsed: 75s, ETA:   123s
[>>>>>>>>>>>                   ] 191/500, 2.5 task/s, elapsed: 76s, ETA:   122s
[>>>>>>>>>>>                   ] 192/500, 2.5 task/s, elapsed: 76s, ETA:   122s
[>>>>>>>>>>>                   ] 193/500, 2.5 task/s, elapsed: 76s, ETA:   122s
[>>>>>>>>>>>                   ] 194/500, 2.5 task/s, elapsed: 77s, ETA:   121s
[>>>>>>>>>>>                   ] 195/500, 2.5 task/s, elapsed: 77s, ETA:   121s
[>>>>>>>>>>>                   ] 196/500, 2.5 task/s, elapsed: 78s, ETA:   120s
[>>>>>>>>>>>                   ] 197/500, 2.5 task/s, elapsed: 78s, ETA:   120s
[>>>>>>>>>>>                   ] 198/500, 2.5 task/s, elapsed: 78s, ETA:   120s
[>>>>>>>>>>>                   ] 199/500, 2.5 task/s, elapsed: 79s, ETA:   119s
[>>>>>>>>>>>>                  ] 200/500, 2.5 task/s, elapsed: 79s, ETA:   119s
[>>>>>>>>>>>>                  ] 201/500, 2.5 task/s, elapsed: 80s, ETA:   118s
[>>>>>>>>>>>>                  ] 202/500, 2.5 task/s, elapsed: 80s, ETA:   118s
[>>>>>>>>>>>>                  ] 203/500, 2.5 task/s, elapsed: 80s, ETA:   118s
[>>>>>>>>>>>>                  ] 204/500, 2.5 task/s, elapsed: 81s, ETA:   117s
[>>>>>>>>>>>>                  ] 205/500, 2.5 task/s, elapsed: 81s, ETA:   117s
[>>>>>>>>>>>>                  ] 206/500, 2.5 task/s, elapsed: 82s, ETA:   116s
[>>>>>>>>>>>>                  ] 207/500, 2.5 task/s, elapsed: 82s, ETA:   116s
[>>>>>>>>>>>>                  ] 208/500, 2.5 task/s, elapsed: 82s, ETA:   116s
[>>>>>>>>>>>>                  ] 209/500, 2.5 task/s, elapsed: 83s, ETA:   115s
[>>>>>>>>>>>>                  ] 210/500, 2.5 task/s, elapsed: 83s, ETA:   115s
[>>>>>>>>>>>>                  ] 211/500, 2.5 task/s, elapsed: 83s, ETA:   114s
[>>>>>>>>>>>>                  ] 212/500, 2.5 task/s, elapsed: 84s, ETA:   114s
[>>>>>>>>>>>>                  ] 213/500, 2.5 task/s, elapsed: 84s, ETA:   113s
[>>>>>>>>>>>>                  ] 214/500, 2.5 task/s, elapsed: 85s, ETA:   113s
[>>>>>>>>>>>>                  ] 215/500, 2.5 task/s, elapsed: 85s, ETA:   113s
[>>>>>>>>>>>>                  ] 216/500, 2.5 task/s, elapsed: 85s, ETA:   112s
[>>>>>>>>>>>>>                 ] 217/500, 2.5 task/s, elapsed: 86s, ETA:   112s
[>>>>>>>>>>>>>                 ] 218/500, 2.5 task/s, elapsed: 86s, ETA:   111s
[>>>>>>>>>>>>>                 ] 219/500, 2.5 task/s, elapsed: 87s, ETA:   111s
[>>>>>>>>>>>>>                 ] 220/500, 2.5 task/s, elapsed: 87s, ETA:   111s
[>>>>>>>>>>>>>                 ] 221/500, 2.5 task/s, elapsed: 87s, ETA:   110s
[>>>>>>>>>>>>>                 ] 222/500, 2.5 task/s, elapsed: 88s, ETA:   110s
[>>>>>>>>>>>>>                 ] 223/500, 2.5 task/s, elapsed: 88s, ETA:   109s
[>>>>>>>>>>>>>                 ] 224/500, 2.5 task/s, elapsed: 88s, ETA:   109s
[>>>>>>>>>>>>>                 ] 225/500, 2.5 task/s, elapsed: 89s, ETA:   109s
[>>>>>>>>>>>>>                 ] 226/500, 2.5 task/s, elapsed: 89s, ETA:   108s
[>>>>>>>>>>>>>                 ] 227/500, 2.5 task/s, elapsed: 90s, ETA:   108s
[>>>>>>>>>>>>>                 ] 228/500, 2.5 task/s, elapsed: 90s, ETA:   107s
[>>>>>>>>>>>>>                 ] 229/500, 2.5 task/s, elapsed: 90s, ETA:   107s
[>>>>>>>>>>>>>                 ] 230/500, 2.5 task/s, elapsed: 91s, ETA:   107s
[>>>>>>>>>>>>>                 ] 231/500, 2.5 task/s, elapsed: 91s, ETA:   106s
[>>>>>>>>>>>>>                 ] 232/500, 2.5 task/s, elapsed: 92s, ETA:   106s
[>>>>>>>>>>>>>                 ] 233/500, 2.5 task/s, elapsed: 92s, ETA:   105s
[>>>>>>>>>>>>>>                ] 234/500, 2.5 task/s, elapsed: 92s, ETA:   105s
[>>>>>>>>>>>>>>                ] 235/500, 2.5 task/s, elapsed: 93s, ETA:   105s
[>>>>>>>>>>>>>>                ] 236/500, 2.5 task/s, elapsed: 93s, ETA:   104s
[>>>>>>>>>>>>>>                ] 237/500, 2.5 task/s, elapsed: 94s, ETA:   104s
[>>>>>>>>>>>>>>                ] 238/500, 2.5 task/s, elapsed: 94s, ETA:   103s
[>>>>>>>>>>>>>>                ] 239/500, 2.5 task/s, elapsed: 94s, ETA:   103s
[>>>>>>>>>>>>>>                ] 240/500, 2.5 task/s, elapsed: 95s, ETA:   103s
[>>>>>>>>>>>>>>                ] 241/500, 2.5 task/s, elapsed: 95s, ETA:   102s
[>>>>>>>>>>>>>>                ] 242/500, 2.5 task/s, elapsed: 95s, ETA:   102s
[>>>>>>>>>>>>>>                ] 243/500, 2.5 task/s, elapsed: 96s, ETA:   101s
[>>>>>>>>>>>>>>                ] 244/500, 2.5 task/s, elapsed: 96s, ETA:   101s
[>>>>>>>>>>>>>>                ] 245/500, 2.5 task/s, elapsed: 97s, ETA:   101s
[>>>>>>>>>>>>>>                ] 246/500, 2.5 task/s, elapsed: 97s, ETA:   100s
[>>>>>>>>>>>>>>                ] 247/500, 2.5 task/s, elapsed: 97s, ETA:   100s
[>>>>>>>>>>>>>>                ] 248/500, 2.5 task/s, elapsed: 98s, ETA:    99s
[>>>>>>>>>>>>>>                ] 249/500, 2.5 task/s, elapsed: 98s, ETA:    99s
[>>>>>>>>>>>>>>>               ] 250/500, 2.5 task/s, elapsed: 99s, ETA:    99s
[>>>>>>>>>>>>>>>               ] 251/500, 2.5 task/s, elapsed: 99s, ETA:    98s
[>>>>>>>>>>>>>>>               ] 252/500, 2.5 task/s, elapsed: 99s, ETA:    98s
[>>>>>>>>>>>>>>               ] 253/500, 2.5 task/s, elapsed: 100s, ETA:    97s
[>>>>>>>>>>>>>>               ] 254/500, 2.5 task/s, elapsed: 100s, ETA:    97s
[>>>>>>>>>>>>>>               ] 255/500, 2.5 task/s, elapsed: 101s, ETA:    97s
[>>>>>>>>>>>>>>               ] 256/500, 2.5 task/s, elapsed: 101s, ETA:    96s
[>>>>>>>>>>>>>>               ] 257/500, 2.5 task/s, elapsed: 101s, ETA:    96s
[>>>>>>>>>>>>>>               ] 258/500, 2.5 task/s, elapsed: 102s, ETA:    95s
[>>>>>>>>>>>>>>>              ] 259/500, 2.5 task/s, elapsed: 102s, ETA:    95s
[>>>>>>>>>>>>>>>              ] 260/500, 2.5 task/s, elapsed: 102s, ETA:    95s
[>>>>>>>>>>>>>>>              ] 261/500, 2.5 task/s, elapsed: 103s, ETA:    94s
[>>>>>>>>>>>>>>>              ] 262/500, 2.5 task/s, elapsed: 103s, ETA:    94s
[>>>>>>>>>>>>>>>              ] 263/500, 2.5 task/s, elapsed: 104s, ETA:    93s
[>>>>>>>>>>>>>>>              ] 264/500, 2.5 task/s, elapsed: 104s, ETA:    93s
[>>>>>>>>>>>>>>>              ] 265/500, 2.5 task/s, elapsed: 104s, ETA:    93s
[>>>>>>>>>>>>>>>              ] 266/500, 2.5 task/s, elapsed: 105s, ETA:    92s
[>>>>>>>>>>>>>>>              ] 267/500, 2.5 task/s, elapsed: 105s, ETA:    92s
[>>>>>>>>>>>>>>>              ] 268/500, 2.5 task/s, elapsed: 106s, ETA:    91s
[>>>>>>>>>>>>>>>              ] 269/500, 2.5 task/s, elapsed: 106s, ETA:    91s
[>>>>>>>>>>>>>>>              ] 270/500, 2.5 task/s, elapsed: 106s, ETA:    91s
[>>>>>>>>>>>>>>>              ] 271/500, 2.5 task/s, elapsed: 107s, ETA:    90s
[>>>>>>>>>>>>>>>              ] 272/500, 2.5 task/s, elapsed: 107s, ETA:    90s
[>>>>>>>>>>>>>>>              ] 273/500, 2.5 task/s, elapsed: 108s, ETA:    89s
[>>>>>>>>>>>>>>>              ] 274/500, 2.5 task/s, elapsed: 108s, ETA:    89s
[>>>>>>>>>>>>>>>              ] 275/500, 2.5 task/s, elapsed: 108s, ETA:    89s
[>>>>>>>>>>>>>>>>             ] 276/500, 2.5 task/s, elapsed: 109s, ETA:    88s
[>>>>>>>>>>>>>>>>             ] 277/500, 2.5 task/s, elapsed: 109s, ETA:    88s
[>>>>>>>>>>>>>>>>             ] 278/500, 2.5 task/s, elapsed: 109s, ETA:    87s
[>>>>>>>>>>>>>>>>             ] 279/500, 2.5 task/s, elapsed: 110s, ETA:    87s
[>>>>>>>>>>>>>>>>             ] 280/500, 2.5 task/s, elapsed: 110s, ETA:    87s
[>>>>>>>>>>>>>>>>             ] 281/500, 2.5 task/s, elapsed: 111s, ETA:    86s
[>>>>>>>>>>>>>>>>             ] 282/500, 2.5 task/s, elapsed: 111s, ETA:    86s
[>>>>>>>>>>>>>>>>             ] 283/500, 2.5 task/s, elapsed: 111s, ETA:    85s
[>>>>>>>>>>>>>>>>             ] 284/500, 2.5 task/s, elapsed: 112s, ETA:    85s
[>>>>>>>>>>>>>>>>             ] 285/500, 2.5 task/s, elapsed: 112s, ETA:    85s
[>>>>>>>>>>>>>>>>             ] 286/500, 2.5 task/s, elapsed: 113s, ETA:    84s
[>>>>>>>>>>>>>>>>             ] 287/500, 2.5 task/s, elapsed: 113s, ETA:    84s
[>>>>>>>>>>>>>>>>             ] 288/500, 2.5 task/s, elapsed: 113s, ETA:    83s
[>>>>>>>>>>>>>>>>             ] 289/500, 2.5 task/s, elapsed: 114s, ETA:    83s
[>>>>>>>>>>>>>>>>             ] 290/500, 2.5 task/s, elapsed: 114s, ETA:    83s
[>>>>>>>>>>>>>>>>             ] 291/500, 2.5 task/s, elapsed: 115s, ETA:    82s
[>>>>>>>>>>>>>>>>             ] 292/500, 2.5 task/s, elapsed: 115s, ETA:    82s
[>>>>>>>>>>>>>>>>             ] 293/500, 2.5 task/s, elapsed: 115s, ETA:    81s
[>>>>>>>>>>>>>>>>>            ] 294/500, 2.5 task/s, elapsed: 116s, ETA:    81s
[>>>>>>>>>>>>>>>>>            ] 295/500, 2.5 task/s, elapsed: 116s, ETA:    81s
[>>>>>>>>>>>>>>>>>            ] 296/500, 2.5 task/s, elapsed: 116s, ETA:    80s
[>>>>>>>>>>>>>>>>>            ] 297/500, 2.5 task/s, elapsed: 117s, ETA:    80s
[>>>>>>>>>>>>>>>>>            ] 298/500, 2.5 task/s, elapsed: 117s, ETA:    79s
[>>>>>>>>>>>>>>>>>            ] 299/500, 2.5 task/s, elapsed: 118s, ETA:    79s
[>>>>>>>>>>>>>>>>>            ] 300/500, 2.5 task/s, elapsed: 118s, ETA:    79s
[>>>>>>>>>>>>>>>>>            ] 301/500, 2.5 task/s, elapsed: 118s, ETA:    78s
[>>>>>>>>>>>>>>>>>            ] 302/500, 2.5 task/s, elapsed: 119s, ETA:    78s
[>>>>>>>>>>>>>>>>>            ] 303/500, 2.5 task/s, elapsed: 119s, ETA:    78s
[>>>>>>>>>>>>>>>>>            ] 304/500, 2.5 task/s, elapsed: 120s, ETA:    77s
[>>>>>>>>>>>>>>>>>            ] 305/500, 2.5 task/s, elapsed: 120s, ETA:    77s
[>>>>>>>>>>>>>>>>>            ] 306/500, 2.5 task/s, elapsed: 120s, ETA:    76s
[>>>>>>>>>>>>>>>>>            ] 307/500, 2.5 task/s, elapsed: 121s, ETA:    76s
[>>>>>>>>>>>>>>>>>            ] 308/500, 2.5 task/s, elapsed: 121s, ETA:    76s
[>>>>>>>>>>>>>>>>>            ] 309/500, 2.5 task/s, elapsed: 122s, ETA:    75s
[>>>>>>>>>>>>>>>>>            ] 310/500, 2.5 task/s, elapsed: 122s, ETA:    75s
[>>>>>>>>>>>>>>>>>>           ] 311/500, 2.5 task/s, elapsed: 122s, ETA:    74s
[>>>>>>>>>>>>>>>>>>           ] 312/500, 2.5 task/s, elapsed: 123s, ETA:    74s
[>>>>>>>>>>>>>>>>>>           ] 313/500, 2.5 task/s, elapsed: 123s, ETA:    74s
[>>>>>>>>>>>>>>>>>>           ] 314/500, 2.5 task/s, elapsed: 123s, ETA:    73s
[>>>>>>>>>>>>>>>>>>           ] 315/500, 2.5 task/s, elapsed: 124s, ETA:    73s
[>>>>>>>>>>>>>>>>>>           ] 316/500, 2.5 task/s, elapsed: 124s, ETA:    72s
[>>>>>>>>>>>>>>>>>>           ] 317/500, 2.5 task/s, elapsed: 125s, ETA:    72s
[>>>>>>>>>>>>>>>>>>           ] 318/500, 2.5 task/s, elapsed: 125s, ETA:    72s
[>>>>>>>>>>>>>>>>>>           ] 319/500, 2.5 task/s, elapsed: 125s, ETA:    71s
[>>>>>>>>>>>>>>>>>>           ] 320/500, 2.5 task/s, elapsed: 126s, ETA:    71s
[>>>>>>>>>>>>>>>>>>           ] 321/500, 2.5 task/s, elapsed: 126s, ETA:    70s
[>>>>>>>>>>>>>>>>>>           ] 322/500, 2.5 task/s, elapsed: 127s, ETA:    70s
[>>>>>>>>>>>>>>>>>>           ] 323/500, 2.5 task/s, elapsed: 127s, ETA:    70s
[>>>>>>>>>>>>>>>>>>           ] 324/500, 2.5 task/s, elapsed: 127s, ETA:    69s
[>>>>>>>>>>>>>>>>>>           ] 325/500, 2.5 task/s, elapsed: 128s, ETA:    69s
[>>>>>>>>>>>>>>>>>>           ] 326/500, 2.5 task/s, elapsed: 128s, ETA:    68s
[>>>>>>>>>>>>>>>>>>           ] 327/500, 2.5 task/s, elapsed: 129s, ETA:    68s
[>>>>>>>>>>>>>>>>>>>          ] 328/500, 2.5 task/s, elapsed: 129s, ETA:    68s
[>>>>>>>>>>>>>>>>>>>          ] 329/500, 2.5 task/s, elapsed: 129s, ETA:    67s
[>>>>>>>>>>>>>>>>>>>          ] 330/500, 2.5 task/s, elapsed: 130s, ETA:    67s
[>>>>>>>>>>>>>>>>>>>          ] 331/500, 2.5 task/s, elapsed: 130s, ETA:    66s
[>>>>>>>>>>>>>>>>>>>          ] 332/500, 2.5 task/s, elapsed: 130s, ETA:    66s
[>>>>>>>>>>>>>>>>>>>          ] 333/500, 2.5 task/s, elapsed: 131s, ETA:    66s
[>>>>>>>>>>>>>>>>>>>          ] 334/500, 2.5 task/s, elapsed: 131s, ETA:    65s
[>>>>>>>>>>>>>>>>>>>          ] 335/500, 2.5 task/s, elapsed: 132s, ETA:    65s
[>>>>>>>>>>>>>>>>>>>          ] 336/500, 2.5 task/s, elapsed: 132s, ETA:    64s
[>>>>>>>>>>>>>>>>>>>          ] 337/500, 2.5 task/s, elapsed: 132s, ETA:    64s
[>>>>>>>>>>>>>>>>>>>          ] 338/500, 2.5 task/s, elapsed: 133s, ETA:    64s
[>>>>>>>>>>>>>>>>>>>          ] 339/500, 2.5 task/s, elapsed: 133s, ETA:    63s
[>>>>>>>>>>>>>>>>>>>          ] 340/500, 2.5 task/s, elapsed: 134s, ETA:    63s
[>>>>>>>>>>>>>>>>>>>          ] 341/500, 2.5 task/s, elapsed: 134s, ETA:    62s
[>>>>>>>>>>>>>>>>>>>          ] 342/500, 2.5 task/s, elapsed: 134s, ETA:    62s
[>>>>>>>>>>>>>>>>>>>          ] 343/500, 2.5 task/s, elapsed: 135s, ETA:    62s
[>>>>>>>>>>>>>>>>>>>          ] 344/500, 2.5 task/s, elapsed: 135s, ETA:    61s
[>>>>>>>>>>>>>>>>>>>>         ] 345/500, 2.5 task/s, elapsed: 136s, ETA:    61s
[>>>>>>>>>>>>>>>>>>>>         ] 346/500, 2.5 task/s, elapsed: 136s, ETA:    61s
[>>>>>>>>>>>>>>>>>>>>         ] 347/500, 2.5 task/s, elapsed: 136s, ETA:    60s
[>>>>>>>>>>>>>>>>>>>>         ] 348/500, 2.5 task/s, elapsed: 137s, ETA:    60s
[>>>>>>>>>>>>>>>>>>>>         ] 349/500, 2.5 task/s, elapsed: 137s, ETA:    59s
[>>>>>>>>>>>>>>>>>>>>         ] 350/500, 2.5 task/s, elapsed: 138s, ETA:    59s
[>>>>>>>>>>>>>>>>>>>>         ] 351/500, 2.5 task/s, elapsed: 138s, ETA:    59s
[>>>>>>>>>>>>>>>>>>>>         ] 352/500, 2.5 task/s, elapsed: 138s, ETA:    58s
[>>>>>>>>>>>>>>>>>>>>         ] 353/500, 2.5 task/s, elapsed: 139s, ETA:    58s
[>>>>>>>>>>>>>>>>>>>>         ] 354/500, 2.5 task/s, elapsed: 139s, ETA:    57s
[>>>>>>>>>>>>>>>>>>>>         ] 355/500, 2.5 task/s, elapsed: 140s, ETA:    57s
[>>>>>>>>>>>>>>>>>>>>         ] 356/500, 2.5 task/s, elapsed: 140s, ETA:    57s
[>>>>>>>>>>>>>>>>>>>>         ] 357/500, 2.5 task/s, elapsed: 140s, ETA:    56s
[>>>>>>>>>>>>>>>>>>>>         ] 358/500, 2.5 task/s, elapsed: 141s, ETA:    56s
[>>>>>>>>>>>>>>>>>>>>         ] 359/500, 2.5 task/s, elapsed: 141s, ETA:    55s
[>>>>>>>>>>>>>>>>>>>>         ] 360/500, 2.5 task/s, elapsed: 142s, ETA:    55s
[>>>>>>>>>>>>>>>>>>>>         ] 361/500, 2.5 task/s, elapsed: 142s, ETA:    55s
[>>>>>>>>>>>>>>>>>>>>         ] 362/500, 2.5 task/s, elapsed: 142s, ETA:    54s
[>>>>>>>>>>>>>>>>>>>>>        ] 363/500, 2.5 task/s, elapsed: 143s, ETA:    54s
[>>>>>>>>>>>>>>>>>>>>>        ] 364/500, 2.5 task/s, elapsed: 143s, ETA:    54s
[>>>>>>>>>>>>>>>>>>>>>        ] 365/500, 2.5 task/s, elapsed: 144s, ETA:    53s
[>>>>>>>>>>>>>>>>>>>>>        ] 366/500, 2.5 task/s, elapsed: 144s, ETA:    53s
[>>>>>>>>>>>>>>>>>>>>>        ] 367/500, 2.5 task/s, elapsed: 144s, ETA:    52s
[>>>>>>>>>>>>>>>>>>>>>        ] 368/500, 2.5 task/s, elapsed: 145s, ETA:    52s
[>>>>>>>>>>>>>>>>>>>>>        ] 369/500, 2.5 task/s, elapsed: 145s, ETA:    52s
[>>>>>>>>>>>>>>>>>>>>>        ] 370/500, 2.5 task/s, elapsed: 146s, ETA:    51s
[>>>>>>>>>>>>>>>>>>>>>        ] 371/500, 2.5 task/s, elapsed: 146s, ETA:    51s
[>>>>>>>>>>>>>>>>>>>>>        ] 372/500, 2.5 task/s, elapsed: 146s, ETA:    50s
[>>>>>>>>>>>>>>>>>>>>>        ] 373/500, 2.5 task/s, elapsed: 147s, ETA:    50s
[>>>>>>>>>>>>>>>>>>>>>        ] 374/500, 2.5 task/s, elapsed: 147s, ETA:    50s
[>>>>>>>>>>>>>>>>>>>>>        ] 375/500, 2.5 task/s, elapsed: 148s, ETA:    49s
[>>>>>>>>>>>>>>>>>>>>>        ] 376/500, 2.5 task/s, elapsed: 148s, ETA:    49s
[>>>>>>>>>>>>>>>>>>>>>        ] 377/500, 2.5 task/s, elapsed: 148s, ETA:    48s
[>>>>>>>>>>>>>>>>>>>>>        ] 378/500, 2.5 task/s, elapsed: 149s, ETA:    48s
[>>>>>>>>>>>>>>>>>>>>>        ] 379/500, 2.5 task/s, elapsed: 149s, ETA:    48s
[>>>>>>>>>>>>>>>>>>>>>>       ] 380/500, 2.5 task/s, elapsed: 150s, ETA:    47s
[>>>>>>>>>>>>>>>>>>>>>>       ] 381/500, 2.5 task/s, elapsed: 150s, ETA:    47s
[>>>>>>>>>>>>>>>>>>>>>>       ] 382/500, 2.5 task/s, elapsed: 150s, ETA:    46s
[>>>>>>>>>>>>>>>>>>>>>>       ] 383/500, 2.5 task/s, elapsed: 151s, ETA:    46s
[>>>>>>>>>>>>>>>>>>>>>>       ] 384/500, 2.5 task/s, elapsed: 151s, ETA:    46s
[>>>>>>>>>>>>>>>>>>>>>>       ] 385/500, 2.5 task/s, elapsed: 152s, ETA:    45s
[>>>>>>>>>>>>>>>>>>>>>>       ] 386/500, 2.5 task/s, elapsed: 152s, ETA:    45s
[>>>>>>>>>>>>>>>>>>>>>>       ] 387/500, 2.5 task/s, elapsed: 152s, ETA:    45s
[>>>>>>>>>>>>>>>>>>>>>>       ] 388/500, 2.5 task/s, elapsed: 153s, ETA:    44s
[>>>>>>>>>>>>>>>>>>>>>>       ] 389/500, 2.5 task/s, elapsed: 153s, ETA:    44s
[>>>>>>>>>>>>>>>>>>>>>>       ] 390/500, 2.5 task/s, elapsed: 154s, ETA:    43s
[>>>>>>>>>>>>>>>>>>>>>>       ] 391/500, 2.5 task/s, elapsed: 154s, ETA:    43s
[>>>>>>>>>>>>>>>>>>>>>>       ] 392/500, 2.5 task/s, elapsed: 155s, ETA:    43s
[>>>>>>>>>>>>>>>>>>>>>>       ] 393/500, 2.5 task/s, elapsed: 155s, ETA:    42s
[>>>>>>>>>>>>>>>>>>>>>>       ] 394/500, 2.5 task/s, elapsed: 155s, ETA:    42s
[>>>>>>>>>>>>>>>>>>>>>>       ] 395/500, 2.5 task/s, elapsed: 156s, ETA:    41s
[>>>>>>>>>>>>>>>>>>>>>>       ] 396/500, 2.5 task/s, elapsed: 156s, ETA:    41s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 397/500, 2.5 task/s, elapsed: 157s, ETA:    41s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 398/500, 2.5 task/s, elapsed: 157s, ETA:    40s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 399/500, 2.5 task/s, elapsed: 157s, ETA:    40s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 400/500, 2.5 task/s, elapsed: 158s, ETA:    39s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 401/500, 2.5 task/s, elapsed: 158s, ETA:    39s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 402/500, 2.5 task/s, elapsed: 159s, ETA:    39s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 403/500, 2.5 task/s, elapsed: 159s, ETA:    38s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 404/500, 2.5 task/s, elapsed: 159s, ETA:    38s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 405/500, 2.5 task/s, elapsed: 160s, ETA:    37s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 406/500, 2.5 task/s, elapsed: 160s, ETA:    37s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 407/500, 2.5 task/s, elapsed: 161s, ETA:    37s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 408/500, 2.5 task/s, elapsed: 161s, ETA:    36s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 409/500, 2.5 task/s, elapsed: 161s, ETA:    36s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 410/500, 2.5 task/s, elapsed: 162s, ETA:    36s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 411/500, 2.5 task/s, elapsed: 162s, ETA:    35s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 412/500, 2.5 task/s, elapsed: 163s, ETA:    35s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 413/500, 2.5 task/s, elapsed: 163s, ETA:    34s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 414/500, 2.5 task/s, elapsed: 163s, ETA:    34s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 415/500, 2.5 task/s, elapsed: 164s, ETA:    34s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 416/500, 2.5 task/s, elapsed: 164s, ETA:    33s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 417/500, 2.5 task/s, elapsed: 165s, ETA:    33s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 418/500, 2.5 task/s, elapsed: 165s, ETA:    32s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 419/500, 2.5 task/s, elapsed: 165s, ETA:    32s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 420/500, 2.5 task/s, elapsed: 166s, ETA:    32s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 421/500, 2.5 task/s, elapsed: 166s, ETA:    31s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 422/500, 2.5 task/s, elapsed: 167s, ETA:    31s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 423/500, 2.5 task/s, elapsed: 167s, ETA:    30s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 424/500, 2.5 task/s, elapsed: 168s, ETA:    30s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 425/500, 2.5 task/s, elapsed: 168s, ETA:    30s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 426/500, 2.5 task/s, elapsed: 168s, ETA:    29s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 427/500, 2.5 task/s, elapsed: 169s, ETA:    29s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 428/500, 2.5 task/s, elapsed: 169s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 429/500, 2.5 task/s, elapsed: 170s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 430/500, 2.5 task/s, elapsed: 170s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 431/500, 2.5 task/s, elapsed: 170s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 432/500, 2.5 task/s, elapsed: 171s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 433/500, 2.5 task/s, elapsed: 171s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 434/500, 2.5 task/s, elapsed: 172s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 435/500, 2.5 task/s, elapsed: 172s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 436/500, 2.5 task/s, elapsed: 172s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 437/500, 2.5 task/s, elapsed: 173s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 438/500, 2.5 task/s, elapsed: 173s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 439/500, 2.5 task/s, elapsed: 174s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 440/500, 2.5 task/s, elapsed: 174s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 441/500, 2.5 task/s, elapsed: 174s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 442/500, 2.5 task/s, elapsed: 175s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 443/500, 2.5 task/s, elapsed: 175s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 444/500, 2.5 task/s, elapsed: 176s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 445/500, 2.5 task/s, elapsed: 176s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 446/500, 2.5 task/s, elapsed: 177s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 447/500, 2.5 task/s, elapsed: 177s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 448/500, 2.5 task/s, elapsed: 177s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 449/500, 2.5 task/s, elapsed: 178s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 450/500, 2.5 task/s, elapsed: 178s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 451/500, 2.5 task/s, elapsed: 179s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 452/500, 2.5 task/s, elapsed: 179s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 453/500, 2.5 task/s, elapsed: 180s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 454/500, 2.5 task/s, elapsed: 180s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 455/500, 2.5 task/s, elapsed: 180s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 456/500, 2.5 task/s, elapsed: 181s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 457/500, 2.5 task/s, elapsed: 181s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 458/500, 2.5 task/s, elapsed: 182s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 459/500, 2.5 task/s, elapsed: 182s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 460/500, 2.5 task/s, elapsed: 183s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 461/500, 2.5 task/s, elapsed: 183s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 462/500, 2.5 task/s, elapsed: 183s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 463/500, 2.5 task/s, elapsed: 184s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 464/500, 2.5 task/s, elapsed: 184s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 465/500, 2.5 task/s, elapsed: 185s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 466/500, 2.5 task/s, elapsed: 185s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 467/500, 2.5 task/s, elapsed: 186s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 468/500, 2.5 task/s, elapsed: 186s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 469/500, 2.5 task/s, elapsed: 186s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 470/500, 2.5 task/s, elapsed: 187s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 471/500, 2.5 task/s, elapsed: 187s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 472/500, 2.5 task/s, elapsed: 188s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 473/500, 2.5 task/s, elapsed: 188s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 474/500, 2.5 task/s, elapsed: 188s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 475/500, 2.5 task/s, elapsed: 189s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 476/500, 2.5 task/s, elapsed: 189s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 477/500, 2.5 task/s, elapsed: 190s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 478/500, 2.5 task/s, elapsed: 190s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 479/500, 2.5 task/s, elapsed: 191s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 480/500, 2.5 task/s, elapsed: 191s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 481/500, 2.5 task/s, elapsed: 191s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 482/500, 2.5 task/s, elapsed: 192s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 483/500, 2.5 task/s, elapsed: 192s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 484/500, 2.5 task/s, elapsed: 193s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 485/500, 2.5 task/s, elapsed: 193s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 486/500, 2.5 task/s, elapsed: 193s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 487/500, 2.5 task/s, elapsed: 194s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 488/500, 2.5 task/s, elapsed: 194s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 489/500, 2.5 task/s, elapsed: 195s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 490/500, 2.5 task/s, elapsed: 195s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 491/500, 2.5 task/s, elapsed: 195s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 492/500, 2.5 task/s, elapsed: 196s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 493/500, 2.5 task/s, elapsed: 196s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 494/500, 2.5 task/s, elapsed: 197s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 495/500, 2.5 task/s, elapsed: 197s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 496/500, 2.5 task/s, elapsed: 197s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 497/500, 2.5 task/s, elapsed: 198s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 498/500, 2.5 task/s, elapsed: 198s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 499/500, 2.5 task/s, elapsed: 199s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 500/500, 2.5 task/s, elapsed: 199s, ETA:     0s2022-04-19 08:18:08,018 - mmseg - INFO - per class results:
2022-04-19 08:18:08,020 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|      road     | 37.66 | 100.0 |
|    sidewalk   |  0.0  |  0.0  |
|    building   |  0.0  |  0.0  |
|      wall     |  0.0  |  0.0  |
|     fence     |  0.0  |  0.0  |
|      pole     |  0.0  |  0.0  |
| traffic light |  0.0  |  0.0  |
|  traffic sign |  0.0  |  0.0  |
|   vegetation  |  0.0  |  0.0  |
|    terrain    |  0.0  |  0.0  |
|      sky      |  0.0  |  0.0  |
|     person    |  0.0  |  0.0  |
|     rider     |  0.0  |  0.0  |
|      car      |  0.0  |  0.0  |
|     truck     |  0.0  |  0.0  |
|      bus      |  0.0  |  0.0  |
|     train     |  0.0  |  0.0  |
|   motorcycle  |  0.0  |  0.0  |
|    bicycle    |  0.0  |  0.0  |
+---------------+-------+-------+
2022-04-19 08:18:08,020 - mmseg - INFO - Summary:
2022-04-19 08:18:08,020 - mmseg - INFO - 
+-------+------+------+
|  aAcc | mIoU | mAcc |
+-------+------+------+
| 37.66 | 1.98 | 5.26 |
+-------+------+------+
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:18:08,030 - mmseg - INFO - Exp name: 220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02
2022-04-19 08:18:08,030 - mmseg - INFO - Iter [500/40000]	lr: 3.000e-05, eta: 18:57:18, time: 3.356, data_time: 0.018, memory: 9784, aAcc: 0.3766, mIoU: 0.0198, mAcc: 0.0526, IoU.road: 0.3766, IoU.sidewalk: 0.0000, IoU.building: 0.0000, IoU.wall: 0.0000, IoU.fence: 0.0000, IoU.pole: 0.0000, IoU.traffic light: 0.0000, IoU.traffic sign: 0.0000, IoU.vegetation: 0.0000, IoU.terrain: 0.0000, IoU.sky: 0.0000, IoU.person: 0.0000, IoU.rider: 0.0000, IoU.car: 0.0000, IoU.truck: 0.0000, IoU.bus: 0.0000, IoU.train: 0.0000, IoU.motorcycle: 0.0000, IoU.bicycle: 0.0000, Acc.road: 1.0000, Acc.sidewalk: 0.0000, Acc.building: 0.0000, Acc.wall: 0.0000, Acc.fence: 0.0000, Acc.pole: 0.0000, Acc.traffic light: 0.0000, Acc.traffic sign: 0.0000, Acc.vegetation: 0.0000, Acc.terrain: 0.0000, Acc.sky: 0.0000, Acc.person: 0.0000, Acc.rider: 0.0000, Acc.car: 0.0000, Acc.truck: 0.0000, Acc.bus: 0.0000, Acc.train: 0.0000, Acc.motorcycle: 0.0000, Acc.bicycle: 0.0000, decode.loss_seg: nan, decode.acc_seg: 20.1936, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.3368
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:20:58,774 - mmseg - INFO - Iter [20050/40000]	lr: 2.993e-05, eta: 18:58:31, time: 8.311, data_time: 4.916, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.8618, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.7815
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:23:47,845 - mmseg - INFO - Iter [20100/40000]	lr: 2.985e-05, eta: 18:55:38, time: 3.381, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.1504, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.9154
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:26:36,821 - mmseg - INFO - Iter [20150/40000]	lr: 2.978e-05, eta: 18:52:45, time: 3.380, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.5824, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.4124
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:29:26,236 - mmseg - INFO - Iter [20200/40000]	lr: 2.970e-05, eta: 18:49:52, time: 3.388, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.1234, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.4371
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:32:17,317 - mmseg - INFO - Iter [20250/40000]	lr: 2.963e-05, eta: 18:47:00, time: 3.422, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.6945, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 55.5821
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:35:04,874 - mmseg - INFO - Iter [20300/40000]	lr: 2.955e-05, eta: 18:44:06, time: 3.351, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.1905, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.9075
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:37:53,184 - mmseg - INFO - Iter [20350/40000]	lr: 2.948e-05, eta: 18:41:12, time: 3.366, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.0592, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.7225
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:40:39,923 - mmseg - INFO - Iter [20400/40000]	lr: 2.940e-05, eta: 18:38:16, time: 3.335, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 24.4682, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.3901
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:43:29,145 - mmseg - INFO - Iter [20450/40000]	lr: 2.933e-05, eta: 18:35:23, time: 3.384, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.6126, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.0197
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:46:17,421 - mmseg - INFO - Iter [20500/40000]	lr: 2.925e-05, eta: 18:32:29, time: 3.366, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.0910, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.0962
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:49:06,415 - mmseg - INFO - Iter [20550/40000]	lr: 2.918e-05, eta: 18:29:36, time: 3.380, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.3186, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.2206
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:51:53,837 - mmseg - INFO - Iter [20600/40000]	lr: 2.910e-05, eta: 18:26:41, time: 3.348, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.1378, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.1303
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:54:43,008 - mmseg - INFO - Iter [20650/40000]	lr: 2.903e-05, eta: 18:23:49, time: 3.383, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.3076, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.5564
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 08:57:31,928 - mmseg - INFO - Iter [20700/40000]	lr: 2.895e-05, eta: 18:20:55, time: 3.378, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.0892, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.5567
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:00:22,139 - mmseg - INFO - Iter [20750/40000]	lr: 2.888e-05, eta: 18:18:03, time: 3.404, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.6862, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.9631
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:03:08,802 - mmseg - INFO - Iter [20800/40000]	lr: 2.880e-05, eta: 18:15:08, time: 3.333, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.9854, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.2720
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:05:58,116 - mmseg - INFO - Iter [20850/40000]	lr: 2.873e-05, eta: 18:12:15, time: 3.386, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.0869, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.4968
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:08:47,089 - mmseg - INFO - Iter [20900/40000]	lr: 2.865e-05, eta: 18:09:22, time: 3.379, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 24.2848, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 64.6715
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:11:36,102 - mmseg - INFO - Iter [20950/40000]	lr: 2.858e-05, eta: 18:06:29, time: 3.380, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.2252, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.8710
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:14:23,606 - mmseg - INFO - Exp name: 220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02
2022-04-19 09:14:23,607 - mmseg - INFO - Iter [21000/40000]	lr: 2.850e-05, eta: 18:03:35, time: 3.350, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.7911, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.0618
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:17:13,595 - mmseg - INFO - Iter [21050/40000]	lr: 2.843e-05, eta: 18:00:43, time: 3.400, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.2577, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.2600
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:20:01,319 - mmseg - INFO - Iter [21100/40000]	lr: 2.835e-05, eta: 17:57:49, time: 3.354, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.9396, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.8957
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:22:50,004 - mmseg - INFO - Iter [21150/40000]	lr: 2.828e-05, eta: 17:54:55, time: 3.374, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.7012, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.1649
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:25:39,196 - mmseg - INFO - Iter [21200/40000]	lr: 2.820e-05, eta: 17:52:03, time: 3.384, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.5839, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.4914
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:28:28,669 - mmseg - INFO - Iter [21250/40000]	lr: 2.813e-05, eta: 17:49:10, time: 3.389, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.6812, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.1513
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:31:16,073 - mmseg - INFO - Iter [21300/40000]	lr: 2.805e-05, eta: 17:46:16, time: 3.348, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.7248, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.6275
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:34:06,124 - mmseg - INFO - Iter [21350/40000]	lr: 2.798e-05, eta: 17:43:24, time: 3.401, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 18.7139, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.7301
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:36:54,334 - mmseg - INFO - Iter [21400/40000]	lr: 2.790e-05, eta: 17:40:30, time: 3.364, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.3651, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.3000
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:39:43,432 - mmseg - INFO - Iter [21450/40000]	lr: 2.783e-05, eta: 17:37:38, time: 3.382, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.6405, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 63.4984
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:42:31,193 - mmseg - INFO - Iter [21500/40000]	lr: 2.775e-05, eta: 17:34:44, time: 3.355, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.6835, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.3174
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:45:19,766 - mmseg - INFO - Iter [21550/40000]	lr: 2.768e-05, eta: 17:31:51, time: 3.371, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.2137, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.1051
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:48:07,071 - mmseg - INFO - Iter [21600/40000]	lr: 2.760e-05, eta: 17:28:56, time: 3.346, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.1425, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.0996
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:50:57,180 - mmseg - INFO - Iter [21650/40000]	lr: 2.753e-05, eta: 17:26:05, time: 3.402, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.0659, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.0482
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:53:43,899 - mmseg - INFO - Iter [21700/40000]	lr: 2.745e-05, eta: 17:23:10, time: 3.334, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.7814, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.8433
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:56:32,165 - mmseg - INFO - Iter [21750/40000]	lr: 2.738e-05, eta: 17:20:17, time: 3.365, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.6198, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.0440
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 09:59:21,165 - mmseg - INFO - Iter [21800/40000]	lr: 2.730e-05, eta: 17:17:24, time: 3.380, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.0731, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.4032
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:02:10,872 - mmseg - INFO - Iter [21850/40000]	lr: 2.723e-05, eta: 17:14:32, time: 3.394, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.3905, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.0412
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:04:58,351 - mmseg - INFO - Iter [21900/40000]	lr: 2.715e-05, eta: 17:11:38, time: 3.350, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.7382, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.7929
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:07:48,655 - mmseg - INFO - Iter [21950/40000]	lr: 2.708e-05, eta: 17:08:47, time: 3.406, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.8235, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.0551
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:10:35,528 - mmseg - INFO - Exp name: 220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02
2022-04-19 10:10:35,528 - mmseg - INFO - Iter [22000/40000]	lr: 2.700e-05, eta: 17:05:52, time: 3.337, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.6463, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.2421
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:13:24,588 - mmseg - INFO - Iter [22050/40000]	lr: 2.693e-05, eta: 17:03:00, time: 3.381, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.4884, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.2509
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:16:13,913 - mmseg - INFO - Iter [22100/40000]	lr: 2.685e-05, eta: 17:00:07, time: 3.387, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.3672, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.4921
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:19:03,006 - mmseg - INFO - Iter [22150/40000]	lr: 2.678e-05, eta: 16:57:15, time: 3.382, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.1571, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.3636
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:21:50,850 - mmseg - INFO - Iter [22200/40000]	lr: 2.670e-05, eta: 16:54:21, time: 3.357, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.8777, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.1505
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:24:42,893 - mmseg - INFO - Iter [22250/40000]	lr: 2.663e-05, eta: 16:51:31, time: 3.441, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.1586, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.4028
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:27:31,420 - mmseg - INFO - Iter [22300/40000]	lr: 2.655e-05, eta: 16:48:38, time: 3.371, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 24.1096, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.3146
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:30:21,603 - mmseg - INFO - Iter [22350/40000]	lr: 2.648e-05, eta: 16:45:47, time: 3.404, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.1866, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.0936
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:33:10,254 - mmseg - INFO - Iter [22400/40000]	lr: 2.640e-05, eta: 16:42:54, time: 3.373, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.3294, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.6980
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:36:00,263 - mmseg - INFO - Iter [22450/40000]	lr: 2.633e-05, eta: 16:40:02, time: 3.400, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.9128, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.8214
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:38:47,694 - mmseg - INFO - Iter [22500/40000]	lr: 2.625e-05, eta: 16:37:09, time: 3.349, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.1742, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.5962
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:41:38,033 - mmseg - INFO - Iter [22550/40000]	lr: 2.618e-05, eta: 16:34:17, time: 3.407, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.8272, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.4519
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:44:24,960 - mmseg - INFO - Iter [22600/40000]	lr: 2.610e-05, eta: 16:31:23, time: 3.339, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.1102, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 63.0647
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:47:16,548 - mmseg - INFO - Iter [22650/40000]	lr: 2.603e-05, eta: 16:28:33, time: 3.432, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.4284, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.8371
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:50:06,085 - mmseg - INFO - Iter [22700/40000]	lr: 2.595e-05, eta: 16:25:41, time: 3.391, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.3957, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.0198
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:52:56,171 - mmseg - INFO - Iter [22750/40000]	lr: 2.588e-05, eta: 16:22:49, time: 3.402, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.8144, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.5349
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:55:45,806 - mmseg - INFO - Iter [22800/40000]	lr: 2.580e-05, eta: 16:19:57, time: 3.393, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.2829, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.5168
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 10:58:36,192 - mmseg - INFO - Iter [22850/40000]	lr: 2.573e-05, eta: 16:17:06, time: 3.408, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.2327, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.0962
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:01:23,178 - mmseg - INFO - Iter [22900/40000]	lr: 2.565e-05, eta: 16:14:12, time: 3.340, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.8262, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.3313
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:04:11,689 - mmseg - INFO - Iter [22950/40000]	lr: 2.558e-05, eta: 16:11:19, time: 3.370, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.9196, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.0180
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:06:59,575 - mmseg - INFO - Exp name: 220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02
2022-04-19 11:06:59,575 - mmseg - INFO - Iter [23000/40000]	lr: 2.550e-05, eta: 16:08:26, time: 3.358, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.9753, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.1080
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:09:48,878 - mmseg - INFO - Iter [23050/40000]	lr: 2.543e-05, eta: 16:05:34, time: 3.386, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 24.4336, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.8703
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:12:36,276 - mmseg - INFO - Iter [23100/40000]	lr: 2.535e-05, eta: 16:02:41, time: 3.348, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.5769, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.6354
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:15:26,492 - mmseg - INFO - Iter [23150/40000]	lr: 2.528e-05, eta: 15:59:49, time: 3.404, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.9756, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.3240
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:18:12,954 - mmseg - INFO - Iter [23200/40000]	lr: 2.520e-05, eta: 15:56:55, time: 3.329, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.7364, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 55.5200
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:21:02,660 - mmseg - INFO - Iter [23250/40000]	lr: 2.513e-05, eta: 15:54:04, time: 3.394, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.1345, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.0393
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:23:50,717 - mmseg - INFO - Iter [23300/40000]	lr: 2.505e-05, eta: 15:51:11, time: 3.361, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 24.3741, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.1894
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:26:39,497 - mmseg - INFO - Iter [23350/40000]	lr: 2.498e-05, eta: 15:48:18, time: 3.376, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.8646, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.3786
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:29:29,851 - mmseg - INFO - Iter [23400/40000]	lr: 2.490e-05, eta: 15:45:27, time: 3.407, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.2387, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 54.5768
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:32:19,739 - mmseg - INFO - Iter [23450/40000]	lr: 2.483e-05, eta: 15:42:35, time: 3.398, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.5340, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.4113
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:35:07,640 - mmseg - INFO - Iter [23500/40000]	lr: 2.475e-05, eta: 15:39:43, time: 3.358, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.0317, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 54.6509
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:37:57,129 - mmseg - INFO - Iter [23550/40000]	lr: 2.468e-05, eta: 15:36:51, time: 3.390, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.5850, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.1686
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:40:44,070 - mmseg - INFO - Iter [23600/40000]	lr: 2.460e-05, eta: 15:33:57, time: 3.339, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 18.8159, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 55.9495
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:43:33,743 - mmseg - INFO - Iter [23650/40000]	lr: 2.453e-05, eta: 15:31:05, time: 3.393, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.8309, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.0100
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:46:20,647 - mmseg - INFO - Iter [23700/40000]	lr: 2.445e-05, eta: 15:28:12, time: 3.338, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.5605, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.4511
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:49:11,202 - mmseg - INFO - Iter [23750/40000]	lr: 2.438e-05, eta: 15:25:21, time: 3.411, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.9261, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.0330
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:52:00,454 - mmseg - INFO - Iter [23800/40000]	lr: 2.430e-05, eta: 15:22:29, time: 3.385, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.4198, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.2544
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:54:50,990 - mmseg - INFO - Iter [23850/40000]	lr: 2.423e-05, eta: 15:19:38, time: 3.411, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.6121, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 55.4490
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 11:57:38,240 - mmseg - INFO - Iter [23900/40000]	lr: 2.415e-05, eta: 15:16:45, time: 3.345, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.1048, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.7782
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:00:26,823 - mmseg - INFO - Iter [23950/40000]	lr: 2.408e-05, eta: 15:13:52, time: 3.372, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.3183, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.5055
[                                                  ] 0/500, elapsed: 0s, ETA:
[                                 ] 1/500, 1.3 task/s, elapsed: 1s, ETA:   396s
[                                 ] 2/500, 1.7 task/s, elapsed: 1s, ETA:   295s
[                                 ] 3/500, 1.9 task/s, elapsed: 2s, ETA:   261s
[                                 ] 4/500, 2.0 task/s, elapsed: 2s, ETA:   243s
[                                 ] 5/500, 2.1 task/s, elapsed: 2s, ETA:   233s
[                                 ] 6/500, 2.2 task/s, elapsed: 3s, ETA:   226s
[                                 ] 7/500, 2.2 task/s, elapsed: 3s, ETA:   220s
[                                 ] 8/500, 2.3 task/s, elapsed: 4s, ETA:   217s
[                                 ] 9/500, 2.3 task/s, elapsed: 4s, ETA:   213s
[                                ] 10/500, 2.3 task/s, elapsed: 4s, ETA:   211s
[                                ] 11/500, 2.3 task/s, elapsed: 5s, ETA:   208s
[                                ] 12/500, 2.4 task/s, elapsed: 5s, ETA:   206s
[                                ] 13/500, 2.4 task/s, elapsed: 5s, ETA:   205s
[                                ] 14/500, 2.4 task/s, elapsed: 6s, ETA:   203s
[                                ] 15/500, 2.4 task/s, elapsed: 6s, ETA:   202s
[>                               ] 16/500, 2.4 task/s, elapsed: 7s, ETA:   200s
[>                               ] 17/500, 2.4 task/s, elapsed: 7s, ETA:   199s
[>                               ] 18/500, 2.4 task/s, elapsed: 7s, ETA:   198s
[>                               ] 19/500, 2.4 task/s, elapsed: 8s, ETA:   197s
[>                               ] 20/500, 2.4 task/s, elapsed: 8s, ETA:   196s
[>                               ] 21/500, 2.5 task/s, elapsed: 9s, ETA:   195s
[>                               ] 22/500, 2.5 task/s, elapsed: 9s, ETA:   194s
[>                               ] 23/500, 2.5 task/s, elapsed: 9s, ETA:   194s
[>                              ] 24/500, 2.5 task/s, elapsed: 10s, ETA:   193s
[>                              ] 25/500, 2.5 task/s, elapsed: 10s, ETA:   192s
[>                              ] 26/500, 2.5 task/s, elapsed: 10s, ETA:   191s
[>                              ] 27/500, 2.5 task/s, elapsed: 11s, ETA:   191s
[>                              ] 28/500, 2.5 task/s, elapsed: 11s, ETA:   190s
[>                              ] 29/500, 2.5 task/s, elapsed: 12s, ETA:   189s
[>                              ] 30/500, 2.5 task/s, elapsed: 12s, ETA:   189s
[>                              ] 31/500, 2.5 task/s, elapsed: 12s, ETA:   188s
[>                              ] 32/500, 2.5 task/s, elapsed: 13s, ETA:   188s
[>>                             ] 33/500, 2.5 task/s, elapsed: 13s, ETA:   187s
[>>                             ] 34/500, 2.5 task/s, elapsed: 14s, ETA:   187s
[>>                             ] 35/500, 2.5 task/s, elapsed: 14s, ETA:   186s
[>>                             ] 36/500, 2.5 task/s, elapsed: 14s, ETA:   185s
[>>                             ] 37/500, 2.5 task/s, elapsed: 15s, ETA:   185s
[>>                             ] 38/500, 2.5 task/s, elapsed: 15s, ETA:   184s
[>>                             ] 39/500, 2.5 task/s, elapsed: 16s, ETA:   184s
[>>                             ] 40/500, 2.5 task/s, elapsed: 16s, ETA:   183s
[>>                             ] 41/500, 2.5 task/s, elapsed: 16s, ETA:   183s
[>>                             ] 42/500, 2.5 task/s, elapsed: 17s, ETA:   182s
[>>                             ] 43/500, 2.5 task/s, elapsed: 17s, ETA:   182s
[>>                             ] 44/500, 2.5 task/s, elapsed: 17s, ETA:   181s
[>>                             ] 45/500, 2.5 task/s, elapsed: 18s, ETA:   181s
[>>                             ] 46/500, 2.5 task/s, elapsed: 18s, ETA:   180s
[>>                             ] 47/500, 2.5 task/s, elapsed: 19s, ETA:   180s
[>>                             ] 48/500, 2.5 task/s, elapsed: 19s, ETA:   179s
[>>>                            ] 49/500, 2.5 task/s, elapsed: 19s, ETA:   179s
[>>>                            ] 50/500, 2.5 task/s, elapsed: 20s, ETA:   179s
[>>>                            ] 51/500, 2.5 task/s, elapsed: 20s, ETA:   178s
[>>>                            ] 52/500, 2.5 task/s, elapsed: 21s, ETA:   178s
[>>>                            ] 53/500, 2.5 task/s, elapsed: 21s, ETA:   177s
[>>>                            ] 54/500, 2.5 task/s, elapsed: 21s, ETA:   177s
[>>>                            ] 55/500, 2.5 task/s, elapsed: 22s, ETA:   176s
[>>>                            ] 56/500, 2.5 task/s, elapsed: 22s, ETA:   176s
[>>>                            ] 57/500, 2.5 task/s, elapsed: 23s, ETA:   175s
[>>>                            ] 58/500, 2.5 task/s, elapsed: 23s, ETA:   175s
[>>>                            ] 59/500, 2.5 task/s, elapsed: 23s, ETA:   174s
[>>>                            ] 60/500, 2.5 task/s, elapsed: 24s, ETA:   174s
[>>>                            ] 61/500, 2.5 task/s, elapsed: 24s, ETA:   174s
[>>>                            ] 62/500, 2.5 task/s, elapsed: 25s, ETA:   173s
[>>>                            ] 63/500, 2.5 task/s, elapsed: 25s, ETA:   173s
[>>>                            ] 64/500, 2.5 task/s, elapsed: 25s, ETA:   172s
[>>>>                           ] 65/500, 2.5 task/s, elapsed: 26s, ETA:   172s
[>>>>                           ] 66/500, 2.5 task/s, elapsed: 26s, ETA:   171s
[>>>>                           ] 67/500, 2.5 task/s, elapsed: 26s, ETA:   171s
[>>>>                           ] 68/500, 2.5 task/s, elapsed: 27s, ETA:   171s
[>>>>                           ] 69/500, 2.5 task/s, elapsed: 27s, ETA:   170s
[>>>>                           ] 70/500, 2.5 task/s, elapsed: 28s, ETA:   170s
[>>>>                           ] 71/500, 2.5 task/s, elapsed: 28s, ETA:   169s
[>>>>                           ] 72/500, 2.5 task/s, elapsed: 28s, ETA:   169s
[>>>>                           ] 73/500, 2.5 task/s, elapsed: 29s, ETA:   168s
[>>>>                           ] 74/500, 2.5 task/s, elapsed: 29s, ETA:   168s
[>>>>                           ] 75/500, 2.5 task/s, elapsed: 30s, ETA:   168s
[>>>>                           ] 76/500, 2.5 task/s, elapsed: 30s, ETA:   167s
[>>>>                           ] 77/500, 2.5 task/s, elapsed: 30s, ETA:   167s
[>>>>                           ] 78/500, 2.5 task/s, elapsed: 31s, ETA:   166s
[>>>>                           ] 79/500, 2.5 task/s, elapsed: 31s, ETA:   166s
[>>>>                           ] 80/500, 2.5 task/s, elapsed: 32s, ETA:   165s
[>>>>>                          ] 81/500, 2.5 task/s, elapsed: 32s, ETA:   165s
[>>>>>                          ] 82/500, 2.5 task/s, elapsed: 32s, ETA:   165s
[>>>>>                          ] 83/500, 2.5 task/s, elapsed: 33s, ETA:   164s
[>>>>>                          ] 84/500, 2.5 task/s, elapsed: 33s, ETA:   164s
[>>>>>                          ] 85/500, 2.5 task/s, elapsed: 33s, ETA:   163s
[>>>>>                          ] 86/500, 2.5 task/s, elapsed: 34s, ETA:   163s
[>>>>>                          ] 87/500, 2.5 task/s, elapsed: 34s, ETA:   163s
[>>>>>                          ] 88/500, 2.5 task/s, elapsed: 35s, ETA:   162s
[>>>>>                          ] 89/500, 2.5 task/s, elapsed: 35s, ETA:   162s
[>>>>>                          ] 90/500, 2.5 task/s, elapsed: 35s, ETA:   161s
[>>>>>                          ] 91/500, 2.5 task/s, elapsed: 36s, ETA:   161s
[>>>>>                          ] 92/500, 2.5 task/s, elapsed: 36s, ETA:   160s
[>>>>>                          ] 93/500, 2.5 task/s, elapsed: 37s, ETA:   160s
[>>>>>                          ] 94/500, 2.5 task/s, elapsed: 37s, ETA:   160s
[>>>>>                          ] 95/500, 2.5 task/s, elapsed: 37s, ETA:   159s
[>>>>>                          ] 96/500, 2.5 task/s, elapsed: 38s, ETA:   159s
[>>>>>>                         ] 97/500, 2.5 task/s, elapsed: 38s, ETA:   158s
[>>>>>>                         ] 98/500, 2.5 task/s, elapsed: 39s, ETA:   158s
[>>>>>>                         ] 99/500, 2.5 task/s, elapsed: 39s, ETA:   158s
[>>>>>>                        ] 100/500, 2.5 task/s, elapsed: 39s, ETA:   157s
[>>>>>>                        ] 101/500, 2.5 task/s, elapsed: 40s, ETA:   157s
[>>>>>>                        ] 102/500, 2.5 task/s, elapsed: 40s, ETA:   156s
[>>>>>>                        ] 103/500, 2.5 task/s, elapsed: 40s, ETA:   156s
[>>>>>>                        ] 104/500, 2.5 task/s, elapsed: 41s, ETA:   156s
[>>>>>>                        ] 105/500, 2.5 task/s, elapsed: 41s, ETA:   155s
[>>>>>>                        ] 106/500, 2.5 task/s, elapsed: 42s, ETA:   155s
[>>>>>>                        ] 107/500, 2.5 task/s, elapsed: 42s, ETA:   154s
[>>>>>>                        ] 108/500, 2.5 task/s, elapsed: 42s, ETA:   154s
[>>>>>>                        ] 109/500, 2.5 task/s, elapsed: 43s, ETA:   154s
[>>>>>>                        ] 110/500, 2.5 task/s, elapsed: 43s, ETA:   153s
[>>>>>>                        ] 111/500, 2.5 task/s, elapsed: 44s, ETA:   153s
[>>>>>>                        ] 112/500, 2.5 task/s, elapsed: 44s, ETA:   152s
[>>>>>>                        ] 113/500, 2.5 task/s, elapsed: 44s, ETA:   152s
[>>>>>>                        ] 114/500, 2.5 task/s, elapsed: 45s, ETA:   151s
[>>>>>>                        ] 115/500, 2.5 task/s, elapsed: 45s, ETA:   151s
[>>>>>>                        ] 116/500, 2.5 task/s, elapsed: 46s, ETA:   151s
[>>>>>>>                       ] 117/500, 2.5 task/s, elapsed: 46s, ETA:   150s
[>>>>>>>                       ] 118/500, 2.5 task/s, elapsed: 46s, ETA:   150s
[>>>>>>>                       ] 119/500, 2.5 task/s, elapsed: 47s, ETA:   149s
[>>>>>>>                       ] 120/500, 2.5 task/s, elapsed: 47s, ETA:   149s
[>>>>>>>                       ] 121/500, 2.5 task/s, elapsed: 47s, ETA:   149s
[>>>>>>>                       ] 122/500, 2.5 task/s, elapsed: 48s, ETA:   148s
[>>>>>>>                       ] 123/500, 2.6 task/s, elapsed: 48s, ETA:   148s
[>>>>>>>                       ] 124/500, 2.6 task/s, elapsed: 49s, ETA:   147s
[>>>>>>>                       ] 125/500, 2.6 task/s, elapsed: 49s, ETA:   147s
[>>>>>>>                       ] 126/500, 2.6 task/s, elapsed: 49s, ETA:   147s
[>>>>>>>                       ] 127/500, 2.6 task/s, elapsed: 50s, ETA:   146s
[>>>>>>>                       ] 128/500, 2.6 task/s, elapsed: 50s, ETA:   146s
[>>>>>>>                       ] 129/500, 2.6 task/s, elapsed: 51s, ETA:   145s
[>>>>>>>                       ] 130/500, 2.6 task/s, elapsed: 51s, ETA:   145s
[>>>>>>>                       ] 131/500, 2.6 task/s, elapsed: 51s, ETA:   145s
[>>>>>>>                       ] 132/500, 2.6 task/s, elapsed: 52s, ETA:   144s
[>>>>>>>                       ] 133/500, 2.6 task/s, elapsed: 52s, ETA:   144s
[>>>>>>>>                      ] 134/500, 2.6 task/s, elapsed: 53s, ETA:   143s
[>>>>>>>>                      ] 135/500, 2.6 task/s, elapsed: 53s, ETA:   143s
[>>>>>>>>                      ] 136/500, 2.6 task/s, elapsed: 53s, ETA:   143s
[>>>>>>>>                      ] 137/500, 2.6 task/s, elapsed: 54s, ETA:   142s
[>>>>>>>>                      ] 138/500, 2.6 task/s, elapsed: 54s, ETA:   142s
[>>>>>>>>                      ] 139/500, 2.6 task/s, elapsed: 54s, ETA:   141s
[>>>>>>>>                      ] 140/500, 2.6 task/s, elapsed: 55s, ETA:   141s
[>>>>>>>>                      ] 141/500, 2.6 task/s, elapsed: 55s, ETA:   141s
[>>>>>>>>                      ] 142/500, 2.6 task/s, elapsed: 56s, ETA:   140s
[>>>>>>>>                      ] 143/500, 2.6 task/s, elapsed: 56s, ETA:   140s
[>>>>>>>>                      ] 144/500, 2.6 task/s, elapsed: 56s, ETA:   139s
[>>>>>>>>                      ] 145/500, 2.6 task/s, elapsed: 57s, ETA:   139s
[>>>>>>>>                      ] 146/500, 2.6 task/s, elapsed: 57s, ETA:   139s
[>>>>>>>>                      ] 147/500, 2.6 task/s, elapsed: 58s, ETA:   138s
[>>>>>>>>                      ] 148/500, 2.6 task/s, elapsed: 58s, ETA:   138s
[>>>>>>>>                      ] 149/500, 2.6 task/s, elapsed: 58s, ETA:   137s
[>>>>>>>>>                     ] 150/500, 2.6 task/s, elapsed: 59s, ETA:   137s
[>>>>>>>>>                     ] 151/500, 2.6 task/s, elapsed: 59s, ETA:   137s
[>>>>>>>>>                     ] 152/500, 2.6 task/s, elapsed: 60s, ETA:   136s
[>>>>>>>>>                     ] 153/500, 2.6 task/s, elapsed: 60s, ETA:   136s
[>>>>>>>>>                     ] 154/500, 2.6 task/s, elapsed: 60s, ETA:   135s
[>>>>>>>>>                     ] 155/500, 2.6 task/s, elapsed: 61s, ETA:   135s
[>>>>>>>>>                     ] 156/500, 2.6 task/s, elapsed: 61s, ETA:   135s
[>>>>>>>>>                     ] 157/500, 2.6 task/s, elapsed: 61s, ETA:   134s
[>>>>>>>>>                     ] 158/500, 2.6 task/s, elapsed: 62s, ETA:   134s
[>>>>>>>>>                     ] 159/500, 2.6 task/s, elapsed: 62s, ETA:   133s
[>>>>>>>>>                     ] 160/500, 2.6 task/s, elapsed: 63s, ETA:   133s
[>>>>>>>>>                     ] 161/500, 2.6 task/s, elapsed: 63s, ETA:   133s
[>>>>>>>>>                     ] 162/500, 2.6 task/s, elapsed: 63s, ETA:   132s
[>>>>>>>>>                     ] 163/500, 2.6 task/s, elapsed: 64s, ETA:   132s
[>>>>>>>>>                     ] 164/500, 2.6 task/s, elapsed: 64s, ETA:   132s
[>>>>>>>>>                     ] 165/500, 2.6 task/s, elapsed: 65s, ETA:   131s
[>>>>>>>>>                     ] 166/500, 2.6 task/s, elapsed: 65s, ETA:   131s
[>>>>>>>>>>                    ] 167/500, 2.6 task/s, elapsed: 65s, ETA:   130s
[>>>>>>>>>>                    ] 168/500, 2.6 task/s, elapsed: 66s, ETA:   130s
[>>>>>>>>>>                    ] 169/500, 2.6 task/s, elapsed: 66s, ETA:   130s
[>>>>>>>>>>                    ] 170/500, 2.6 task/s, elapsed: 67s, ETA:   129s
[>>>>>>>>>>                    ] 171/500, 2.6 task/s, elapsed: 67s, ETA:   129s
[>>>>>>>>>>                    ] 172/500, 2.6 task/s, elapsed: 67s, ETA:   128s
[>>>>>>>>>>                    ] 173/500, 2.6 task/s, elapsed: 68s, ETA:   128s
[>>>>>>>>>>                    ] 174/500, 2.6 task/s, elapsed: 68s, ETA:   128s
[>>>>>>>>>>                    ] 175/500, 2.6 task/s, elapsed: 69s, ETA:   127s
[>>>>>>>>>>                    ] 176/500, 2.6 task/s, elapsed: 69s, ETA:   127s
[>>>>>>>>>>                    ] 177/500, 2.6 task/s, elapsed: 69s, ETA:   127s
[>>>>>>>>>>                    ] 178/500, 2.6 task/s, elapsed: 70s, ETA:   126s
[>>>>>>>>>>                    ] 179/500, 2.6 task/s, elapsed: 70s, ETA:   126s
[>>>>>>>>>>                    ] 180/500, 2.6 task/s, elapsed: 71s, ETA:   125s
[>>>>>>>>>>                    ] 181/500, 2.6 task/s, elapsed: 71s, ETA:   125s
[>>>>>>>>>>                    ] 182/500, 2.6 task/s, elapsed: 71s, ETA:   125s
[>>>>>>>>>>                    ] 183/500, 2.6 task/s, elapsed: 72s, ETA:   124s
[>>>>>>>>>>>                   ] 184/500, 2.6 task/s, elapsed: 72s, ETA:   124s
[>>>>>>>>>>>                   ] 185/500, 2.6 task/s, elapsed: 73s, ETA:   123s
[>>>>>>>>>>>                   ] 186/500, 2.6 task/s, elapsed: 73s, ETA:   123s
[>>>>>>>>>>>                   ] 187/500, 2.6 task/s, elapsed: 73s, ETA:   123s
[>>>>>>>>>>>                   ] 188/500, 2.6 task/s, elapsed: 74s, ETA:   122s
[>>>>>>>>>>>                   ] 189/500, 2.6 task/s, elapsed: 74s, ETA:   122s
[>>>>>>>>>>>                   ] 190/500, 2.6 task/s, elapsed: 74s, ETA:   122s
[>>>>>>>>>>>                   ] 191/500, 2.6 task/s, elapsed: 75s, ETA:   121s
[>>>>>>>>>>>                   ] 192/500, 2.6 task/s, elapsed: 75s, ETA:   121s
[>>>>>>>>>>>                   ] 193/500, 2.6 task/s, elapsed: 76s, ETA:   120s
[>>>>>>>>>>>                   ] 194/500, 2.6 task/s, elapsed: 76s, ETA:   120s
[>>>>>>>>>>>                   ] 195/500, 2.6 task/s, elapsed: 76s, ETA:   120s
[>>>>>>>>>>>                   ] 196/500, 2.6 task/s, elapsed: 77s, ETA:   119s
[>>>>>>>>>>>                   ] 197/500, 2.6 task/s, elapsed: 77s, ETA:   119s
[>>>>>>>>>>>                   ] 198/500, 2.6 task/s, elapsed: 78s, ETA:   118s
[>>>>>>>>>>>                   ] 199/500, 2.6 task/s, elapsed: 78s, ETA:   118s
[>>>>>>>>>>>>                  ] 200/500, 2.6 task/s, elapsed: 78s, ETA:   118s
[>>>>>>>>>>>>                  ] 201/500, 2.6 task/s, elapsed: 79s, ETA:   117s
[>>>>>>>>>>>>                  ] 202/500, 2.6 task/s, elapsed: 79s, ETA:   117s
[>>>>>>>>>>>>                  ] 203/500, 2.6 task/s, elapsed: 80s, ETA:   116s
[>>>>>>>>>>>>                  ] 204/500, 2.6 task/s, elapsed: 80s, ETA:   116s
[>>>>>>>>>>>>                  ] 205/500, 2.6 task/s, elapsed: 80s, ETA:   116s
[>>>>>>>>>>>>                  ] 206/500, 2.6 task/s, elapsed: 81s, ETA:   115s
[>>>>>>>>>>>>                  ] 207/500, 2.5 task/s, elapsed: 81s, ETA:   115s
[>>>>>>>>>>>>                  ] 208/500, 2.5 task/s, elapsed: 82s, ETA:   115s
[>>>>>>>>>>>>                  ] 209/500, 2.5 task/s, elapsed: 82s, ETA:   114s
[>>>>>>>>>>>>                  ] 210/500, 2.5 task/s, elapsed: 82s, ETA:   114s
[>>>>>>>>>>>>                  ] 211/500, 2.5 task/s, elapsed: 83s, ETA:   113s
[>>>>>>>>>>>>                  ] 212/500, 2.5 task/s, elapsed: 83s, ETA:   113s
[>>>>>>>>>>>>                  ] 213/500, 2.5 task/s, elapsed: 84s, ETA:   113s
[>>>>>>>>>>>>                  ] 214/500, 2.5 task/s, elapsed: 84s, ETA:   112s
[>>>>>>>>>>>>                  ] 215/500, 2.5 task/s, elapsed: 84s, ETA:   112s
[>>>>>>>>>>>>                  ] 216/500, 2.5 task/s, elapsed: 85s, ETA:   111s
[>>>>>>>>>>>>>                 ] 217/500, 2.5 task/s, elapsed: 85s, ETA:   111s
[>>>>>>>>>>>>>                 ] 218/500, 2.5 task/s, elapsed: 86s, ETA:   111s
[>>>>>>>>>>>>>                 ] 219/500, 2.5 task/s, elapsed: 86s, ETA:   110s
[>>>>>>>>>>>>>                 ] 220/500, 2.5 task/s, elapsed: 86s, ETA:   110s
[>>>>>>>>>>>>>                 ] 221/500, 2.5 task/s, elapsed: 87s, ETA:   109s
[>>>>>>>>>>>>>                 ] 222/500, 2.5 task/s, elapsed: 87s, ETA:   109s
[>>>>>>>>>>>>>                 ] 223/500, 2.5 task/s, elapsed: 87s, ETA:   109s
[>>>>>>>>>>>>>                 ] 224/500, 2.5 task/s, elapsed: 88s, ETA:   108s
[>>>>>>>>>>>>>                 ] 225/500, 2.5 task/s, elapsed: 88s, ETA:   108s
[>>>>>>>>>>>>>                 ] 226/500, 2.5 task/s, elapsed: 89s, ETA:   107s
[>>>>>>>>>>>>>                 ] 227/500, 2.5 task/s, elapsed: 89s, ETA:   107s
[>>>>>>>>>>>>>                 ] 228/500, 2.5 task/s, elapsed: 89s, ETA:   107s
[>>>>>>>>>>>>>                 ] 229/500, 2.5 task/s, elapsed: 90s, ETA:   106s
[>>>>>>>>>>>>>                 ] 230/500, 2.5 task/s, elapsed: 90s, ETA:   106s
[>>>>>>>>>>>>>                 ] 231/500, 2.5 task/s, elapsed: 91s, ETA:   106s
[>>>>>>>>>>>>>                 ] 232/500, 2.5 task/s, elapsed: 91s, ETA:   105s
[>>>>>>>>>>>>>                 ] 233/500, 2.5 task/s, elapsed: 91s, ETA:   105s
[>>>>>>>>>>>>>>                ] 234/500, 2.5 task/s, elapsed: 92s, ETA:   104s
[>>>>>>>>>>>>>>                ] 235/500, 2.5 task/s, elapsed: 92s, ETA:   104s
[>>>>>>>>>>>>>>                ] 236/500, 2.5 task/s, elapsed: 93s, ETA:   104s
[>>>>>>>>>>>>>>                ] 237/500, 2.5 task/s, elapsed: 93s, ETA:   103s
[>>>>>>>>>>>>>>                ] 238/500, 2.5 task/s, elapsed: 93s, ETA:   103s
[>>>>>>>>>>>>>>                ] 239/500, 2.5 task/s, elapsed: 94s, ETA:   102s
[>>>>>>>>>>>>>>                ] 240/500, 2.5 task/s, elapsed: 94s, ETA:   102s
[>>>>>>>>>>>>>>                ] 241/500, 2.6 task/s, elapsed: 95s, ETA:   102s
[>>>>>>>>>>>>>>                ] 242/500, 2.6 task/s, elapsed: 95s, ETA:   101s
[>>>>>>>>>>>>>>                ] 243/500, 2.6 task/s, elapsed: 95s, ETA:   101s
[>>>>>>>>>>>>>>                ] 244/500, 2.6 task/s, elapsed: 96s, ETA:   100s
[>>>>>>>>>>>>>>                ] 245/500, 2.6 task/s, elapsed: 96s, ETA:   100s
[>>>>>>>>>>>>>>                ] 246/500, 2.5 task/s, elapsed: 96s, ETA:   100s
[>>>>>>>>>>>>>>                ] 247/500, 2.5 task/s, elapsed: 97s, ETA:    99s
[>>>>>>>>>>>>>>                ] 248/500, 2.5 task/s, elapsed: 97s, ETA:    99s
[>>>>>>>>>>>>>>                ] 249/500, 2.5 task/s, elapsed: 98s, ETA:    98s
[>>>>>>>>>>>>>>>               ] 250/500, 2.5 task/s, elapsed: 98s, ETA:    98s
[>>>>>>>>>>>>>>>               ] 251/500, 2.5 task/s, elapsed: 98s, ETA:    98s
[>>>>>>>>>>>>>>>               ] 252/500, 2.5 task/s, elapsed: 99s, ETA:    97s
[>>>>>>>>>>>>>>>               ] 253/500, 2.5 task/s, elapsed: 99s, ETA:    97s
[>>>>>>>>>>>>>>               ] 254/500, 2.5 task/s, elapsed: 100s, ETA:    96s
[>>>>>>>>>>>>>>               ] 255/500, 2.5 task/s, elapsed: 100s, ETA:    96s
[>>>>>>>>>>>>>>               ] 256/500, 2.5 task/s, elapsed: 100s, ETA:    96s
[>>>>>>>>>>>>>>               ] 257/500, 2.5 task/s, elapsed: 101s, ETA:    95s
[>>>>>>>>>>>>>>               ] 258/500, 2.5 task/s, elapsed: 101s, ETA:    95s
[>>>>>>>>>>>>>>>              ] 259/500, 2.5 task/s, elapsed: 102s, ETA:    95s
[>>>>>>>>>>>>>>>              ] 260/500, 2.5 task/s, elapsed: 102s, ETA:    94s
[>>>>>>>>>>>>>>>              ] 261/500, 2.5 task/s, elapsed: 102s, ETA:    94s
[>>>>>>>>>>>>>>>              ] 262/500, 2.5 task/s, elapsed: 103s, ETA:    93s
[>>>>>>>>>>>>>>>              ] 263/500, 2.5 task/s, elapsed: 103s, ETA:    93s
[>>>>>>>>>>>>>>>              ] 264/500, 2.5 task/s, elapsed: 104s, ETA:    93s
[>>>>>>>>>>>>>>>              ] 265/500, 2.5 task/s, elapsed: 104s, ETA:    92s
[>>>>>>>>>>>>>>>              ] 266/500, 2.5 task/s, elapsed: 104s, ETA:    92s
[>>>>>>>>>>>>>>>              ] 267/500, 2.5 task/s, elapsed: 105s, ETA:    91s
[>>>>>>>>>>>>>>>              ] 268/500, 2.5 task/s, elapsed: 105s, ETA:    91s
[>>>>>>>>>>>>>>>              ] 269/500, 2.5 task/s, elapsed: 106s, ETA:    91s
[>>>>>>>>>>>>>>>              ] 270/500, 2.5 task/s, elapsed: 106s, ETA:    90s
[>>>>>>>>>>>>>>>              ] 271/500, 2.5 task/s, elapsed: 106s, ETA:    90s
[>>>>>>>>>>>>>>>              ] 272/500, 2.5 task/s, elapsed: 107s, ETA:    89s
[>>>>>>>>>>>>>>>              ] 273/500, 2.5 task/s, elapsed: 107s, ETA:    89s
[>>>>>>>>>>>>>>>              ] 274/500, 2.5 task/s, elapsed: 108s, ETA:    89s
[>>>>>>>>>>>>>>>              ] 275/500, 2.5 task/s, elapsed: 108s, ETA:    88s
[>>>>>>>>>>>>>>>>             ] 276/500, 2.5 task/s, elapsed: 108s, ETA:    88s
[>>>>>>>>>>>>>>>>             ] 277/500, 2.5 task/s, elapsed: 109s, ETA:    88s
[>>>>>>>>>>>>>>>>             ] 278/500, 2.5 task/s, elapsed: 109s, ETA:    87s
[>>>>>>>>>>>>>>>>             ] 279/500, 2.5 task/s, elapsed: 110s, ETA:    87s
[>>>>>>>>>>>>>>>>             ] 280/500, 2.5 task/s, elapsed: 110s, ETA:    86s
[>>>>>>>>>>>>>>>>             ] 281/500, 2.5 task/s, elapsed: 110s, ETA:    86s
[>>>>>>>>>>>>>>>>             ] 282/500, 2.5 task/s, elapsed: 111s, ETA:    86s
[>>>>>>>>>>>>>>>>             ] 283/500, 2.5 task/s, elapsed: 111s, ETA:    85s
[>>>>>>>>>>>>>>>>             ] 284/500, 2.5 task/s, elapsed: 111s, ETA:    85s
[>>>>>>>>>>>>>>>>             ] 285/500, 2.5 task/s, elapsed: 112s, ETA:    84s
[>>>>>>>>>>>>>>>>             ] 286/500, 2.5 task/s, elapsed: 112s, ETA:    84s
[>>>>>>>>>>>>>>>>             ] 287/500, 2.5 task/s, elapsed: 113s, ETA:    84s
[>>>>>>>>>>>>>>>>             ] 288/500, 2.5 task/s, elapsed: 113s, ETA:    83s
[>>>>>>>>>>>>>>>>             ] 289/500, 2.5 task/s, elapsed: 113s, ETA:    83s
[>>>>>>>>>>>>>>>>             ] 290/500, 2.5 task/s, elapsed: 114s, ETA:    82s
[>>>>>>>>>>>>>>>>             ] 291/500, 2.5 task/s, elapsed: 114s, ETA:    82s
[>>>>>>>>>>>>>>>>             ] 292/500, 2.5 task/s, elapsed: 115s, ETA:    82s
[>>>>>>>>>>>>>>>>             ] 293/500, 2.5 task/s, elapsed: 115s, ETA:    81s
[>>>>>>>>>>>>>>>>>            ] 294/500, 2.5 task/s, elapsed: 115s, ETA:    81s
[>>>>>>>>>>>>>>>>>            ] 295/500, 2.5 task/s, elapsed: 116s, ETA:    81s
[>>>>>>>>>>>>>>>>>            ] 296/500, 2.5 task/s, elapsed: 116s, ETA:    80s
[>>>>>>>>>>>>>>>>>            ] 297/500, 2.5 task/s, elapsed: 117s, ETA:    80s
[>>>>>>>>>>>>>>>>>            ] 298/500, 2.5 task/s, elapsed: 117s, ETA:    79s
[>>>>>>>>>>>>>>>>>            ] 299/500, 2.5 task/s, elapsed: 117s, ETA:    79s
[>>>>>>>>>>>>>>>>>            ] 300/500, 2.5 task/s, elapsed: 118s, ETA:    79s
[>>>>>>>>>>>>>>>>>            ] 301/500, 2.5 task/s, elapsed: 118s, ETA:    78s
[>>>>>>>>>>>>>>>>>            ] 302/500, 2.5 task/s, elapsed: 119s, ETA:    78s
[>>>>>>>>>>>>>>>>>            ] 303/500, 2.5 task/s, elapsed: 119s, ETA:    77s
[>>>>>>>>>>>>>>>>>            ] 304/500, 2.5 task/s, elapsed: 120s, ETA:    77s
[>>>>>>>>>>>>>>>>>            ] 305/500, 2.5 task/s, elapsed: 120s, ETA:    77s
[>>>>>>>>>>>>>>>>>            ] 306/500, 2.5 task/s, elapsed: 120s, ETA:    76s
[>>>>>>>>>>>>>>>>>            ] 307/500, 2.5 task/s, elapsed: 121s, ETA:    76s
[>>>>>>>>>>>>>>>>>            ] 308/500, 2.5 task/s, elapsed: 121s, ETA:    76s
[>>>>>>>>>>>>>>>>>            ] 309/500, 2.5 task/s, elapsed: 122s, ETA:    75s
[>>>>>>>>>>>>>>>>>            ] 310/500, 2.5 task/s, elapsed: 122s, ETA:    75s
[>>>>>>>>>>>>>>>>>>           ] 311/500, 2.5 task/s, elapsed: 122s, ETA:    74s
[>>>>>>>>>>>>>>>>>>           ] 312/500, 2.5 task/s, elapsed: 123s, ETA:    74s
[>>>>>>>>>>>>>>>>>>           ] 313/500, 2.5 task/s, elapsed: 123s, ETA:    74s
[>>>>>>>>>>>>>>>>>>           ] 314/500, 2.5 task/s, elapsed: 124s, ETA:    73s
[>>>>>>>>>>>>>>>>>>           ] 315/500, 2.5 task/s, elapsed: 124s, ETA:    73s
[>>>>>>>>>>>>>>>>>>           ] 316/500, 2.5 task/s, elapsed: 125s, ETA:    73s
[>>>>>>>>>>>>>>>>>>           ] 317/500, 2.5 task/s, elapsed: 125s, ETA:    72s
[>>>>>>>>>>>>>>>>>>           ] 318/500, 2.5 task/s, elapsed: 125s, ETA:    72s
[>>>>>>>>>>>>>>>>>>           ] 319/500, 2.5 task/s, elapsed: 126s, ETA:    71s
[>>>>>>>>>>>>>>>>>>           ] 320/500, 2.5 task/s, elapsed: 126s, ETA:    71s
[>>>>>>>>>>>>>>>>>>           ] 321/500, 2.5 task/s, elapsed: 127s, ETA:    71s
[>>>>>>>>>>>>>>>>>>           ] 322/500, 2.5 task/s, elapsed: 127s, ETA:    70s
[>>>>>>>>>>>>>>>>>>           ] 323/500, 2.5 task/s, elapsed: 127s, ETA:    70s
[>>>>>>>>>>>>>>>>>>           ] 324/500, 2.5 task/s, elapsed: 128s, ETA:    69s
[>>>>>>>>>>>>>>>>>>           ] 325/500, 2.5 task/s, elapsed: 128s, ETA:    69s
[>>>>>>>>>>>>>>>>>>           ] 326/500, 2.5 task/s, elapsed: 129s, ETA:    69s
[>>>>>>>>>>>>>>>>>>           ] 327/500, 2.5 task/s, elapsed: 129s, ETA:    68s
[>>>>>>>>>>>>>>>>>>>          ] 328/500, 2.5 task/s, elapsed: 129s, ETA:    68s
[>>>>>>>>>>>>>>>>>>>          ] 329/500, 2.5 task/s, elapsed: 130s, ETA:    67s
[>>>>>>>>>>>>>>>>>>>          ] 330/500, 2.5 task/s, elapsed: 130s, ETA:    67s
[>>>>>>>>>>>>>>>>>>>          ] 331/500, 2.5 task/s, elapsed: 131s, ETA:    67s
[>>>>>>>>>>>>>>>>>>>          ] 332/500, 2.5 task/s, elapsed: 131s, ETA:    66s
[>>>>>>>>>>>>>>>>>>>          ] 333/500, 2.5 task/s, elapsed: 131s, ETA:    66s
[>>>>>>>>>>>>>>>>>>>          ] 334/500, 2.5 task/s, elapsed: 132s, ETA:    66s
[>>>>>>>>>>>>>>>>>>>          ] 335/500, 2.5 task/s, elapsed: 132s, ETA:    65s
[>>>>>>>>>>>>>>>>>>>          ] 336/500, 2.5 task/s, elapsed: 133s, ETA:    65s
[>>>>>>>>>>>>>>>>>>>          ] 337/500, 2.5 task/s, elapsed: 133s, ETA:    64s
[>>>>>>>>>>>>>>>>>>>          ] 338/500, 2.5 task/s, elapsed: 133s, ETA:    64s
[>>>>>>>>>>>>>>>>>>>          ] 339/500, 2.5 task/s, elapsed: 134s, ETA:    64s
[>>>>>>>>>>>>>>>>>>>          ] 340/500, 2.5 task/s, elapsed: 134s, ETA:    63s
[>>>>>>>>>>>>>>>>>>>          ] 341/500, 2.5 task/s, elapsed: 135s, ETA:    63s
[>>>>>>>>>>>>>>>>>>>          ] 342/500, 2.5 task/s, elapsed: 135s, ETA:    62s
[>>>>>>>>>>>>>>>>>>>          ] 343/500, 2.5 task/s, elapsed: 135s, ETA:    62s
[>>>>>>>>>>>>>>>>>>>          ] 344/500, 2.5 task/s, elapsed: 136s, ETA:    62s
[>>>>>>>>>>>>>>>>>>>>         ] 345/500, 2.5 task/s, elapsed: 136s, ETA:    61s
[>>>>>>>>>>>>>>>>>>>>         ] 346/500, 2.5 task/s, elapsed: 136s, ETA:    61s
[>>>>>>>>>>>>>>>>>>>>         ] 347/500, 2.5 task/s, elapsed: 137s, ETA:    60s
[>>>>>>>>>>>>>>>>>>>>         ] 348/500, 2.5 task/s, elapsed: 137s, ETA:    60s
[>>>>>>>>>>>>>>>>>>>>         ] 349/500, 2.5 task/s, elapsed: 138s, ETA:    60s
[>>>>>>>>>>>>>>>>>>>>         ] 350/500, 2.5 task/s, elapsed: 138s, ETA:    59s
[>>>>>>>>>>>>>>>>>>>>         ] 351/500, 2.5 task/s, elapsed: 138s, ETA:    59s
[>>>>>>>>>>>>>>>>>>>>         ] 352/500, 2.5 task/s, elapsed: 139s, ETA:    58s
[>>>>>>>>>>>>>>>>>>>>         ] 353/500, 2.5 task/s, elapsed: 139s, ETA:    58s
[>>>>>>>>>>>>>>>>>>>>         ] 354/500, 2.5 task/s, elapsed: 140s, ETA:    58s
[>>>>>>>>>>>>>>>>>>>>         ] 355/500, 2.5 task/s, elapsed: 140s, ETA:    57s
[>>>>>>>>>>>>>>>>>>>>         ] 356/500, 2.5 task/s, elapsed: 140s, ETA:    57s
[>>>>>>>>>>>>>>>>>>>>         ] 357/500, 2.5 task/s, elapsed: 141s, ETA:    56s
[>>>>>>>>>>>>>>>>>>>>         ] 358/500, 2.5 task/s, elapsed: 141s, ETA:    56s
[>>>>>>>>>>>>>>>>>>>>         ] 359/500, 2.5 task/s, elapsed: 142s, ETA:    56s
[>>>>>>>>>>>>>>>>>>>>         ] 360/500, 2.5 task/s, elapsed: 142s, ETA:    55s
[>>>>>>>>>>>>>>>>>>>>         ] 361/500, 2.5 task/s, elapsed: 142s, ETA:    55s
[>>>>>>>>>>>>>>>>>>>>         ] 362/500, 2.5 task/s, elapsed: 143s, ETA:    54s
[>>>>>>>>>>>>>>>>>>>>>        ] 363/500, 2.5 task/s, elapsed: 143s, ETA:    54s
[>>>>>>>>>>>>>>>>>>>>>        ] 364/500, 2.5 task/s, elapsed: 144s, ETA:    54s
[>>>>>>>>>>>>>>>>>>>>>        ] 365/500, 2.5 task/s, elapsed: 144s, ETA:    53s
[>>>>>>>>>>>>>>>>>>>>>        ] 366/500, 2.5 task/s, elapsed: 144s, ETA:    53s
[>>>>>>>>>>>>>>>>>>>>>        ] 367/500, 2.5 task/s, elapsed: 145s, ETA:    52s
[>>>>>>>>>>>>>>>>>>>>>        ] 368/500, 2.5 task/s, elapsed: 145s, ETA:    52s
[>>>>>>>>>>>>>>>>>>>>>        ] 369/500, 2.5 task/s, elapsed: 145s, ETA:    52s
[>>>>>>>>>>>>>>>>>>>>>        ] 370/500, 2.5 task/s, elapsed: 146s, ETA:    51s
[>>>>>>>>>>>>>>>>>>>>>        ] 371/500, 2.5 task/s, elapsed: 146s, ETA:    51s
[>>>>>>>>>>>>>>>>>>>>>        ] 372/500, 2.5 task/s, elapsed: 147s, ETA:    50s
[>>>>>>>>>>>>>>>>>>>>>        ] 373/500, 2.5 task/s, elapsed: 147s, ETA:    50s
[>>>>>>>>>>>>>>>>>>>>>        ] 374/500, 2.5 task/s, elapsed: 147s, ETA:    50s
[>>>>>>>>>>>>>>>>>>>>>        ] 375/500, 2.5 task/s, elapsed: 148s, ETA:    49s
[>>>>>>>>>>>>>>>>>>>>>        ] 376/500, 2.5 task/s, elapsed: 148s, ETA:    49s
[>>>>>>>>>>>>>>>>>>>>>        ] 377/500, 2.5 task/s, elapsed: 149s, ETA:    48s
[>>>>>>>>>>>>>>>>>>>>>        ] 378/500, 2.5 task/s, elapsed: 149s, ETA:    48s
[>>>>>>>>>>>>>>>>>>>>>        ] 379/500, 2.5 task/s, elapsed: 149s, ETA:    48s
[>>>>>>>>>>>>>>>>>>>>>>       ] 380/500, 2.5 task/s, elapsed: 150s, ETA:    47s
[>>>>>>>>>>>>>>>>>>>>>>       ] 381/500, 2.5 task/s, elapsed: 150s, ETA:    47s
[>>>>>>>>>>>>>>>>>>>>>>       ] 382/500, 2.5 task/s, elapsed: 151s, ETA:    46s
[>>>>>>>>>>>>>>>>>>>>>>       ] 383/500, 2.5 task/s, elapsed: 151s, ETA:    46s
[>>>>>>>>>>>>>>>>>>>>>>       ] 384/500, 2.5 task/s, elapsed: 151s, ETA:    46s
[>>>>>>>>>>>>>>>>>>>>>>       ] 385/500, 2.5 task/s, elapsed: 152s, ETA:    45s
[>>>>>>>>>>>>>>>>>>>>>>       ] 386/500, 2.5 task/s, elapsed: 152s, ETA:    45s
[>>>>>>>>>>>>>>>>>>>>>>       ] 387/500, 2.5 task/s, elapsed: 152s, ETA:    45s
[>>>>>>>>>>>>>>>>>>>>>>       ] 388/500, 2.5 task/s, elapsed: 153s, ETA:    44s
[>>>>>>>>>>>>>>>>>>>>>>       ] 389/500, 2.5 task/s, elapsed: 153s, ETA:    44s
[>>>>>>>>>>>>>>>>>>>>>>       ] 390/500, 2.5 task/s, elapsed: 154s, ETA:    43s
[>>>>>>>>>>>>>>>>>>>>>>       ] 391/500, 2.5 task/s, elapsed: 154s, ETA:    43s
[>>>>>>>>>>>>>>>>>>>>>>       ] 392/500, 2.5 task/s, elapsed: 154s, ETA:    43s
[>>>>>>>>>>>>>>>>>>>>>>       ] 393/500, 2.5 task/s, elapsed: 155s, ETA:    42s
[>>>>>>>>>>>>>>>>>>>>>>       ] 394/500, 2.5 task/s, elapsed: 155s, ETA:    42s
[>>>>>>>>>>>>>>>>>>>>>>       ] 395/500, 2.5 task/s, elapsed: 156s, ETA:    41s
[>>>>>>>>>>>>>>>>>>>>>>       ] 396/500, 2.5 task/s, elapsed: 156s, ETA:    41s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 397/500, 2.5 task/s, elapsed: 156s, ETA:    41s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 398/500, 2.5 task/s, elapsed: 157s, ETA:    40s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 399/500, 2.5 task/s, elapsed: 157s, ETA:    40s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 400/500, 2.5 task/s, elapsed: 158s, ETA:    39s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 401/500, 2.5 task/s, elapsed: 158s, ETA:    39s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 402/500, 2.5 task/s, elapsed: 158s, ETA:    39s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 403/500, 2.5 task/s, elapsed: 159s, ETA:    38s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 404/500, 2.5 task/s, elapsed: 159s, ETA:    38s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 405/500, 2.5 task/s, elapsed: 160s, ETA:    37s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 406/500, 2.5 task/s, elapsed: 160s, ETA:    37s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 407/500, 2.5 task/s, elapsed: 160s, ETA:    37s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 408/500, 2.5 task/s, elapsed: 161s, ETA:    36s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 409/500, 2.5 task/s, elapsed: 161s, ETA:    36s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 410/500, 2.5 task/s, elapsed: 162s, ETA:    35s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 411/500, 2.5 task/s, elapsed: 162s, ETA:    35s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 412/500, 2.5 task/s, elapsed: 162s, ETA:    35s
[>>>>>>>>>>>>>>>>>>>>>>>      ] 413/500, 2.5 task/s, elapsed: 163s, ETA:    34s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 414/500, 2.5 task/s, elapsed: 163s, ETA:    34s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 415/500, 2.5 task/s, elapsed: 164s, ETA:    34s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 416/500, 2.5 task/s, elapsed: 164s, ETA:    33s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 417/500, 2.5 task/s, elapsed: 164s, ETA:    33s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 418/500, 2.5 task/s, elapsed: 165s, ETA:    32s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 419/500, 2.5 task/s, elapsed: 165s, ETA:    32s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 420/500, 2.5 task/s, elapsed: 166s, ETA:    32s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 421/500, 2.5 task/s, elapsed: 166s, ETA:    31s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 422/500, 2.5 task/s, elapsed: 166s, ETA:    31s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 423/500, 2.5 task/s, elapsed: 167s, ETA:    30s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 424/500, 2.5 task/s, elapsed: 167s, ETA:    30s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 425/500, 2.5 task/s, elapsed: 168s, ETA:    30s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 426/500, 2.5 task/s, elapsed: 168s, ETA:    29s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 427/500, 2.5 task/s, elapsed: 168s, ETA:    29s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 428/500, 2.5 task/s, elapsed: 169s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 429/500, 2.5 task/s, elapsed: 169s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 430/500, 2.5 task/s, elapsed: 170s, ETA:    28s
[>>>>>>>>>>>>>>>>>>>>>>>>     ] 431/500, 2.5 task/s, elapsed: 170s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 432/500, 2.5 task/s, elapsed: 170s, ETA:    27s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 433/500, 2.5 task/s, elapsed: 171s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 434/500, 2.5 task/s, elapsed: 171s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 435/500, 2.5 task/s, elapsed: 172s, ETA:    26s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 436/500, 2.5 task/s, elapsed: 172s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 437/500, 2.5 task/s, elapsed: 172s, ETA:    25s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 438/500, 2.5 task/s, elapsed: 173s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 439/500, 2.5 task/s, elapsed: 173s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 440/500, 2.5 task/s, elapsed: 174s, ETA:    24s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 441/500, 2.5 task/s, elapsed: 174s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 442/500, 2.5 task/s, elapsed: 174s, ETA:    23s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 443/500, 2.5 task/s, elapsed: 175s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 444/500, 2.5 task/s, elapsed: 175s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 445/500, 2.5 task/s, elapsed: 176s, ETA:    22s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 446/500, 2.5 task/s, elapsed: 176s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 447/500, 2.5 task/s, elapsed: 176s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>>>>>    ] 448/500, 2.5 task/s, elapsed: 177s, ETA:    21s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 449/500, 2.5 task/s, elapsed: 177s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 450/500, 2.5 task/s, elapsed: 178s, ETA:    20s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 451/500, 2.5 task/s, elapsed: 178s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 452/500, 2.5 task/s, elapsed: 178s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 453/500, 2.5 task/s, elapsed: 179s, ETA:    19s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 454/500, 2.5 task/s, elapsed: 179s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 455/500, 2.5 task/s, elapsed: 180s, ETA:    18s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 456/500, 2.5 task/s, elapsed: 180s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 457/500, 2.5 task/s, elapsed: 181s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 458/500, 2.5 task/s, elapsed: 181s, ETA:    17s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 459/500, 2.5 task/s, elapsed: 181s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 460/500, 2.5 task/s, elapsed: 182s, ETA:    16s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 461/500, 2.5 task/s, elapsed: 182s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 462/500, 2.5 task/s, elapsed: 183s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 463/500, 2.5 task/s, elapsed: 183s, ETA:    15s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 464/500, 2.5 task/s, elapsed: 183s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 465/500, 2.5 task/s, elapsed: 184s, ETA:    14s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 466/500, 2.5 task/s, elapsed: 184s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 467/500, 2.5 task/s, elapsed: 185s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 468/500, 2.5 task/s, elapsed: 185s, ETA:    13s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 469/500, 2.5 task/s, elapsed: 185s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 470/500, 2.5 task/s, elapsed: 186s, ETA:    12s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 471/500, 2.5 task/s, elapsed: 186s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 472/500, 2.5 task/s, elapsed: 187s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 473/500, 2.5 task/s, elapsed: 187s, ETA:    11s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 474/500, 2.5 task/s, elapsed: 187s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 475/500, 2.5 task/s, elapsed: 188s, ETA:    10s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 476/500, 2.5 task/s, elapsed: 188s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 477/500, 2.5 task/s, elapsed: 189s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 478/500, 2.5 task/s, elapsed: 189s, ETA:     9s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 479/500, 2.5 task/s, elapsed: 189s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 480/500, 2.5 task/s, elapsed: 190s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 481/500, 2.5 task/s, elapsed: 190s, ETA:     8s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 482/500, 2.5 task/s, elapsed: 191s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 483/500, 2.5 task/s, elapsed: 191s, ETA:     7s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 484/500, 2.5 task/s, elapsed: 191s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 485/500, 2.5 task/s, elapsed: 192s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 486/500, 2.5 task/s, elapsed: 192s, ETA:     6s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 487/500, 2.5 task/s, elapsed: 193s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 488/500, 2.5 task/s, elapsed: 193s, ETA:     5s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 489/500, 2.5 task/s, elapsed: 193s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 490/500, 2.5 task/s, elapsed: 194s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 491/500, 2.5 task/s, elapsed: 194s, ETA:     4s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 492/500, 2.5 task/s, elapsed: 195s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 493/500, 2.5 task/s, elapsed: 195s, ETA:     3s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 494/500, 2.5 task/s, elapsed: 196s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 495/500, 2.5 task/s, elapsed: 196s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 496/500, 2.5 task/s, elapsed: 196s, ETA:     2s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 497/500, 2.5 task/s, elapsed: 197s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 498/500, 2.5 task/s, elapsed: 197s, ETA:     1s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 499/500, 2.5 task/s, elapsed: 198s, ETA:     0s
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 500/500, 2.5 task/s, elapsed: 198s, ETA:     0s2022-04-19 12:07:16,512 - mmseg - INFO - per class results:
2022-04-19 12:07:16,514 - mmseg - INFO - 
+---------------+-------+-------+
|     Class     |  IoU  |  Acc  |
+---------------+-------+-------+
|      road     | 37.66 | 100.0 |
|    sidewalk   |  0.0  |  0.0  |
|    building   |  0.0  |  0.0  |
|      wall     |  0.0  |  0.0  |
|     fence     |  0.0  |  0.0  |
|      pole     |  0.0  |  0.0  |
| traffic light |  0.0  |  0.0  |
|  traffic sign |  0.0  |  0.0  |
|   vegetation  |  0.0  |  0.0  |
|    terrain    |  0.0  |  0.0  |
|      sky      |  0.0  |  0.0  |
|     person    |  0.0  |  0.0  |
|     rider     |  0.0  |  0.0  |
|      car      |  0.0  |  0.0  |
|     truck     |  0.0  |  0.0  |
|      bus      |  0.0  |  0.0  |
|     train     |  0.0  |  0.0  |
|   motorcycle  |  0.0  |  0.0  |
|    bicycle    |  0.0  |  0.0  |
+---------------+-------+-------+
2022-04-19 12:07:16,514 - mmseg - INFO - Summary:
2022-04-19 12:07:16,514 - mmseg - INFO - 
+-------+------+------+
|  aAcc | mIoU | mAcc |
+-------+------+------+
| 37.66 | 1.98 | 5.26 |
+-------+------+------+
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:07:16,525 - mmseg - INFO - Exp name: 220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02
2022-04-19 12:07:16,526 - mmseg - INFO - Iter [500/40000]	lr: 2.400e-05, eta: 15:10:59, time: 3.327, data_time: 0.018, memory: 9784, aAcc: 0.3766, mIoU: 0.0198, mAcc: 0.0526, IoU.road: 0.3766, IoU.sidewalk: 0.0000, IoU.building: 0.0000, IoU.wall: 0.0000, IoU.fence: 0.0000, IoU.pole: 0.0000, IoU.traffic light: 0.0000, IoU.traffic sign: 0.0000, IoU.vegetation: 0.0000, IoU.terrain: 0.0000, IoU.sky: 0.0000, IoU.person: 0.0000, IoU.rider: 0.0000, IoU.car: 0.0000, IoU.truck: 0.0000, IoU.bus: 0.0000, IoU.train: 0.0000, IoU.motorcycle: 0.0000, IoU.bicycle: 0.0000, Acc.road: 1.0000, Acc.sidewalk: 0.0000, Acc.building: 0.0000, Acc.wall: 0.0000, Acc.fence: 0.0000, Acc.pole: 0.0000, Acc.traffic light: 0.0000, Acc.traffic sign: 0.0000, Acc.vegetation: 0.0000, Acc.terrain: 0.0000, Acc.sky: 0.0000, Acc.person: 0.0000, Acc.rider: 0.0000, Acc.car: 0.0000, Acc.truck: 0.0000, Acc.bus: 0.0000, Acc.train: 0.0000, Acc.motorcycle: 0.0000, Acc.bicycle: 0.0000, decode.loss_seg: nan, decode.acc_seg: 20.9419, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.7674
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:10:06,266 - mmseg - INFO - Iter [24050/40000]	lr: 2.393e-05, eta: 15:10:48, time: 8.262, data_time: 4.887, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.5946, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.6489
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:12:57,075 - mmseg - INFO - Iter [24100/40000]	lr: 2.385e-05, eta: 15:07:57, time: 3.416, data_time: 0.019, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.9093, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.3232
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:15:46,010 - mmseg - INFO - Iter [24150/40000]	lr: 2.378e-05, eta: 15:05:04, time: 3.379, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.3544, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.6746
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:18:35,819 - mmseg - INFO - Iter [24200/40000]	lr: 2.370e-05, eta: 15:02:12, time: 3.396, data_time: 0.019, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.3505, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.2070
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:21:26,650 - mmseg - INFO - Iter [24250/40000]	lr: 2.363e-05, eta: 14:59:20, time: 3.417, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.2992, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.3548
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:24:13,066 - mmseg - INFO - Iter [24300/40000]	lr: 2.355e-05, eta: 14:56:26, time: 3.328, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.1009, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.1571
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:27:02,595 - mmseg - INFO - Iter [24350/40000]	lr: 2.348e-05, eta: 14:53:33, time: 3.391, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.0208, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 55.7296
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:29:50,484 - mmseg - INFO - Iter [24400/40000]	lr: 2.340e-05, eta: 14:50:40, time: 3.358, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.0361, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 54.2124
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:32:40,036 - mmseg - INFO - Iter [24450/40000]	lr: 2.333e-05, eta: 14:47:47, time: 3.391, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.1940, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.8080
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:35:27,700 - mmseg - INFO - Iter [24500/40000]	lr: 2.325e-05, eta: 14:44:54, time: 3.353, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.7279, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.6675
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:38:16,290 - mmseg - INFO - Iter [24550/40000]	lr: 2.318e-05, eta: 14:42:01, time: 3.372, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 17.4995, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.8662
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:41:05,851 - mmseg - INFO - Iter [24600/40000]	lr: 2.310e-05, eta: 14:39:08, time: 3.391, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.3678, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.1017
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:43:55,117 - mmseg - INFO - Iter [24650/40000]	lr: 2.303e-05, eta: 14:36:16, time: 3.385, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.9666, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.7569
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:46:43,209 - mmseg - INFO - Iter [24700/40000]	lr: 2.295e-05, eta: 14:33:23, time: 3.362, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.8439, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.4425
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:49:33,623 - mmseg - INFO - Iter [24750/40000]	lr: 2.288e-05, eta: 14:30:31, time: 3.408, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.9091, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.9148
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:52:20,143 - mmseg - INFO - Iter [24800/40000]	lr: 2.280e-05, eta: 14:27:37, time: 3.330, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.5525, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.4906
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:55:10,052 - mmseg - INFO - Iter [24850/40000]	lr: 2.273e-05, eta: 14:24:45, time: 3.398, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.4239, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.9078
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 12:57:58,077 - mmseg - INFO - Iter [24900/40000]	lr: 2.265e-05, eta: 14:21:52, time: 3.361, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.7471, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.2381
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:00:50,186 - mmseg - INFO - Iter [24950/40000]	lr: 2.258e-05, eta: 14:19:01, time: 3.442, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.1654, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.9607
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:03:37,855 - mmseg - INFO - Exp name: 220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02
2022-04-19 13:03:37,855 - mmseg - INFO - Iter [25000/40000]	lr: 2.250e-05, eta: 14:16:07, time: 3.353, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 16.8504, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.2695
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:06:27,377 - mmseg - INFO - Iter [25050/40000]	lr: 2.243e-05, eta: 14:13:15, time: 3.390, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.9342, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.5070
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:09:14,617 - mmseg - INFO - Iter [25100/40000]	lr: 2.235e-05, eta: 14:10:22, time: 3.345, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.6053, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.8935
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:12:03,579 - mmseg - INFO - Iter [25150/40000]	lr: 2.228e-05, eta: 14:07:29, time: 3.379, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.8260, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.5405
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:14:53,379 - mmseg - INFO - Iter [25200/40000]	lr: 2.220e-05, eta: 14:04:37, time: 3.396, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 24.0084, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.6762
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:17:42,632 - mmseg - INFO - Iter [25250/40000]	lr: 2.213e-05, eta: 14:01:45, time: 3.385, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.8288, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.8494
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:20:31,156 - mmseg - INFO - Iter [25300/40000]	lr: 2.205e-05, eta: 13:58:52, time: 3.370, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.5812, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.8268
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:23:21,326 - mmseg - INFO - Iter [25350/40000]	lr: 2.198e-05, eta: 13:56:00, time: 3.403, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.9006, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.5495
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:26:09,771 - mmseg - INFO - Iter [25400/40000]	lr: 2.190e-05, eta: 13:53:07, time: 3.369, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.0466, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.3319
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:28:59,129 - mmseg - INFO - Iter [25450/40000]	lr: 2.183e-05, eta: 13:50:15, time: 3.387, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.6530, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.4950
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:31:48,194 - mmseg - INFO - Iter [25500/40000]	lr: 2.175e-05, eta: 13:47:23, time: 3.381, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.1588, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.2297
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:34:38,504 - mmseg - INFO - Iter [25550/40000]	lr: 2.168e-05, eta: 13:44:31, time: 3.406, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.5099, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.1157
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:37:26,693 - mmseg - INFO - Iter [25600/40000]	lr: 2.160e-05, eta: 13:41:38, time: 3.364, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.1782, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.9592
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:40:17,483 - mmseg - INFO - Iter [25650/40000]	lr: 2.153e-05, eta: 13:38:47, time: 3.416, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.0206, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.3157
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:43:05,093 - mmseg - INFO - Iter [25700/40000]	lr: 2.145e-05, eta: 13:35:54, time: 3.352, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.8631, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 60.0099
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:45:54,082 - mmseg - INFO - Iter [25750/40000]	lr: 2.138e-05, eta: 13:33:01, time: 3.380, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.2972, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.1325
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:48:41,044 - mmseg - INFO - Iter [25800/40000]	lr: 2.130e-05, eta: 13:30:08, time: 3.339, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.3424, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.8762
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:51:30,823 - mmseg - INFO - Iter [25850/40000]	lr: 2.123e-05, eta: 13:27:16, time: 3.396, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.4121, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.5968
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:54:19,818 - mmseg - INFO - Iter [25900/40000]	lr: 2.115e-05, eta: 13:24:24, time: 3.380, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.0795, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 56.1033
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:57:10,406 - mmseg - INFO - Iter [25950/40000]	lr: 2.108e-05, eta: 13:21:32, time: 3.412, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.5557, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 57.1042
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 13:59:57,669 - mmseg - INFO - Exp name: 220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02
2022-04-19 13:59:57,670 - mmseg - INFO - Iter [26000/40000]	lr: 2.100e-05, eta: 13:18:39, time: 3.345, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.4132, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.2839
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 14:02:51,063 - mmseg - INFO - Iter [26050/40000]	lr: 2.093e-05, eta: 13:15:49, time: 3.468, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.5632, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 58.1010
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 14:05:37,895 - mmseg - INFO - Iter [26100/40000]	lr: 2.085e-05, eta: 13:12:56, time: 3.337, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.1799, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.9515
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 14:08:27,164 - mmseg - INFO - Iter [26150/40000]	lr: 2.078e-05, eta: 13:10:03, time: 3.385, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.1450, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.0528
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 14:11:17,157 - mmseg - INFO - Iter [26200/40000]	lr: 2.070e-05, eta: 13:07:12, time: 3.400, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.3913, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 55.9104
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 14:14:07,674 - mmseg - INFO - Iter [26250/40000]	lr: 2.063e-05, eta: 13:04:20, time: 3.410, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.4827, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 55.8643
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 14:16:56,439 - mmseg - INFO - Iter [26300/40000]	lr: 2.055e-05, eta: 13:01:28, time: 3.375, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.7123, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 62.2067
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 14:19:45,765 - mmseg - INFO - Iter [26350/40000]	lr: 2.048e-05, eta: 12:58:36, time: 3.386, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 23.5178, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.2907
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 14:22:34,077 - mmseg - INFO - Iter [26400/40000]	lr: 2.040e-05, eta: 12:55:43, time: 3.366, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 19.4904, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.2310
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 14:25:24,755 - mmseg - INFO - Iter [26450/40000]	lr: 2.033e-05, eta: 12:52:52, time: 3.414, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.5441, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 55.7043
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 14:28:13,541 - mmseg - INFO - Iter [26500/40000]	lr: 2.025e-05, eta: 12:50:00, time: 3.376, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 20.8257, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.1588
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 14:31:03,441 - mmseg - INFO - Iter [26550/40000]	lr: 2.018e-05, eta: 12:47:08, time: 3.398, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 24.8613, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 63.1765
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 14:33:52,554 - mmseg - INFO - Iter [26600/40000]	lr: 2.010e-05, eta: 12:44:16, time: 3.382, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 22.5994, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 61.5236
/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py:55: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  mem_mb = torch.tensor([mem / (1024 * 1024)],
2022-04-19 14:36:43,065 - mmseg - INFO - Iter [26650/40000]	lr: 2.003e-05, eta: 12:41:24, time: 3.410, data_time: 0.018, memory: 9784, decode.loss_seg: nan, decode.acc_seg: 21.1322, src.loss_imnet_feat_dist: nan, mix.decode.loss_seg: nan, mix.decode.acc_seg: 59.2799
Traceback (most recent call last):
  File "/scratch_net/biwidl204/vramasamy/DAFormer/run_experiments.py", line 101, in <module>
    train.main([config_files[i]])
  File "/scratch_net/biwidl204/vramasamy/DAFormer/tools/train.py", line 166, in main
    train_segmentor(
  File "/scratch_net/biwidl204/vramasamy/DAFormer/mmseg/apis/train.py", line 131, in train_segmentor
    runner.run(data_loaders, cfg.workflow)
  File "/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/iter_based_runner.py", line 131, in run
    iter_runner(iter_loaders[i], **kwargs)
  File "/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/iter_based_runner.py", line 66, in train
    self.call_hook('after_train_iter')
  File "/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/base_runner.py", line 307, in call_hook
    getattr(hook, fn_name)(self)
  File "/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/base.py", line 152, in after_train_iter
    self.log(runner)
  File "/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py", line 178, in log
    self._dump_log(log_dict, runner)
  File "/itet-stor/vramasamy/net_scratch/conda_envs/daf/lib/python3.9/site-packages/mmcv/runner/hooks/logger/text.py", line 135, in _dump_log
    with open(self.json_log_path, 'a+') as f:
FileNotFoundError: [Errno 2] No such file or directory: '/scratch_net/biwidl204/vramasamy/DAFormer/work_dirs/local-exp7/220418_1314_syn2cs_dacs_a999_fdthings_rcs001_cpl_daformer_sepaspp_mitb5_poly10warm_s0_7aa02/20220418_131618.log.json'
